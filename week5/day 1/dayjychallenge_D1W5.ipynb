{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Text Analysis of books using word cloud"
      ],
      "metadata": {
        "id": "PQ9ptcQ995PZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "--jaXX_d9whf"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.gutenberg.org/files/11/11-0.txt\"  # Lien vers Alice au Pays des Merveilles\n",
        "response = requests.get(url)  # On t√©l√©charge le texte\n",
        "text = response.text  # On extrait le texte brut\n",
        "\n",
        "print(text[:500])  # Afficher les 500 premiers caract√®res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQgv-BF0_Pk5",
        "outputId": "3312e385-086a-450b-cec6-7f6f1800882b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** START OF THE PROJECT GUTENBERG EBOOK 11 ***\r\n",
            "[Illustration]\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Alice‚Äôs Adventures in Wonderland\r\n",
            "\r\n",
            "by Lewis Carroll\r\n",
            "\r\n",
            "THE MILLENNIUM FULCRUM EDITION 3.0\r\n",
            "\r\n",
            "Contents\r\n",
            "\r\n",
            " CHAPTER I.     Down the Rabbit-Hole\r\n",
            " CHAPTER II.    The Pool of Tears\r\n",
            " CHAPTER III.   A Caucus-Race and a Long Tale\r\n",
            " CHAPTER IV.    The Rabbit Sends in a Little Bill\r\n",
            " CHAPTER V.     Advice from a Caterpillar\r\n",
            " CHAPTER VI.    Pig and Pepper\r\n",
            " CHAPTER VII.   A Mad Tea-Party\r\n",
            " CHAPTER VIII.  The Queen‚Äôs Croquet-Ground\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re  # Biblioth√®que pour manipuler le texte\n",
        "\n",
        "# Supprimer tout ce qui n'est pas une lettre ou un espace\n",
        "clean_text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "\n",
        "print(clean_text[:500])  # Afficher les 500 premiers caract√®res nettoy√©s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KVYQCcT_wuX",
        "outputId": "fe6333b9-b835-4a82-c431-46ec2dcc34b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " START OF THE PROJECT GUTENBERG EBOOK  \r\n",
            "Illustration\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Alices Adventures in Wonderland\r\n",
            "\r\n",
            "by Lewis Carroll\r\n",
            "\r\n",
            "THE MILLENNIUM FULCRUM EDITION \r\n",
            "\r\n",
            "Contents\r\n",
            "\r\n",
            " CHAPTER I     Down the RabbitHole\r\n",
            " CHAPTER II    The Pool of Tears\r\n",
            " CHAPTER III   A CaucusRace and a Long Tale\r\n",
            " CHAPTER IV    The Rabbit Sends in a Little Bill\r\n",
            " CHAPTER V     Advice from a Caterpillar\r\n",
            " CHAPTER VI    Pig and Pepper\r\n",
            " CHAPTER VII   A Mad TeaParty\r\n",
            " CHAPTER VIII  The Queens CroquetGround\r\n",
            " CHAPTER IX    The Mock T\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_texts(urls):\n",
        "    corpus = []\n",
        "\n",
        "    for url in urls:\n",
        "        # T√©l√©charger le texte\n",
        "        response = requests.get(url)\n",
        "        text = response.text\n",
        "\n",
        "        # Supprimer les caract√®res sp√©ciaux\n",
        "        text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "\n",
        "        # Trouver la position de 'START' et '*** END'\n",
        "        start_idx = text.find('START')\n",
        "        end_idx = text.find('*** END')\n",
        "\n",
        "        # Garder uniquement le texte entre START et END\n",
        "        if start_idx != -1 and end_idx != -1:\n",
        "            text = text[start_idx+5:end_idx]\n",
        "\n",
        "        # Ajouter au corpus\n",
        "        corpus.append(text)\n",
        "\n",
        "    return corpus\n",
        "\n",
        "# Liste des URLs des livres\n",
        "urls = [\n",
        "    \"https://www.gutenberg.org/files/11/11-0.txt\",  # Alice‚Äôs Adventures in Wonderland\n",
        "    \"https://www.gutenberg.org/files/12/12-0.txt\",  # THROUGH THE LOOKING-GLASS\n",
        "    \"https://www.gutenberg.org/files/10538/10538-0.txt\"  # A Tangled Tale\n",
        "]\n",
        "\n",
        "# Charger et nettoyer les textes\n",
        "corpus = load_texts(urls)\n",
        "\n",
        "# Afficher les 200 premiers caract√®res de chaque livre\n",
        "for i, text in enumerate(corpus):\n",
        "    print(f\"\\nLivre {i+1} - Extrait :\")\n",
        "    print(text[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-Y7Fca69_5Q",
        "outputId": "cf9df19a-b5e7-4067-ad53-c5f237a4f996"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Livre 1 - Extrait :\n",
            " START OF THE PROJECT GUTENBERG EBOOK  \r\n",
            "Illustration\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Alices Adventures in Wonderland\r\n",
            "\r\n",
            "by Lewis Carroll\r\n",
            "\r\n",
            "THE MILLENNIUM FULCRUM EDITION \r\n",
            "\r\n",
            "Contents\r\n",
            "\r\n",
            " CHAPTER I     Down the RabbitHole\r\n",
            "\n",
            "Livre 2 - Extrait :\n",
            " START OF THE PROJECT GUTENBERG EBOOK  \r\n",
            "Illustration\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "THROUGH THE LOOKINGGLASS\r\n",
            "\r\n",
            "And What Alice Found There\r\n",
            "\r\n",
            "By Lewis Carroll\r\n",
            "\r\n",
            "The Millennium Fulcrum Edition \r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "DRAMATIS PERSON\r\n",
            "A\n",
            "\n",
            "Livre 3 - Extrait :\n",
            " START OF THE PROJECT GUTENBERG EBOOK  \r\n",
            "\r\n",
            "HYACINTH\r\n",
            "\r\n",
            "By George A Birmingham\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "CHAPTER I\r\n",
            "\r\n",
            "In the year  or thereabouts religious and charitable society in\r\n",
            "England was seized with a desir\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Trouver la position des marqueurs START et END\n",
        "    start_idx = text.find(\"START OF THE PROJECT\")\n",
        "    end_idx = text.find(\"*** END\")\n",
        "\n",
        "    # V√©rifier que les marqueurs existent\n",
        "    if start_idx != -1 and end_idx != -1:\n",
        "        text = text[start_idx+30:end_idx]  # On garde uniquement la partie utile\n",
        "\n",
        "    # Supprimer les caract√®res inutiles (ponctuation, symboles)\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Appliquer la fonction √† chaque livre\n",
        "cleaned_corpus = [clean_text(text) for text in [text]]  # Si plusieurs livres, on boucle dessus\n",
        "\n",
        "# Afficher les 500 premiers caract√®res du texte nettoy√©\n",
        "print(cleaned_corpus[0][:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHWc-t2srU-h",
        "outputId": "0c8dbd74-8196-4266-d72d-b7c2f0bd2bca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " START OF THE PROJECT GUTENBERG EBOOK  \r\n",
            "\r\n",
            "HYACINTH\r\n",
            "\r\n",
            "By George A Birmingham\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "CHAPTER I\r\n",
            "\r\n",
            "In the year  or thereabouts religious and charitable society in\r\n",
            "England was seized with a desire to convert Irish Roman Catholics to\r\n",
            "the Protestant faith It is clear to everyone with any experience of\r\n",
            "missionary societies that the more remote the field of actual work the\r\n",
            "easier it is to keep alive the interest of subscribers The mission to\r\n",
            "Roman Catholics therefore commenced in that weste\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, text in enumerate([text]):  # Si plusieurs livres, remplace [text] par ton corpus\n",
        "    start_idx = text.find(\"START OF THE PROJECT\")\n",
        "    end_idx = text.find(\"*** END\")\n",
        "    print(f\"Livre {i+1} - Index de START : {start_idx}, Index de END : {end_idx}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2OosWFWyfQa",
        "outputId": "522f12e4-06f3-4898-89f2-f5ac83996946"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Livre 1 - Index de START : 1, Index de END : -1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[-500:])  # Afficher les 500 derniers caract√®res du texte"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVMu9wh2yxoy",
        "outputId": "e4e48adc-14b2-428f-860d-a1d4f25f5fe3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aving fine friends like that if they wont get\r\n",
            "you sent to a place like Carrowkeel that never another minister but\r\n",
            "yourself would as much as eat his dinner in twice if he could help it\r\n",
            "\r\n",
            "Hyacinth glanced doubtfully at Marion The child lay quiet in her arms\r\n",
            "She slept uncomfortably It was clear that she had not cared to listen\r\n",
            "to the conversation of the two men\r\n",
            "\r\n",
            "THE END\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "End of the Project Gutenberg EBook of Hyacinth by George A Birmingham\r\n",
            "\r\n",
            " END OF THE PROJECT GUTENBERG EBOOK  \r\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "reajuste le code pour trouve la fin"
      ],
      "metadata": {
        "id": "etpQ9jdRzJcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"üìö Nombre de livres charg√©s : {len(corpus)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mc9SdQG-z_Ii",
        "outputId": "3fa3895e-6c80-4713-ddbf-0595def72bb3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Nombre de livres charg√©s : 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Trouver la position du d√©but de l‚Äôhistoire\n",
        "    start_idx = text.find(\"START OF THE PROJECT\")\n",
        "\n",
        "    # Essayer plusieurs variantes pour d√©tecter la fin\n",
        "    end_variants = [\"THE END\", \"End of the Project Gutenberg\", \"END OF THE PROJECT GUTENBERG EBOOK\"]\n",
        "    end_idx = -1\n",
        "\n",
        "    for variant in end_variants:\n",
        "        found_idx = text.find(variant)\n",
        "        if found_idx != -1:\n",
        "            end_idx = found_idx\n",
        "            break  # On prend le premier qui fonctionne\n",
        "\n",
        "    # V√©rifier que les marqueurs existent\n",
        "    if start_idx != -1 and end_idx != -1:\n",
        "        text = text[start_idx+30:end_idx]  # On garde uniquement la partie utile\n",
        "\n",
        "    # Supprimer la ponctuation et caract√®res inutiles\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Appliquer la fonction √† chaque livre du corpus\n",
        "cleaned_corpus = [clean_text(text) for text in corpus]\n",
        "\n",
        "# V√©rifier si nous avons bien 3 livres apr√®s nettoyage\n",
        "print(f\" Nombre de livres apr√®s nettoyage : {len(cleaned_corpus)}\")\n",
        "\n",
        "# Afficher les 200 premiers caract√®res de chaque livre apr√®s nettoyage\n",
        "for i, text in enumerate(cleaned_corpus):\n",
        "    print(f\"\\n Livre {i+1} - Extrait apr√®s nettoyage :\")\n",
        "    print(text[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSKNA2h60hVZ",
        "outputId": "2259afa1-629b-4473-af53-77ccabc26087"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Nombre de livres apr√®s nettoyage : 3\n",
            "\n",
            " Livre 1 - Extrait apr√®s nettoyage :\n",
            " EBOOK  \r\n",
            "Illustration\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Alices Adventures in Wonderland\r\n",
            "\r\n",
            "by Lewis Carroll\r\n",
            "\r\n",
            "THE MILLENNIUM FULCRUM EDITION \r\n",
            "\r\n",
            "Contents\r\n",
            "\r\n",
            " CHAPTER I     Down the RabbitHole\r\n",
            " CHAPTER II    The Pool of Tea\n",
            "\n",
            " Livre 2 - Extrait apr√®s nettoyage :\n",
            " EBOOK  \r\n",
            "Illustration\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "THROUGH THE LOOKINGGLASS\r\n",
            "\r\n",
            "And What Alice Found There\r\n",
            "\r\n",
            "By Lewis Carroll\r\n",
            "\r\n",
            "The Millennium Fulcrum Edition \r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "DRAMATIS PERSON\r\n",
            "As arranged before commencement \n",
            "\n",
            " Livre 3 - Extrait apr√®s nettoyage :\n",
            " EBOOK  \r\n",
            "\r\n",
            "HYACINTH\r\n",
            "\r\n",
            "By George A Birmingham\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "CHAPTER I\r\n",
            "\r\n",
            "In the year  or thereabouts religious and charitable society in\r\n",
            "England was seized with a desire to convert Irish Roman Cathol\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Appliquer la tokenisation aux 3 livres"
      ],
      "metadata": {
        "id": "oq-Tee2Q13xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # T√©l√©charger le tokenizer\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvGzZflM1xQT",
        "outputId": "b2676f6e-10ba-48d3-f9a5-2fcd387cb56d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokeniser chaque livre\n",
        "tokenized_corpus = [word_tokenize(text) for text in cleaned_corpus]\n",
        "\n",
        "# Afficher les 150 premiers tokens de chaque livre\n",
        "for i, tokens in enumerate(tokenized_corpus):\n",
        "    print(f\"\\n Livre {i+1} - 150 premiers tokens :\")\n",
        "    print(tokens[:150])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxcuYUJe1yjZ",
        "outputId": "24211f1d-e9b3-4ee9-dce0-3fb109fb51be"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Livre 1 - 150 premiers tokens :\n",
            "['EBOOK', 'Illustration', 'Alices', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', 'THE', 'MILLENNIUM', 'FULCRUM', 'EDITION', 'Contents', 'CHAPTER', 'I', 'Down', 'the', 'RabbitHole', 'CHAPTER', 'II', 'The', 'Pool', 'of', 'Tears', 'CHAPTER', 'III', 'A', 'CaucusRace', 'and', 'a', 'Long', 'Tale', 'CHAPTER', 'IV', 'The', 'Rabbit', 'Sends', 'in', 'a', 'Little', 'Bill', 'CHAPTER', 'V', 'Advice', 'from', 'a', 'Caterpillar', 'CHAPTER', 'VI', 'Pig', 'and', 'Pepper', 'CHAPTER', 'VII', 'A', 'Mad', 'TeaParty', 'CHAPTER', 'VIII', 'The', 'Queens', 'CroquetGround', 'CHAPTER', 'IX', 'The', 'Mock', 'Turtles', 'Story', 'CHAPTER', 'X', 'The', 'Lobster', 'Quadrille', 'CHAPTER', 'XI', 'Who', 'Stole', 'the', 'Tarts', 'CHAPTER', 'XII', 'Alices', 'Evidence', 'CHAPTER', 'I', 'Down', 'the', 'RabbitHole', 'Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', 'and', 'of', 'having', 'nothing', 'to', 'do', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', 'and', 'what', 'is', 'the', 'use', 'of', 'a', 'book', 'thought', 'Alice', 'without', 'pictures', 'or', 'conversations', 'So', 'she', 'was', 'considering']\n",
            "\n",
            " Livre 2 - 150 premiers tokens :\n",
            "['EBOOK', 'Illustration', 'THROUGH', 'THE', 'LOOKINGGLASS', 'And', 'What', 'Alice', 'Found', 'There', 'By', 'Lewis', 'Carroll', 'The', 'Millennium', 'Fulcrum', 'Edition', 'DRAMATIS', 'PERSON', 'As', 'arranged', 'before', 'commencement', 'of', 'game', 'WHITE', 'RED', 'PIECES', 'PAWNS', 'PAWNS', 'PIECES', 'Tweedledee', 'Daisy', 'Daisy', 'Humpty', 'Dumpty', 'Unicorn', 'Haigha', 'Messenger', 'Carpenter', 'Sheep', 'Oyster', 'Oyster', 'Walrus', 'W', 'Queen', 'Lily', 'Tigerlily', 'R', 'Queen', 'W', 'King', 'Fawn', 'Rose', 'R', 'King', 'Aged', 'man', 'Oyster', 'Oyster', 'Crow', 'W', 'Knight', 'Hatta', 'Frog', 'R', 'Knight', 'Tweedledum', 'Daisy', 'Daisy', 'Lion', 'RED', 'Illustration', 'chessboard', 'WHITE', 'White', 'Pawn', 'Alice', 'to', 'play', 'and', 'win', 'in', 'eleven', 'moves', 'Alice', 'meets', 'R', 'Q', 'R', 'Q', 'to', 'K', 'Rs', 'th', 'Alice', 'through', 'Qs', 'd', 'by', 'railway', 'to', 'th', 'Tweedledum', 'and', 'Tweedledee', 'W', 'Q', 'to', 'Q', 'Bs', 'th', 'after', 'shawl', 'Alice', 'meets', 'W', 'Q', 'with', 'shawl', 'W', 'Q', 'to', 'QBs', 'th', 'becomes', 'sheep', 'Alice', 'to', 'Qs', 'th', 'shop', 'river', 'shop', 'W', 'Q', 'to', 'K', 'Bs', 'th', 'leaves', 'egg', 'on', 'shelf', 'Alice', 'to', 'Qs', 'th', 'Humpty', 'Dumpty']\n",
            "\n",
            " Livre 3 - 150 premiers tokens :\n",
            "['EBOOK', 'HYACINTH', 'By', 'George', 'A', 'Birmingham', 'CHAPTER', 'I', 'In', 'the', 'year', 'or', 'thereabouts', 'religious', 'and', 'charitable', 'society', 'in', 'England', 'was', 'seized', 'with', 'a', 'desire', 'to', 'convert', 'Irish', 'Roman', 'Catholics', 'to', 'the', 'Protestant', 'faith', 'It', 'is', 'clear', 'to', 'everyone', 'with', 'any', 'experience', 'of', 'missionary', 'societies', 'that', 'the', 'more', 'remote', 'the', 'field', 'of', 'actual', 'work', 'the', 'easier', 'it', 'is', 'to', 'keep', 'alive', 'the', 'interest', 'of', 'subscribers', 'The', 'mission', 'to', 'Roman', 'Catholics', 'therefore', 'commenced', 'in', 'that', 'western', 'portion', 'of', 'Galway', 'which', 'the', 'modern', 'tourist', 'knows', 'as', 'Connemara', 'and', 'the', 'enthusiasm', 'was', 'immense', 'Elderly', 'ladies', 'often', 'with', 'titles', 'were', 'energetic', 'in', 'the', 'cause', 'of', 'the', 'new', 'reformation', 'Young', 'ladies', 'some', 'of', 'them', 'very', 'attractive', 'collected', 'money', 'from', 'their', 'brothers', 'and', 'admirers', 'Statesmen', 'and', 'Bishops', 'headed', 'the', 'subscriptionlists', 'and', 'influential', 'committees', 'earnestly', 'debated', 'plans', 'for', 'spending', 'the', 'money', 'which', 'poured', 'in', 'Faith', 'in', 'the', 'efficacy', 'of', 'money', 'handled', 'by', 'influential', 'committees', 'is', 'one', 'of', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppression des stopwords"
      ],
      "metadata": {
        "id": "9-Ua3Fc33BmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78Mz6JvO3TQS",
        "outputId": "11867379-07ae-4afa-c8f7-d1084d5ff90b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger les stopwords en anglais\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# V√©rifier quelques stopwords\n",
        "print(f\"Quelques stopwords en anglais : {list(stop_words)[:10]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYt5b9Dq3CbB",
        "outputId": "5623f1c3-f393-4cdd-e69a-3c860d884a55"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quelques stopwords en anglais : ['them', 'do', 'nor', 'on', 'very', \"he's\", 'shan', 'theirs', 'will', \"you'll\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Supprimer les stopwords de chaque livre\n",
        "filtered_corpus = [\n",
        "    [word for word in tokens if word.lower() not in stop_words]  # Convertir en minuscule et filtrer\n",
        "    for tokens in tokenized_corpus\n",
        "]\n",
        "\n",
        "# Afficher les 150 premiers tokens apr√®s suppression des stopwords\n",
        "for i, tokens in enumerate(filtered_corpus):\n",
        "    print(f\"\\nLivre {i+1} - 150 premiers tokens apr√®s suppression des stopwords :\")\n",
        "    print(tokens[:150])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE-0g4l_3pdJ",
        "outputId": "665584f1-124e-4355-b0d7-426d28cbd786"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Livre 1 - 150 premiers tokens apr√®s suppression des stopwords :\n",
            "['EBOOK', 'Illustration', 'Alices', 'Adventures', 'Wonderland', 'Lewis', 'Carroll', 'MILLENNIUM', 'FULCRUM', 'EDITION', 'Contents', 'CHAPTER', 'RabbitHole', 'CHAPTER', 'II', 'Pool', 'Tears', 'CHAPTER', 'III', 'CaucusRace', 'Long', 'Tale', 'CHAPTER', 'IV', 'Rabbit', 'Sends', 'Little', 'Bill', 'CHAPTER', 'V', 'Advice', 'Caterpillar', 'CHAPTER', 'VI', 'Pig', 'Pepper', 'CHAPTER', 'VII', 'Mad', 'TeaParty', 'CHAPTER', 'VIII', 'Queens', 'CroquetGround', 'CHAPTER', 'IX', 'Mock', 'Turtles', 'Story', 'CHAPTER', 'X', 'Lobster', 'Quadrille', 'CHAPTER', 'XI', 'Stole', 'Tarts', 'CHAPTER', 'XII', 'Alices', 'Evidence', 'CHAPTER', 'RabbitHole', 'Alice', 'beginning', 'get', 'tired', 'sitting', 'sister', 'bank', 'nothing', 'twice', 'peeped', 'book', 'sister', 'reading', 'pictures', 'conversations', 'use', 'book', 'thought', 'Alice', 'without', 'pictures', 'conversations', 'considering', 'mind', 'well', 'could', 'hot', 'day', 'made', 'feel', 'sleepy', 'stupid', 'whether', 'pleasure', 'making', 'daisychain', 'would', 'worth', 'trouble', 'getting', 'picking', 'daisies', 'suddenly', 'White', 'Rabbit', 'pink', 'eyes', 'ran', 'close', 'nothing', 'remarkable', 'Alice', 'think', 'much', 'way', 'hear', 'Rabbit', 'say', 'Oh', 'dear', 'Oh', 'dear', 'shall', 'late', 'thought', 'afterwards', 'occurred', 'ought', 'wondered', 'time', 'seemed', 'quite', 'natural', 'Rabbit', 'actually', 'took', 'watch', 'waistcoatpocket', 'looked', 'hurried', 'Alice', 'started', 'feet', 'flashed', 'across', 'mind', 'never']\n",
            "\n",
            "Livre 2 - 150 premiers tokens apr√®s suppression des stopwords :\n",
            "['EBOOK', 'Illustration', 'LOOKINGGLASS', 'Alice', 'Found', 'Lewis', 'Carroll', 'Millennium', 'Fulcrum', 'Edition', 'DRAMATIS', 'PERSON', 'arranged', 'commencement', 'game', 'WHITE', 'RED', 'PIECES', 'PAWNS', 'PAWNS', 'PIECES', 'Tweedledee', 'Daisy', 'Daisy', 'Humpty', 'Dumpty', 'Unicorn', 'Haigha', 'Messenger', 'Carpenter', 'Sheep', 'Oyster', 'Oyster', 'Walrus', 'W', 'Queen', 'Lily', 'Tigerlily', 'R', 'Queen', 'W', 'King', 'Fawn', 'Rose', 'R', 'King', 'Aged', 'man', 'Oyster', 'Oyster', 'Crow', 'W', 'Knight', 'Hatta', 'Frog', 'R', 'Knight', 'Tweedledum', 'Daisy', 'Daisy', 'Lion', 'RED', 'Illustration', 'chessboard', 'WHITE', 'White', 'Pawn', 'Alice', 'play', 'win', 'eleven', 'moves', 'Alice', 'meets', 'R', 'Q', 'R', 'Q', 'K', 'Rs', 'th', 'Alice', 'Qs', 'railway', 'th', 'Tweedledum', 'Tweedledee', 'W', 'Q', 'Q', 'Bs', 'th', 'shawl', 'Alice', 'meets', 'W', 'Q', 'shawl', 'W', 'Q', 'QBs', 'th', 'becomes', 'sheep', 'Alice', 'Qs', 'th', 'shop', 'river', 'shop', 'W', 'Q', 'K', 'Bs', 'th', 'leaves', 'egg', 'shelf', 'Alice', 'Qs', 'th', 'Humpty', 'Dumpty', 'W', 'Q', 'Q', 'Bs', 'th', 'flying', 'R', 'Kt', 'Alice', 'Qs', 'th', 'forest', 'R', 'Kt', 'Ks', 'nd', 'ch', 'WKt', 'takes', 'RKt', 'W', 'Kt', 'K', 'Bs', 'th', 'Alice', 'Qs']\n",
            "\n",
            "Livre 3 - 150 premiers tokens apr√®s suppression des stopwords :\n",
            "['EBOOK', 'HYACINTH', 'George', 'Birmingham', 'CHAPTER', 'year', 'thereabouts', 'religious', 'charitable', 'society', 'England', 'seized', 'desire', 'convert', 'Irish', 'Roman', 'Catholics', 'Protestant', 'faith', 'clear', 'everyone', 'experience', 'missionary', 'societies', 'remote', 'field', 'actual', 'work', 'easier', 'keep', 'alive', 'interest', 'subscribers', 'mission', 'Roman', 'Catholics', 'therefore', 'commenced', 'western', 'portion', 'Galway', 'modern', 'tourist', 'knows', 'Connemara', 'enthusiasm', 'immense', 'Elderly', 'ladies', 'often', 'titles', 'energetic', 'cause', 'new', 'reformation', 'Young', 'ladies', 'attractive', 'collected', 'money', 'brothers', 'admirers', 'Statesmen', 'Bishops', 'headed', 'subscriptionlists', 'influential', 'committees', 'earnestly', 'debated', 'plans', 'spending', 'money', 'poured', 'Faith', 'efficacy', 'money', 'handled', 'influential', 'committees', 'one', 'characteristics', 'English', 'people', 'particular', 'case', 'seemed', 'faith', 'justified', 'results', 'encouraging', 'reports', 'sent', 'headquarters', 'Connemara', 'appeared', 'converts', 'flocking', 'schools', 'missionaries', 'filled', 'overflowing', 'matter', 'education', 'circumstances', 'favoured', 'new', 'reformation', 'leonine', 'John', 'McHale', 'Papal', 'Archbishop', 'Tuam', 'pursued', 'policy', 'drove', 'children', 'flock', 'mission', 'schools', 'kind', 'education', 'available', 'humorous', 'English', 'statesman', 'called', 'national', 'seem', 'Archbishop', 'desirable', 'Irish', 'boy', 'beaten', 'speaking', 'language', 'rewarded', 'calling', 'happy', 'English', 'child', 'refused', 'allow', 'building', 'national', 'schools', 'diocese', 'thus', 'left']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V√©rifier la suppression"
      ],
      "metadata": {
        "id": "eIN0p8a04AbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Liste de stopwords √† v√©rifier\n",
        "words_to_check = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ourselves\"]\n",
        "\n",
        "for i, tokens in enumerate(filtered_corpus):\n",
        "    print(f\"\\n Livre {i+1} - V√©rification des stopwords supprim√©s :\")\n",
        "    for word in words_to_check:\n",
        "        count = tokens.count(word)\n",
        "        print(f\"  {word}: {count} fois\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgterl3U3_a6",
        "outputId": "346b94df-3eca-43e9-b74d-41ca43f9f613"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Livre 1 - V√©rification des stopwords supprim√©s :\n",
            "  i: 0 fois\n",
            "  me: 0 fois\n",
            "  my: 0 fois\n",
            "  myself: 0 fois\n",
            "  we: 0 fois\n",
            "  our: 0 fois\n",
            "  ourselves: 0 fois\n",
            "\n",
            " Livre 2 - V√©rification des stopwords supprim√©s :\n",
            "  i: 0 fois\n",
            "  me: 0 fois\n",
            "  my: 0 fois\n",
            "  myself: 0 fois\n",
            "  we: 0 fois\n",
            "  our: 0 fois\n",
            "  ourselves: 0 fois\n",
            "\n",
            " Livre 3 - V√©rification des stopwords supprim√©s :\n",
            "  i: 0 fois\n",
            "  me: 0 fois\n",
            "  my: 0 fois\n",
            "  myself: 0 fois\n",
            "  we: 0 fois\n",
            "  our: 0 fois\n",
            "  ourselves: 0 fois\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "r√©duire les mots √† leur racine (stemming)."
      ],
      "metadata": {
        "id": "4QHIQGu64pXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialiser le stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Appliquer le stemming √† chaque mot dans les livres\n",
        "stemmed_corpus = [\n",
        "    [stemmer.stem(word) for word in tokens]  # Appliquer le stemming\n",
        "    for tokens in filtered_corpus\n",
        "]\n",
        "\n",
        "# Afficher les 50 premiers mots stemm√©s de chaque livre\n",
        "for i, tokens in enumerate(stemmed_corpus):\n",
        "    print(f\"\\n Livre {i+1} - 50 premiers mots apr√®s stemming :\")\n",
        "    print(tokens[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jN25gUD84qm6",
        "outputId": "a475efe4-c10d-4781-89d9-1e0612be924c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Livre 1 - 50 premiers mots apr√®s stemming :\n",
            "['ebook', 'illustr', 'alic', 'adventur', 'wonderland', 'lewi', 'carrol', 'millennium', 'fulcrum', 'edit', 'content', 'chapter', 'rabbithol', 'chapter', 'ii', 'pool', 'tear', 'chapter', 'iii', 'caucusrac', 'long', 'tale', 'chapter', 'iv', 'rabbit', 'send', 'littl', 'bill', 'chapter', 'v', 'advic', 'caterpillar', 'chapter', 'vi', 'pig', 'pepper', 'chapter', 'vii', 'mad', 'teaparti', 'chapter', 'viii', 'queen', 'croquetground', 'chapter', 'ix', 'mock', 'turtl', 'stori', 'chapter']\n",
            "\n",
            " Livre 2 - 50 premiers mots apr√®s stemming :\n",
            "['ebook', 'illustr', 'lookingglass', 'alic', 'found', 'lewi', 'carrol', 'millennium', 'fulcrum', 'edit', 'dramati', 'person', 'arrang', 'commenc', 'game', 'white', 'red', 'piec', 'pawn', 'pawn', 'piec', 'tweedlede', 'daisi', 'daisi', 'humpti', 'dumpti', 'unicorn', 'haigha', 'messeng', 'carpent', 'sheep', 'oyster', 'oyster', 'walru', 'w', 'queen', 'lili', 'tigerlili', 'r', 'queen', 'w', 'king', 'fawn', 'rose', 'r', 'king', 'age', 'man', 'oyster', 'oyster']\n",
            "\n",
            " Livre 3 - 50 premiers mots apr√®s stemming :\n",
            "['ebook', 'hyacinth', 'georg', 'birmingham', 'chapter', 'year', 'thereabout', 'religi', 'charit', 'societi', 'england', 'seiz', 'desir', 'convert', 'irish', 'roman', 'cathol', 'protest', 'faith', 'clear', 'everyon', 'experi', 'missionari', 'societi', 'remot', 'field', 'actual', 'work', 'easier', 'keep', 'aliv', 'interest', 'subscrib', 'mission', 'roman', 'cathol', 'therefor', 'commenc', 'western', 'portion', 'galway', 'modern', 'tourist', 'know', 'connemara', 'enthusiasm', 'immens', 'elderli', 'ladi', 'often']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# T√©l√©charger le mod√®le anglais si ce n'est pas encore fait\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Charger le mod√®le spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAi1D4l_8c9j",
        "outputId": "bf716362-a126-497b-9ae4-e774afa3a625"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Appliquer la lemmatisation aux livres"
      ],
      "metadata": {
        "id": "DkE9DxAQT6DP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Appliquer la lemmatisation √† chaque livre\n",
        "lemmatized_corpus = [\n",
        "    [token.lemma_ for token in nlp(\" \".join(tokens))]  # Convertir tokens en texte et appliquer spaCy\n",
        "    for tokens in filtered_corpus\n",
        "]\n",
        "\n",
        "# Afficher les 50 premiers mots lemmatis√©s de chaque livre\n",
        "for i, tokens in enumerate(lemmatized_corpus):\n",
        "    print(f\"\\n Livre {i+1} - 50 premiers mots apr√®s lemmatisation :\")\n",
        "    print(tokens[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2STbGAToTToT",
        "outputId": "c405bc0d-dcb8-425e-e5b3-efb01590fa2f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Livre 1 - 50 premiers mots apr√®s lemmatisation :\n",
            "['EBOOK', 'Illustration', 'Alices', 'Adventures', 'Wonderland', 'Lewis', 'Carroll', 'MILLENNIUM', 'FULCRUM', 'EDITION', 'Contents', 'CHAPTER', 'RabbitHole', 'chapter', 'II', 'Pool', 'Tears', 'CHAPTER', 'III', 'CaucusRace', 'Long', 'Tale', 'chapter', 'IV', 'Rabbit', 'send', 'Little', 'Bill', 'chapter', 'V', 'Advice', 'Caterpillar', 'CHAPTER', 'VI', 'Pig', 'Pepper', 'chapter', 'VII', 'Mad', 'TeaParty', 'chapter', 'VIII', 'Queens', 'CroquetGround', 'chapter', 'IX', 'Mock', 'Turtles', 'Story', 'CHAPTER']\n",
            "\n",
            " Livre 2 - 50 premiers mots apr√®s lemmatisation :\n",
            "['EBOOK', 'Illustration', 'LOOKINGGLASS', 'Alice', 'find', 'Lewis', 'Carroll', 'Millennium', 'Fulcrum', 'Edition', 'DRAMATIS', 'person', 'arrange', 'commencement', 'game', 'WHITE', 'RED', 'PIECES', 'pawns', 'pawn', 'piece', 'Tweedledee', 'Daisy', 'Daisy', 'Humpty', 'Dumpty', 'Unicorn', 'Haigha', 'Messenger', 'Carpenter', 'Sheep', 'Oyster', 'Oyster', 'Walrus', 'W', 'Queen', 'Lily', 'Tigerlily', 'r', 'Queen', 'W', 'King', 'Fawn', 'Rose', 'r', 'King', 'aged', 'man', 'Oyster', 'Oyster']\n",
            "\n",
            " Livre 3 - 50 premiers mots apr√®s lemmatisation :\n",
            "['ebook', 'HYACINTH', 'George', 'Birmingham', 'CHAPTER', 'year', 'thereabout', 'religious', 'charitable', 'society', 'England', 'seize', 'desire', 'convert', 'Irish', 'Roman', 'Catholics', 'protestant', 'faith', 'clear', 'everyone', 'experience', 'missionary', 'society', 'remote', 'field', 'actual', 'work', 'easier', 'keep', 'alive', 'interest', 'subscriber', 'mission', 'Roman', 'Catholics', 'therefore', 'commence', 'western', 'portion', 'Galway', 'modern', 'tourist', 'know', 'Connemara', 'enthusiasm', 'immense', 'elderly', 'lady', 'often']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "comparaison stemming et lemmatisation :--> la lemmatisation est plus pr√©cise et linguistiquement correcte.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZPTvkQjzVDq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparer les 50 premiers mots apr√®s stemming et lemmatisation pour le premier livre\n",
        "print(\"\\n Comparaison des 50 premiers mots - Livre 1\")\n",
        "print(f\"{'Stemmed':<20} {'Lemmatized':<20}\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "for stem, lemma in zip(stemmed_corpus[0][:50], lemmatized_corpus[0][:50]):\n",
        "    print(f\"{stem:<20} {lemma:<20}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQA9k1X8VXDB",
        "outputId": "3f8f749f-aea9-4df6-8842-bd7b7fcebef3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Comparaison des 50 premiers mots - Livre 1\n",
            "Stemmed              Lemmatized          \n",
            "========================================\n",
            "ebook                EBOOK               \n",
            "illustr              Illustration        \n",
            "alic                 Alices              \n",
            "adventur             Adventures          \n",
            "wonderland           Wonderland          \n",
            "lewi                 Lewis               \n",
            "carrol               Carroll             \n",
            "millennium           MILLENNIUM          \n",
            "fulcrum              FULCRUM             \n",
            "edit                 EDITION             \n",
            "content              Contents            \n",
            "chapter              CHAPTER             \n",
            "rabbithol            RabbitHole          \n",
            "chapter              chapter             \n",
            "ii                   II                  \n",
            "pool                 Pool                \n",
            "tear                 Tears               \n",
            "chapter              CHAPTER             \n",
            "iii                  III                 \n",
            "caucusrac            CaucusRace          \n",
            "long                 Long                \n",
            "tale                 Tale                \n",
            "chapter              chapter             \n",
            "iv                   IV                  \n",
            "rabbit               Rabbit              \n",
            "send                 send                \n",
            "littl                Little              \n",
            "bill                 Bill                \n",
            "chapter              chapter             \n",
            "v                    V                   \n",
            "advic                Advice              \n",
            "caterpillar          Caterpillar         \n",
            "chapter              CHAPTER             \n",
            "vi                   VI                  \n",
            "pig                  Pig                 \n",
            "pepper               Pepper              \n",
            "chapter              chapter             \n",
            "vii                  VII                 \n",
            "mad                  Mad                 \n",
            "teaparti             TeaParty            \n",
            "chapter              chapter             \n",
            "viii                 VIII                \n",
            "queen                Queens              \n",
            "croquetground        CroquetGround       \n",
            "chapter              chapter             \n",
            "ix                   IX                  \n",
            "mock                 Mock                \n",
            "turtl                Turtles             \n",
            "stori                Story               \n",
            "chapter              CHAPTER             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "identify POS tags od."
      ],
      "metadata": {
        "id": "41t_7xDlVkBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')  # T√©l√©charger le mod√®le de POS tagging"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjieF6YzWhHz",
        "outputId": "1bbea18c-1640-4190-dc99-629f0952b4f1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n"
      ],
      "metadata": {
        "id": "TN7JZH82Y-oO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Supprimer et ret√©l√©charger le mod√®le\n",
        "nltk.data.path.append('/usr/local/nltk_data')"
      ],
      "metadata": {
        "id": "shupfuz8ZB0X"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger', download_dir='/usr/local/nltk_data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61Be2RXVZEF5",
        "outputId": "f8bc7055-f2aa-4625-9d33-7312bf041547"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /usr/local/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# T√©l√©charger le mod√®le anglais si ce n'est pas encore fait\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Charger le mod√®le\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Appliquer le POS tagging avec SpaCy\n",
        "pos_tagged_corpus_spacy = [\n",
        "    [(token.text, token.pos_) for token in nlp(\" \".join(tokens))]  # Convertir tokens en texte et analyser\n",
        "    for tokens in filtered_corpus\n",
        "]\n",
        "\n",
        "# Afficher les 50 premiers mots avec leurs POS tags pour chaque livre\n",
        "for i, tagged_tokens in enumerate(pos_tagged_corpus_spacy):\n",
        "    print(f\"\\n Livre {i+1} - 50 premiers mots avec leurs POS tags :\")\n",
        "    print(tagged_tokens[:50])  # Afficher les 50 premiers mots avec leur POS tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "japUiQKKZW5o",
        "outputId": "ca8f121e-49fe-48f4-b243-6b55d9f46a9f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\n",
            "üìñ Livre 1 - 50 premiers mots avec leurs POS tags :\n",
            "[('EBOOK', 'NOUN'), ('Illustration', 'PROPN'), ('Alices', 'PROPN'), ('Adventures', 'PROPN'), ('Wonderland', 'PROPN'), ('Lewis', 'PROPN'), ('Carroll', 'PROPN'), ('MILLENNIUM', 'PROPN'), ('FULCRUM', 'PROPN'), ('EDITION', 'PROPN'), ('Contents', 'PROPN'), ('CHAPTER', 'PROPN'), ('RabbitHole', 'PROPN'), ('CHAPTER', 'NOUN'), ('II', 'PROPN'), ('Pool', 'PROPN'), ('Tears', 'PROPN'), ('CHAPTER', 'PROPN'), ('III', 'PROPN'), ('CaucusRace', 'PROPN'), ('Long', 'PROPN'), ('Tale', 'PROPN'), ('CHAPTER', 'NOUN'), ('IV', 'PROPN'), ('Rabbit', 'PROPN'), ('Sends', 'VERB'), ('Little', 'PROPN'), ('Bill', 'PROPN'), ('CHAPTER', 'NOUN'), ('V', 'PROPN'), ('Advice', 'PROPN'), ('Caterpillar', 'PROPN'), ('CHAPTER', 'PROPN'), ('VI', 'PROPN'), ('Pig', 'PROPN'), ('Pepper', 'PROPN'), ('CHAPTER', 'NOUN'), ('VII', 'PROPN'), ('Mad', 'PROPN'), ('TeaParty', 'PROPN'), ('CHAPTER', 'NOUN'), ('VIII', 'PROPN'), ('Queens', 'PROPN'), ('CroquetGround', 'PROPN'), ('CHAPTER', 'NOUN'), ('IX', 'PROPN'), ('Mock', 'PROPN'), ('Turtles', 'PROPN'), ('Story', 'PROPN'), ('CHAPTER', 'PROPN')]\n",
            "\n",
            "üìñ Livre 2 - 50 premiers mots avec leurs POS tags :\n",
            "[('EBOOK', 'VERB'), ('Illustration', 'PROPN'), ('LOOKINGGLASS', 'PROPN'), ('Alice', 'PROPN'), ('Found', 'VERB'), ('Lewis', 'PROPN'), ('Carroll', 'PROPN'), ('Millennium', 'PROPN'), ('Fulcrum', 'PROPN'), ('Edition', 'PROPN'), ('DRAMATIS', 'PROPN'), ('PERSON', 'NOUN'), ('arranged', 'VERB'), ('commencement', 'NOUN'), ('game', 'NOUN'), ('WHITE', 'PROPN'), ('RED', 'PROPN'), ('PIECES', 'PROPN'), ('PAWNS', 'NOUN'), ('PAWNS', 'VERB'), ('PIECES', 'NOUN'), ('Tweedledee', 'PROPN'), ('Daisy', 'PROPN'), ('Daisy', 'PROPN'), ('Humpty', 'PROPN'), ('Dumpty', 'PROPN'), ('Unicorn', 'PROPN'), ('Haigha', 'PROPN'), ('Messenger', 'PROPN'), ('Carpenter', 'PROPN'), ('Sheep', 'PROPN'), ('Oyster', 'PROPN'), ('Oyster', 'PROPN'), ('Walrus', 'PROPN'), ('W', 'PROPN'), ('Queen', 'PROPN'), ('Lily', 'PROPN'), ('Tigerlily', 'PROPN'), ('R', 'NOUN'), ('Queen', 'PROPN'), ('W', 'PROPN'), ('King', 'PROPN'), ('Fawn', 'PROPN'), ('Rose', 'PROPN'), ('R', 'NOUN'), ('King', 'PROPN'), ('Aged', 'ADJ'), ('man', 'NOUN'), ('Oyster', 'PROPN'), ('Oyster', 'PROPN')]\n",
            "\n",
            "üìñ Livre 3 - 50 premiers mots avec leurs POS tags :\n",
            "[('EBOOK', 'ADJ'), ('HYACINTH', 'PROPN'), ('George', 'PROPN'), ('Birmingham', 'PROPN'), ('CHAPTER', 'PROPN'), ('year', 'NOUN'), ('thereabouts', 'VERB'), ('religious', 'ADJ'), ('charitable', 'ADJ'), ('society', 'NOUN'), ('England', 'PROPN'), ('seized', 'VERB'), ('desire', 'NOUN'), ('convert', 'VERB'), ('Irish', 'PROPN'), ('Roman', 'PROPN'), ('Catholics', 'PROPN'), ('Protestant', 'ADJ'), ('faith', 'NOUN'), ('clear', 'ADJ'), ('everyone', 'PRON'), ('experience', 'VERB'), ('missionary', 'ADJ'), ('societies', 'NOUN'), ('remote', 'ADJ'), ('field', 'NOUN'), ('actual', 'ADJ'), ('work', 'NOUN'), ('easier', 'AUX'), ('keep', 'VERB'), ('alive', 'ADJ'), ('interest', 'NOUN'), ('subscribers', 'NOUN'), ('mission', 'NOUN'), ('Roman', 'PROPN'), ('Catholics', 'PROPN'), ('therefore', 'ADV'), ('commenced', 'VERB'), ('western', 'ADJ'), ('portion', 'NOUN'), ('Galway', 'PROPN'), ('modern', 'ADJ'), ('tourist', 'NOUN'), ('knows', 'VERB'), ('Connemara', 'PROPN'), ('enthusiasm', 'NOUN'), ('immense', 'ADJ'), ('Elderly', 'ADJ'), ('ladies', 'NOUN'), ('often', 'ADV')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# T√©l√©charger le mod√®le anglais si ce n'est pas encore fait\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Charger le mod√®le\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Appliquer le POS tagging avec SpaCy\n",
        "pos_tagged_corpus_spacy = [\n",
        "    [(token.text, token.pos_) for token in nlp(\" \".join(tokens))]  # Convertir tokens en texte et analyser\n",
        "    for tokens in filtered_corpus\n",
        "]\n",
        "\n",
        "# Afficher les 50 premiers mots avec leurs POS tags pour chaque livre\n",
        "for i, tagged_tokens in enumerate(pos_tagged_corpus_spacy):\n",
        "    print(f\"\\nüìñ Livre {i+1} - 50 premiers mots avec leurs POS tags :\")\n",
        "    print(tagged_tokens[:50])  # Afficher les 50 premiers mots avec leur POS tag"
      ],
      "metadata": {
        "id": "gDFUU_lREC_C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}