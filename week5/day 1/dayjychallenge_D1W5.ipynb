{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Text Analysis of books using word cloud"
      ],
      "metadata": {
        "id": "PQ9ptcQ995PZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "--jaXX_d9whf"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.gutenberg.org/files/11/11-0.txt\"  # Lien vers Alice au Pays des Merveilles\n",
        "response = requests.get(url)  # On télécharge le texte\n",
        "text = response.text  # On extrait le texte brut\n",
        "\n",
        "print(text[:500])  # Afficher les 500 premiers caractères"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQgv-BF0_Pk5",
        "outputId": "3312e385-086a-450b-cec6-7f6f1800882b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** START OF THE PROJECT GUTENBERG EBOOK 11 ***\r\n",
            "[Illustration]\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Alice’s Adventures in Wonderland\r\n",
            "\r\n",
            "by Lewis Carroll\r\n",
            "\r\n",
            "THE MILLENNIUM FULCRUM EDITION 3.0\r\n",
            "\r\n",
            "Contents\r\n",
            "\r\n",
            " CHAPTER I.     Down the Rabbit-Hole\r\n",
            " CHAPTER II.    The Pool of Tears\r\n",
            " CHAPTER III.   A Caucus-Race and a Long Tale\r\n",
            " CHAPTER IV.    The Rabbit Sends in a Little Bill\r\n",
            " CHAPTER V.     Advice from a Caterpillar\r\n",
            " CHAPTER VI.    Pig and Pepper\r\n",
            " CHAPTER VII.   A Mad Tea-Party\r\n",
            " CHAPTER VIII.  The Queen’s Croquet-Ground\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re  # Bibliothèque pour manipuler le texte\n",
        "\n",
        "# Supprimer tout ce qui n'est pas une lettre ou un espace\n",
        "clean_text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "\n",
        "print(clean_text[:500])  # Afficher les 500 premiers caractères nettoyés"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KVYQCcT_wuX",
        "outputId": "fe6333b9-b835-4a82-c431-46ec2dcc34b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " START OF THE PROJECT GUTENBERG EBOOK  \r\n",
            "Illustration\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Alices Adventures in Wonderland\r\n",
            "\r\n",
            "by Lewis Carroll\r\n",
            "\r\n",
            "THE MILLENNIUM FULCRUM EDITION \r\n",
            "\r\n",
            "Contents\r\n",
            "\r\n",
            " CHAPTER I     Down the RabbitHole\r\n",
            " CHAPTER II    The Pool of Tears\r\n",
            " CHAPTER III   A CaucusRace and a Long Tale\r\n",
            " CHAPTER IV    The Rabbit Sends in a Little Bill\r\n",
            " CHAPTER V     Advice from a Caterpillar\r\n",
            " CHAPTER VI    Pig and Pepper\r\n",
            " CHAPTER VII   A Mad TeaParty\r\n",
            " CHAPTER VIII  The Queens CroquetGround\r\n",
            " CHAPTER IX    The Mock T\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_texts(urls):\n",
        "    corpus = []\n",
        "\n",
        "    for url in urls:\n",
        "        # Télécharger le texte\n",
        "        response = requests.get(url)\n",
        "        text = response.text\n",
        "\n",
        "        # Supprimer les caractères spéciaux\n",
        "        text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "\n",
        "        # Trouver la position de 'START' et '*** END'\n",
        "        start_idx = text.find('START')\n",
        "        end_idx = text.find('*** END')\n",
        "\n",
        "        # Garder uniquement le texte entre START et END\n",
        "        if start_idx != -1 and end_idx != -1:\n",
        "            text = text[start_idx+5:end_idx]\n",
        "\n",
        "        # Ajouter au corpus\n",
        "        corpus.append(text)\n",
        "\n",
        "    return corpus\n",
        "\n",
        "# Liste des URLs des livres\n",
        "urls = [\n",
        "    \"https://www.gutenberg.org/files/11/11-0.txt\",  # Alice’s Adventures in Wonderland\n",
        "    \"https://www.gutenberg.org/files/12/12-0.txt\",  # THROUGH THE LOOKING-GLASS\n",
        "    \"https://www.gutenberg.org/files/10538/10538-0.txt\"  # A Tangled Tale\n",
        "]\n",
        "\n",
        "# Charger et nettoyer les textes\n",
        "corpus = load_texts(urls)\n",
        "\n",
        "# Afficher les 200 premiers caractères de chaque livre\n",
        "for i, text in enumerate(corpus):\n",
        "    print(f\"\\nLivre {i+1} - Extrait :\")\n",
        "    print(text[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-Y7Fca69_5Q",
        "outputId": "cf9df19a-b5e7-4067-ad53-c5f237a4f996"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Livre 1 - Extrait :\n",
            " START OF THE PROJECT GUTENBERG EBOOK  \r\n",
            "Illustration\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Alices Adventures in Wonderland\r\n",
            "\r\n",
            "by Lewis Carroll\r\n",
            "\r\n",
            "THE MILLENNIUM FULCRUM EDITION \r\n",
            "\r\n",
            "Contents\r\n",
            "\r\n",
            " CHAPTER I     Down the RabbitHole\r\n",
            "\n",
            "Livre 2 - Extrait :\n",
            " START OF THE PROJECT GUTENBERG EBOOK  \r\n",
            "Illustration\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "THROUGH THE LOOKINGGLASS\r\n",
            "\r\n",
            "And What Alice Found There\r\n",
            "\r\n",
            "By Lewis Carroll\r\n",
            "\r\n",
            "The Millennium Fulcrum Edition \r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "DRAMATIS PERSON\r\n",
            "A\n",
            "\n",
            "Livre 3 - Extrait :\n",
            " START OF THE PROJECT GUTENBERG EBOOK  \r\n",
            "\r\n",
            "HYACINTH\r\n",
            "\r\n",
            "By George A Birmingham\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "CHAPTER I\r\n",
            "\r\n",
            "In the year  or thereabouts religious and charitable society in\r\n",
            "England was seized with a desir\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Trouver la position des marqueurs START et END\n",
        "    start_idx = text.find(\"START OF THE PROJECT\")\n",
        "    end_idx = text.find(\"*** END\")\n",
        "\n",
        "    # Vérifier que les marqueurs existent\n",
        "    if start_idx != -1 and end_idx != -1:\n",
        "        text = text[start_idx+30:end_idx]  # On garde uniquement la partie utile\n",
        "\n",
        "    # Supprimer les caractères inutiles (ponctuation, symboles)\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Appliquer la fonction à chaque livre\n",
        "cleaned_corpus = [clean_text(text) for text in [text]]  # Si plusieurs livres, on boucle dessus\n",
        "\n",
        "# Afficher les 500 premiers caractères du texte nettoyé\n",
        "print(cleaned_corpus[0][:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHWc-t2srU-h",
        "outputId": "0c8dbd74-8196-4266-d72d-b7c2f0bd2bca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " START OF THE PROJECT GUTENBERG EBOOK  \r\n",
            "\r\n",
            "HYACINTH\r\n",
            "\r\n",
            "By George A Birmingham\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "CHAPTER I\r\n",
            "\r\n",
            "In the year  or thereabouts religious and charitable society in\r\n",
            "England was seized with a desire to convert Irish Roman Catholics to\r\n",
            "the Protestant faith It is clear to everyone with any experience of\r\n",
            "missionary societies that the more remote the field of actual work the\r\n",
            "easier it is to keep alive the interest of subscribers The mission to\r\n",
            "Roman Catholics therefore commenced in that weste\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, text in enumerate([text]):  # Si plusieurs livres, remplace [text] par ton corpus\n",
        "    start_idx = text.find(\"START OF THE PROJECT\")\n",
        "    end_idx = text.find(\"*** END\")\n",
        "    print(f\"Livre {i+1} - Index de START : {start_idx}, Index de END : {end_idx}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2OosWFWyfQa",
        "outputId": "522f12e4-06f3-4898-89f2-f5ac83996946"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Livre 1 - Index de START : 1, Index de END : -1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[-500:])  # Afficher les 500 derniers caractères du texte"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVMu9wh2yxoy",
        "outputId": "e4e48adc-14b2-428f-860d-a1d4f25f5fe3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aving fine friends like that if they wont get\r\n",
            "you sent to a place like Carrowkeel that never another minister but\r\n",
            "yourself would as much as eat his dinner in twice if he could help it\r\n",
            "\r\n",
            "Hyacinth glanced doubtfully at Marion The child lay quiet in her arms\r\n",
            "She slept uncomfortably It was clear that she had not cared to listen\r\n",
            "to the conversation of the two men\r\n",
            "\r\n",
            "THE END\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "End of the Project Gutenberg EBook of Hyacinth by George A Birmingham\r\n",
            "\r\n",
            " END OF THE PROJECT GUTENBERG EBOOK  \r\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "reajuste le code pour trouve la fin"
      ],
      "metadata": {
        "id": "etpQ9jdRzJcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"📚 Nombre de livres chargés : {len(corpus)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mc9SdQG-z_Ii",
        "outputId": "3fa3895e-6c80-4713-ddbf-0595def72bb3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Nombre de livres chargés : 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Trouver la position du début de l’histoire\n",
        "    start_idx = text.find(\"START OF THE PROJECT\")\n",
        "\n",
        "    # Essayer plusieurs variantes pour détecter la fin\n",
        "    end_variants = [\"THE END\", \"End of the Project Gutenberg\", \"END OF THE PROJECT GUTENBERG EBOOK\"]\n",
        "    end_idx = -1\n",
        "\n",
        "    for variant in end_variants:\n",
        "        found_idx = text.find(variant)\n",
        "        if found_idx != -1:\n",
        "            end_idx = found_idx\n",
        "            break  # On prend le premier qui fonctionne\n",
        "\n",
        "    # Vérifier que les marqueurs existent\n",
        "    if start_idx != -1 and end_idx != -1:\n",
        "        text = text[start_idx+30:end_idx]  # On garde uniquement la partie utile\n",
        "\n",
        "    # Supprimer la ponctuation et caractères inutiles\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Appliquer la fonction à chaque livre du corpus\n",
        "cleaned_corpus = [clean_text(text) for text in corpus]\n",
        "\n",
        "# Vérifier si nous avons bien 3 livres après nettoyage\n",
        "print(f\" Nombre de livres après nettoyage : {len(cleaned_corpus)}\")\n",
        "\n",
        "# Afficher les 200 premiers caractères de chaque livre après nettoyage\n",
        "for i, text in enumerate(cleaned_corpus):\n",
        "    print(f\"\\n Livre {i+1} - Extrait après nettoyage :\")\n",
        "    print(text[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSKNA2h60hVZ",
        "outputId": "2259afa1-629b-4473-af53-77ccabc26087"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Nombre de livres après nettoyage : 3\n",
            "\n",
            " Livre 1 - Extrait après nettoyage :\n",
            " EBOOK  \r\n",
            "Illustration\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Alices Adventures in Wonderland\r\n",
            "\r\n",
            "by Lewis Carroll\r\n",
            "\r\n",
            "THE MILLENNIUM FULCRUM EDITION \r\n",
            "\r\n",
            "Contents\r\n",
            "\r\n",
            " CHAPTER I     Down the RabbitHole\r\n",
            " CHAPTER II    The Pool of Tea\n",
            "\n",
            " Livre 2 - Extrait après nettoyage :\n",
            " EBOOK  \r\n",
            "Illustration\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "THROUGH THE LOOKINGGLASS\r\n",
            "\r\n",
            "And What Alice Found There\r\n",
            "\r\n",
            "By Lewis Carroll\r\n",
            "\r\n",
            "The Millennium Fulcrum Edition \r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "DRAMATIS PERSON\r\n",
            "As arranged before commencement \n",
            "\n",
            " Livre 3 - Extrait après nettoyage :\n",
            " EBOOK  \r\n",
            "\r\n",
            "HYACINTH\r\n",
            "\r\n",
            "By George A Birmingham\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "CHAPTER I\r\n",
            "\r\n",
            "In the year  or thereabouts religious and charitable society in\r\n",
            "England was seized with a desire to convert Irish Roman Cathol\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Appliquer la tokenisation aux 3 livres"
      ],
      "metadata": {
        "id": "oq-Tee2Q13xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # Télécharger le tokenizer\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvGzZflM1xQT",
        "outputId": "b2676f6e-10ba-48d3-f9a5-2fcd387cb56d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokeniser chaque livre\n",
        "tokenized_corpus = [word_tokenize(text) for text in cleaned_corpus]\n",
        "\n",
        "# Afficher les 150 premiers tokens de chaque livre\n",
        "for i, tokens in enumerate(tokenized_corpus):\n",
        "    print(f\"\\n Livre {i+1} - 150 premiers tokens :\")\n",
        "    print(tokens[:150])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxcuYUJe1yjZ",
        "outputId": "24211f1d-e9b3-4ee9-dce0-3fb109fb51be"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Livre 1 - 150 premiers tokens :\n",
            "['EBOOK', 'Illustration', 'Alices', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', 'THE', 'MILLENNIUM', 'FULCRUM', 'EDITION', 'Contents', 'CHAPTER', 'I', 'Down', 'the', 'RabbitHole', 'CHAPTER', 'II', 'The', 'Pool', 'of', 'Tears', 'CHAPTER', 'III', 'A', 'CaucusRace', 'and', 'a', 'Long', 'Tale', 'CHAPTER', 'IV', 'The', 'Rabbit', 'Sends', 'in', 'a', 'Little', 'Bill', 'CHAPTER', 'V', 'Advice', 'from', 'a', 'Caterpillar', 'CHAPTER', 'VI', 'Pig', 'and', 'Pepper', 'CHAPTER', 'VII', 'A', 'Mad', 'TeaParty', 'CHAPTER', 'VIII', 'The', 'Queens', 'CroquetGround', 'CHAPTER', 'IX', 'The', 'Mock', 'Turtles', 'Story', 'CHAPTER', 'X', 'The', 'Lobster', 'Quadrille', 'CHAPTER', 'XI', 'Who', 'Stole', 'the', 'Tarts', 'CHAPTER', 'XII', 'Alices', 'Evidence', 'CHAPTER', 'I', 'Down', 'the', 'RabbitHole', 'Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', 'and', 'of', 'having', 'nothing', 'to', 'do', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', 'and', 'what', 'is', 'the', 'use', 'of', 'a', 'book', 'thought', 'Alice', 'without', 'pictures', 'or', 'conversations', 'So', 'she', 'was', 'considering']\n",
            "\n",
            " Livre 2 - 150 premiers tokens :\n",
            "['EBOOK', 'Illustration', 'THROUGH', 'THE', 'LOOKINGGLASS', 'And', 'What', 'Alice', 'Found', 'There', 'By', 'Lewis', 'Carroll', 'The', 'Millennium', 'Fulcrum', 'Edition', 'DRAMATIS', 'PERSON', 'As', 'arranged', 'before', 'commencement', 'of', 'game', 'WHITE', 'RED', 'PIECES', 'PAWNS', 'PAWNS', 'PIECES', 'Tweedledee', 'Daisy', 'Daisy', 'Humpty', 'Dumpty', 'Unicorn', 'Haigha', 'Messenger', 'Carpenter', 'Sheep', 'Oyster', 'Oyster', 'Walrus', 'W', 'Queen', 'Lily', 'Tigerlily', 'R', 'Queen', 'W', 'King', 'Fawn', 'Rose', 'R', 'King', 'Aged', 'man', 'Oyster', 'Oyster', 'Crow', 'W', 'Knight', 'Hatta', 'Frog', 'R', 'Knight', 'Tweedledum', 'Daisy', 'Daisy', 'Lion', 'RED', 'Illustration', 'chessboard', 'WHITE', 'White', 'Pawn', 'Alice', 'to', 'play', 'and', 'win', 'in', 'eleven', 'moves', 'Alice', 'meets', 'R', 'Q', 'R', 'Q', 'to', 'K', 'Rs', 'th', 'Alice', 'through', 'Qs', 'd', 'by', 'railway', 'to', 'th', 'Tweedledum', 'and', 'Tweedledee', 'W', 'Q', 'to', 'Q', 'Bs', 'th', 'after', 'shawl', 'Alice', 'meets', 'W', 'Q', 'with', 'shawl', 'W', 'Q', 'to', 'QBs', 'th', 'becomes', 'sheep', 'Alice', 'to', 'Qs', 'th', 'shop', 'river', 'shop', 'W', 'Q', 'to', 'K', 'Bs', 'th', 'leaves', 'egg', 'on', 'shelf', 'Alice', 'to', 'Qs', 'th', 'Humpty', 'Dumpty']\n",
            "\n",
            " Livre 3 - 150 premiers tokens :\n",
            "['EBOOK', 'HYACINTH', 'By', 'George', 'A', 'Birmingham', 'CHAPTER', 'I', 'In', 'the', 'year', 'or', 'thereabouts', 'religious', 'and', 'charitable', 'society', 'in', 'England', 'was', 'seized', 'with', 'a', 'desire', 'to', 'convert', 'Irish', 'Roman', 'Catholics', 'to', 'the', 'Protestant', 'faith', 'It', 'is', 'clear', 'to', 'everyone', 'with', 'any', 'experience', 'of', 'missionary', 'societies', 'that', 'the', 'more', 'remote', 'the', 'field', 'of', 'actual', 'work', 'the', 'easier', 'it', 'is', 'to', 'keep', 'alive', 'the', 'interest', 'of', 'subscribers', 'The', 'mission', 'to', 'Roman', 'Catholics', 'therefore', 'commenced', 'in', 'that', 'western', 'portion', 'of', 'Galway', 'which', 'the', 'modern', 'tourist', 'knows', 'as', 'Connemara', 'and', 'the', 'enthusiasm', 'was', 'immense', 'Elderly', 'ladies', 'often', 'with', 'titles', 'were', 'energetic', 'in', 'the', 'cause', 'of', 'the', 'new', 'reformation', 'Young', 'ladies', 'some', 'of', 'them', 'very', 'attractive', 'collected', 'money', 'from', 'their', 'brothers', 'and', 'admirers', 'Statesmen', 'and', 'Bishops', 'headed', 'the', 'subscriptionlists', 'and', 'influential', 'committees', 'earnestly', 'debated', 'plans', 'for', 'spending', 'the', 'money', 'which', 'poured', 'in', 'Faith', 'in', 'the', 'efficacy', 'of', 'money', 'handled', 'by', 'influential', 'committees', 'is', 'one', 'of', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppression des stopwords"
      ],
      "metadata": {
        "id": "9-Ua3Fc33BmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78Mz6JvO3TQS",
        "outputId": "11867379-07ae-4afa-c8f7-d1084d5ff90b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger les stopwords en anglais\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Vérifier quelques stopwords\n",
        "print(f\"Quelques stopwords en anglais : {list(stop_words)[:10]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYt5b9Dq3CbB",
        "outputId": "5623f1c3-f393-4cdd-e69a-3c860d884a55"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quelques stopwords en anglais : ['them', 'do', 'nor', 'on', 'very', \"he's\", 'shan', 'theirs', 'will', \"you'll\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Supprimer les stopwords de chaque livre\n",
        "filtered_corpus = [\n",
        "    [word for word in tokens if word.lower() not in stop_words]  # Convertir en minuscule et filtrer\n",
        "    for tokens in tokenized_corpus\n",
        "]\n",
        "\n",
        "# Afficher les 150 premiers tokens après suppression des stopwords\n",
        "for i, tokens in enumerate(filtered_corpus):\n",
        "    print(f\"\\nLivre {i+1} - 150 premiers tokens après suppression des stopwords :\")\n",
        "    print(tokens[:150])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE-0g4l_3pdJ",
        "outputId": "665584f1-124e-4355-b0d7-426d28cbd786"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Livre 1 - 150 premiers tokens après suppression des stopwords :\n",
            "['EBOOK', 'Illustration', 'Alices', 'Adventures', 'Wonderland', 'Lewis', 'Carroll', 'MILLENNIUM', 'FULCRUM', 'EDITION', 'Contents', 'CHAPTER', 'RabbitHole', 'CHAPTER', 'II', 'Pool', 'Tears', 'CHAPTER', 'III', 'CaucusRace', 'Long', 'Tale', 'CHAPTER', 'IV', 'Rabbit', 'Sends', 'Little', 'Bill', 'CHAPTER', 'V', 'Advice', 'Caterpillar', 'CHAPTER', 'VI', 'Pig', 'Pepper', 'CHAPTER', 'VII', 'Mad', 'TeaParty', 'CHAPTER', 'VIII', 'Queens', 'CroquetGround', 'CHAPTER', 'IX', 'Mock', 'Turtles', 'Story', 'CHAPTER', 'X', 'Lobster', 'Quadrille', 'CHAPTER', 'XI', 'Stole', 'Tarts', 'CHAPTER', 'XII', 'Alices', 'Evidence', 'CHAPTER', 'RabbitHole', 'Alice', 'beginning', 'get', 'tired', 'sitting', 'sister', 'bank', 'nothing', 'twice', 'peeped', 'book', 'sister', 'reading', 'pictures', 'conversations', 'use', 'book', 'thought', 'Alice', 'without', 'pictures', 'conversations', 'considering', 'mind', 'well', 'could', 'hot', 'day', 'made', 'feel', 'sleepy', 'stupid', 'whether', 'pleasure', 'making', 'daisychain', 'would', 'worth', 'trouble', 'getting', 'picking', 'daisies', 'suddenly', 'White', 'Rabbit', 'pink', 'eyes', 'ran', 'close', 'nothing', 'remarkable', 'Alice', 'think', 'much', 'way', 'hear', 'Rabbit', 'say', 'Oh', 'dear', 'Oh', 'dear', 'shall', 'late', 'thought', 'afterwards', 'occurred', 'ought', 'wondered', 'time', 'seemed', 'quite', 'natural', 'Rabbit', 'actually', 'took', 'watch', 'waistcoatpocket', 'looked', 'hurried', 'Alice', 'started', 'feet', 'flashed', 'across', 'mind', 'never']\n",
            "\n",
            "Livre 2 - 150 premiers tokens après suppression des stopwords :\n",
            "['EBOOK', 'Illustration', 'LOOKINGGLASS', 'Alice', 'Found', 'Lewis', 'Carroll', 'Millennium', 'Fulcrum', 'Edition', 'DRAMATIS', 'PERSON', 'arranged', 'commencement', 'game', 'WHITE', 'RED', 'PIECES', 'PAWNS', 'PAWNS', 'PIECES', 'Tweedledee', 'Daisy', 'Daisy', 'Humpty', 'Dumpty', 'Unicorn', 'Haigha', 'Messenger', 'Carpenter', 'Sheep', 'Oyster', 'Oyster', 'Walrus', 'W', 'Queen', 'Lily', 'Tigerlily', 'R', 'Queen', 'W', 'King', 'Fawn', 'Rose', 'R', 'King', 'Aged', 'man', 'Oyster', 'Oyster', 'Crow', 'W', 'Knight', 'Hatta', 'Frog', 'R', 'Knight', 'Tweedledum', 'Daisy', 'Daisy', 'Lion', 'RED', 'Illustration', 'chessboard', 'WHITE', 'White', 'Pawn', 'Alice', 'play', 'win', 'eleven', 'moves', 'Alice', 'meets', 'R', 'Q', 'R', 'Q', 'K', 'Rs', 'th', 'Alice', 'Qs', 'railway', 'th', 'Tweedledum', 'Tweedledee', 'W', 'Q', 'Q', 'Bs', 'th', 'shawl', 'Alice', 'meets', 'W', 'Q', 'shawl', 'W', 'Q', 'QBs', 'th', 'becomes', 'sheep', 'Alice', 'Qs', 'th', 'shop', 'river', 'shop', 'W', 'Q', 'K', 'Bs', 'th', 'leaves', 'egg', 'shelf', 'Alice', 'Qs', 'th', 'Humpty', 'Dumpty', 'W', 'Q', 'Q', 'Bs', 'th', 'flying', 'R', 'Kt', 'Alice', 'Qs', 'th', 'forest', 'R', 'Kt', 'Ks', 'nd', 'ch', 'WKt', 'takes', 'RKt', 'W', 'Kt', 'K', 'Bs', 'th', 'Alice', 'Qs']\n",
            "\n",
            "Livre 3 - 150 premiers tokens après suppression des stopwords :\n",
            "['EBOOK', 'HYACINTH', 'George', 'Birmingham', 'CHAPTER', 'year', 'thereabouts', 'religious', 'charitable', 'society', 'England', 'seized', 'desire', 'convert', 'Irish', 'Roman', 'Catholics', 'Protestant', 'faith', 'clear', 'everyone', 'experience', 'missionary', 'societies', 'remote', 'field', 'actual', 'work', 'easier', 'keep', 'alive', 'interest', 'subscribers', 'mission', 'Roman', 'Catholics', 'therefore', 'commenced', 'western', 'portion', 'Galway', 'modern', 'tourist', 'knows', 'Connemara', 'enthusiasm', 'immense', 'Elderly', 'ladies', 'often', 'titles', 'energetic', 'cause', 'new', 'reformation', 'Young', 'ladies', 'attractive', 'collected', 'money', 'brothers', 'admirers', 'Statesmen', 'Bishops', 'headed', 'subscriptionlists', 'influential', 'committees', 'earnestly', 'debated', 'plans', 'spending', 'money', 'poured', 'Faith', 'efficacy', 'money', 'handled', 'influential', 'committees', 'one', 'characteristics', 'English', 'people', 'particular', 'case', 'seemed', 'faith', 'justified', 'results', 'encouraging', 'reports', 'sent', 'headquarters', 'Connemara', 'appeared', 'converts', 'flocking', 'schools', 'missionaries', 'filled', 'overflowing', 'matter', 'education', 'circumstances', 'favoured', 'new', 'reformation', 'leonine', 'John', 'McHale', 'Papal', 'Archbishop', 'Tuam', 'pursued', 'policy', 'drove', 'children', 'flock', 'mission', 'schools', 'kind', 'education', 'available', 'humorous', 'English', 'statesman', 'called', 'national', 'seem', 'Archbishop', 'desirable', 'Irish', 'boy', 'beaten', 'speaking', 'language', 'rewarded', 'calling', 'happy', 'English', 'child', 'refused', 'allow', 'building', 'national', 'schools', 'diocese', 'thus', 'left']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vérifier la suppression"
      ],
      "metadata": {
        "id": "eIN0p8a04AbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Liste de stopwords à vérifier\n",
        "words_to_check = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ourselves\"]\n",
        "\n",
        "for i, tokens in enumerate(filtered_corpus):\n",
        "    print(f\"\\n Livre {i+1} - Vérification des stopwords supprimés :\")\n",
        "    for word in words_to_check:\n",
        "        count = tokens.count(word)\n",
        "        print(f\"  {word}: {count} fois\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgterl3U3_a6",
        "outputId": "346b94df-3eca-43e9-b74d-41ca43f9f613"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Livre 1 - Vérification des stopwords supprimés :\n",
            "  i: 0 fois\n",
            "  me: 0 fois\n",
            "  my: 0 fois\n",
            "  myself: 0 fois\n",
            "  we: 0 fois\n",
            "  our: 0 fois\n",
            "  ourselves: 0 fois\n",
            "\n",
            " Livre 2 - Vérification des stopwords supprimés :\n",
            "  i: 0 fois\n",
            "  me: 0 fois\n",
            "  my: 0 fois\n",
            "  myself: 0 fois\n",
            "  we: 0 fois\n",
            "  our: 0 fois\n",
            "  ourselves: 0 fois\n",
            "\n",
            " Livre 3 - Vérification des stopwords supprimés :\n",
            "  i: 0 fois\n",
            "  me: 0 fois\n",
            "  my: 0 fois\n",
            "  myself: 0 fois\n",
            "  we: 0 fois\n",
            "  our: 0 fois\n",
            "  ourselves: 0 fois\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "réduire les mots à leur racine (stemming)."
      ],
      "metadata": {
        "id": "4QHIQGu64pXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialiser le stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Appliquer le stemming à chaque mot dans les livres\n",
        "stemmed_corpus = [\n",
        "    [stemmer.stem(word) for word in tokens]  # Appliquer le stemming\n",
        "    for tokens in filtered_corpus\n",
        "]\n",
        "\n",
        "# Afficher les 50 premiers mots stemmés de chaque livre\n",
        "for i, tokens in enumerate(stemmed_corpus):\n",
        "    print(f\"\\n Livre {i+1} - 50 premiers mots après stemming :\")\n",
        "    print(tokens[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jN25gUD84qm6",
        "outputId": "a475efe4-c10d-4781-89d9-1e0612be924c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Livre 1 - 50 premiers mots après stemming :\n",
            "['ebook', 'illustr', 'alic', 'adventur', 'wonderland', 'lewi', 'carrol', 'millennium', 'fulcrum', 'edit', 'content', 'chapter', 'rabbithol', 'chapter', 'ii', 'pool', 'tear', 'chapter', 'iii', 'caucusrac', 'long', 'tale', 'chapter', 'iv', 'rabbit', 'send', 'littl', 'bill', 'chapter', 'v', 'advic', 'caterpillar', 'chapter', 'vi', 'pig', 'pepper', 'chapter', 'vii', 'mad', 'teaparti', 'chapter', 'viii', 'queen', 'croquetground', 'chapter', 'ix', 'mock', 'turtl', 'stori', 'chapter']\n",
            "\n",
            " Livre 2 - 50 premiers mots après stemming :\n",
            "['ebook', 'illustr', 'lookingglass', 'alic', 'found', 'lewi', 'carrol', 'millennium', 'fulcrum', 'edit', 'dramati', 'person', 'arrang', 'commenc', 'game', 'white', 'red', 'piec', 'pawn', 'pawn', 'piec', 'tweedlede', 'daisi', 'daisi', 'humpti', 'dumpti', 'unicorn', 'haigha', 'messeng', 'carpent', 'sheep', 'oyster', 'oyster', 'walru', 'w', 'queen', 'lili', 'tigerlili', 'r', 'queen', 'w', 'king', 'fawn', 'rose', 'r', 'king', 'age', 'man', 'oyster', 'oyster']\n",
            "\n",
            " Livre 3 - 50 premiers mots après stemming :\n",
            "['ebook', 'hyacinth', 'georg', 'birmingham', 'chapter', 'year', 'thereabout', 'religi', 'charit', 'societi', 'england', 'seiz', 'desir', 'convert', 'irish', 'roman', 'cathol', 'protest', 'faith', 'clear', 'everyon', 'experi', 'missionari', 'societi', 'remot', 'field', 'actual', 'work', 'easier', 'keep', 'aliv', 'interest', 'subscrib', 'mission', 'roman', 'cathol', 'therefor', 'commenc', 'western', 'portion', 'galway', 'modern', 'tourist', 'know', 'connemara', 'enthusiasm', 'immens', 'elderli', 'ladi', 'often']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Télécharger le modèle anglais si ce n'est pas encore fait\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Charger le modèle spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAi1D4l_8c9j",
        "outputId": "bf716362-a126-497b-9ae4-e774afa3a625"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Appliquer la lemmatisation aux livres"
      ],
      "metadata": {
        "id": "DkE9DxAQT6DP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Appliquer la lemmatisation à chaque livre\n",
        "lemmatized_corpus = [\n",
        "    [token.lemma_ for token in nlp(\" \".join(tokens))]  # Convertir tokens en texte et appliquer spaCy\n",
        "    for tokens in filtered_corpus\n",
        "]\n",
        "\n",
        "# Afficher les 50 premiers mots lemmatisés de chaque livre\n",
        "for i, tokens in enumerate(lemmatized_corpus):\n",
        "    print(f\"\\n Livre {i+1} - 50 premiers mots après lemmatisation :\")\n",
        "    print(tokens[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2STbGAToTToT",
        "outputId": "c405bc0d-dcb8-425e-e5b3-efb01590fa2f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Livre 1 - 50 premiers mots après lemmatisation :\n",
            "['EBOOK', 'Illustration', 'Alices', 'Adventures', 'Wonderland', 'Lewis', 'Carroll', 'MILLENNIUM', 'FULCRUM', 'EDITION', 'Contents', 'CHAPTER', 'RabbitHole', 'chapter', 'II', 'Pool', 'Tears', 'CHAPTER', 'III', 'CaucusRace', 'Long', 'Tale', 'chapter', 'IV', 'Rabbit', 'send', 'Little', 'Bill', 'chapter', 'V', 'Advice', 'Caterpillar', 'CHAPTER', 'VI', 'Pig', 'Pepper', 'chapter', 'VII', 'Mad', 'TeaParty', 'chapter', 'VIII', 'Queens', 'CroquetGround', 'chapter', 'IX', 'Mock', 'Turtles', 'Story', 'CHAPTER']\n",
            "\n",
            " Livre 2 - 50 premiers mots après lemmatisation :\n",
            "['EBOOK', 'Illustration', 'LOOKINGGLASS', 'Alice', 'find', 'Lewis', 'Carroll', 'Millennium', 'Fulcrum', 'Edition', 'DRAMATIS', 'person', 'arrange', 'commencement', 'game', 'WHITE', 'RED', 'PIECES', 'pawns', 'pawn', 'piece', 'Tweedledee', 'Daisy', 'Daisy', 'Humpty', 'Dumpty', 'Unicorn', 'Haigha', 'Messenger', 'Carpenter', 'Sheep', 'Oyster', 'Oyster', 'Walrus', 'W', 'Queen', 'Lily', 'Tigerlily', 'r', 'Queen', 'W', 'King', 'Fawn', 'Rose', 'r', 'King', 'aged', 'man', 'Oyster', 'Oyster']\n",
            "\n",
            " Livre 3 - 50 premiers mots après lemmatisation :\n",
            "['ebook', 'HYACINTH', 'George', 'Birmingham', 'CHAPTER', 'year', 'thereabout', 'religious', 'charitable', 'society', 'England', 'seize', 'desire', 'convert', 'Irish', 'Roman', 'Catholics', 'protestant', 'faith', 'clear', 'everyone', 'experience', 'missionary', 'society', 'remote', 'field', 'actual', 'work', 'easier', 'keep', 'alive', 'interest', 'subscriber', 'mission', 'Roman', 'Catholics', 'therefore', 'commence', 'western', 'portion', 'Galway', 'modern', 'tourist', 'know', 'Connemara', 'enthusiasm', 'immense', 'elderly', 'lady', 'often']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "comparaison stemming et lemmatisation :--> la lemmatisation est plus précise et linguistiquement correcte.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZPTvkQjzVDq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparer les 50 premiers mots après stemming et lemmatisation pour le premier livre\n",
        "print(\"\\n Comparaison des 50 premiers mots - Livre 1\")\n",
        "print(f\"{'Stemmed':<20} {'Lemmatized':<20}\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "for stem, lemma in zip(stemmed_corpus[0][:50], lemmatized_corpus[0][:50]):\n",
        "    print(f\"{stem:<20} {lemma:<20}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQA9k1X8VXDB",
        "outputId": "3f8f749f-aea9-4df6-8842-bd7b7fcebef3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Comparaison des 50 premiers mots - Livre 1\n",
            "Stemmed              Lemmatized          \n",
            "========================================\n",
            "ebook                EBOOK               \n",
            "illustr              Illustration        \n",
            "alic                 Alices              \n",
            "adventur             Adventures          \n",
            "wonderland           Wonderland          \n",
            "lewi                 Lewis               \n",
            "carrol               Carroll             \n",
            "millennium           MILLENNIUM          \n",
            "fulcrum              FULCRUM             \n",
            "edit                 EDITION             \n",
            "content              Contents            \n",
            "chapter              CHAPTER             \n",
            "rabbithol            RabbitHole          \n",
            "chapter              chapter             \n",
            "ii                   II                  \n",
            "pool                 Pool                \n",
            "tear                 Tears               \n",
            "chapter              CHAPTER             \n",
            "iii                  III                 \n",
            "caucusrac            CaucusRace          \n",
            "long                 Long                \n",
            "tale                 Tale                \n",
            "chapter              chapter             \n",
            "iv                   IV                  \n",
            "rabbit               Rabbit              \n",
            "send                 send                \n",
            "littl                Little              \n",
            "bill                 Bill                \n",
            "chapter              chapter             \n",
            "v                    V                   \n",
            "advic                Advice              \n",
            "caterpillar          Caterpillar         \n",
            "chapter              CHAPTER             \n",
            "vi                   VI                  \n",
            "pig                  Pig                 \n",
            "pepper               Pepper              \n",
            "chapter              chapter             \n",
            "vii                  VII                 \n",
            "mad                  Mad                 \n",
            "teaparti             TeaParty            \n",
            "chapter              chapter             \n",
            "viii                 VIII                \n",
            "queen                Queens              \n",
            "croquetground        CroquetGround       \n",
            "chapter              chapter             \n",
            "ix                   IX                  \n",
            "mock                 Mock                \n",
            "turtl                Turtles             \n",
            "stori                Story               \n",
            "chapter              CHAPTER             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "identify POS tags od."
      ],
      "metadata": {
        "id": "41t_7xDlVkBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')  # Télécharger le modèle de POS tagging"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjieF6YzWhHz",
        "outputId": "1bbea18c-1640-4190-dc99-629f0952b4f1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n"
      ],
      "metadata": {
        "id": "TN7JZH82Y-oO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Supprimer et retélécharger le modèle\n",
        "nltk.data.path.append('/usr/local/nltk_data')"
      ],
      "metadata": {
        "id": "shupfuz8ZB0X"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger', download_dir='/usr/local/nltk_data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61Be2RXVZEF5",
        "outputId": "f8bc7055-f2aa-4625-9d33-7312bf041547"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /usr/local/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Télécharger le modèle anglais si ce n'est pas encore fait\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Charger le modèle\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Appliquer le POS tagging avec SpaCy\n",
        "pos_tagged_corpus_spacy = [\n",
        "    [(token.text, token.pos_) for token in nlp(\" \".join(tokens))]  # Convertir tokens en texte et analyser\n",
        "    for tokens in filtered_corpus\n",
        "]\n",
        "\n",
        "# Afficher les 50 premiers mots avec leurs POS tags pour chaque livre\n",
        "for i, tagged_tokens in enumerate(pos_tagged_corpus_spacy):\n",
        "    print(f\"\\n Livre {i+1} - 50 premiers mots avec leurs POS tags :\")\n",
        "    print(tagged_tokens[:50])  # Afficher les 50 premiers mots avec leur POS tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "japUiQKKZW5o",
        "outputId": "ca8f121e-49fe-48f4-b243-6b55d9f46a9f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\n",
            "📖 Livre 1 - 50 premiers mots avec leurs POS tags :\n",
            "[('EBOOK', 'NOUN'), ('Illustration', 'PROPN'), ('Alices', 'PROPN'), ('Adventures', 'PROPN'), ('Wonderland', 'PROPN'), ('Lewis', 'PROPN'), ('Carroll', 'PROPN'), ('MILLENNIUM', 'PROPN'), ('FULCRUM', 'PROPN'), ('EDITION', 'PROPN'), ('Contents', 'PROPN'), ('CHAPTER', 'PROPN'), ('RabbitHole', 'PROPN'), ('CHAPTER', 'NOUN'), ('II', 'PROPN'), ('Pool', 'PROPN'), ('Tears', 'PROPN'), ('CHAPTER', 'PROPN'), ('III', 'PROPN'), ('CaucusRace', 'PROPN'), ('Long', 'PROPN'), ('Tale', 'PROPN'), ('CHAPTER', 'NOUN'), ('IV', 'PROPN'), ('Rabbit', 'PROPN'), ('Sends', 'VERB'), ('Little', 'PROPN'), ('Bill', 'PROPN'), ('CHAPTER', 'NOUN'), ('V', 'PROPN'), ('Advice', 'PROPN'), ('Caterpillar', 'PROPN'), ('CHAPTER', 'PROPN'), ('VI', 'PROPN'), ('Pig', 'PROPN'), ('Pepper', 'PROPN'), ('CHAPTER', 'NOUN'), ('VII', 'PROPN'), ('Mad', 'PROPN'), ('TeaParty', 'PROPN'), ('CHAPTER', 'NOUN'), ('VIII', 'PROPN'), ('Queens', 'PROPN'), ('CroquetGround', 'PROPN'), ('CHAPTER', 'NOUN'), ('IX', 'PROPN'), ('Mock', 'PROPN'), ('Turtles', 'PROPN'), ('Story', 'PROPN'), ('CHAPTER', 'PROPN')]\n",
            "\n",
            "📖 Livre 2 - 50 premiers mots avec leurs POS tags :\n",
            "[('EBOOK', 'VERB'), ('Illustration', 'PROPN'), ('LOOKINGGLASS', 'PROPN'), ('Alice', 'PROPN'), ('Found', 'VERB'), ('Lewis', 'PROPN'), ('Carroll', 'PROPN'), ('Millennium', 'PROPN'), ('Fulcrum', 'PROPN'), ('Edition', 'PROPN'), ('DRAMATIS', 'PROPN'), ('PERSON', 'NOUN'), ('arranged', 'VERB'), ('commencement', 'NOUN'), ('game', 'NOUN'), ('WHITE', 'PROPN'), ('RED', 'PROPN'), ('PIECES', 'PROPN'), ('PAWNS', 'NOUN'), ('PAWNS', 'VERB'), ('PIECES', 'NOUN'), ('Tweedledee', 'PROPN'), ('Daisy', 'PROPN'), ('Daisy', 'PROPN'), ('Humpty', 'PROPN'), ('Dumpty', 'PROPN'), ('Unicorn', 'PROPN'), ('Haigha', 'PROPN'), ('Messenger', 'PROPN'), ('Carpenter', 'PROPN'), ('Sheep', 'PROPN'), ('Oyster', 'PROPN'), ('Oyster', 'PROPN'), ('Walrus', 'PROPN'), ('W', 'PROPN'), ('Queen', 'PROPN'), ('Lily', 'PROPN'), ('Tigerlily', 'PROPN'), ('R', 'NOUN'), ('Queen', 'PROPN'), ('W', 'PROPN'), ('King', 'PROPN'), ('Fawn', 'PROPN'), ('Rose', 'PROPN'), ('R', 'NOUN'), ('King', 'PROPN'), ('Aged', 'ADJ'), ('man', 'NOUN'), ('Oyster', 'PROPN'), ('Oyster', 'PROPN')]\n",
            "\n",
            "📖 Livre 3 - 50 premiers mots avec leurs POS tags :\n",
            "[('EBOOK', 'ADJ'), ('HYACINTH', 'PROPN'), ('George', 'PROPN'), ('Birmingham', 'PROPN'), ('CHAPTER', 'PROPN'), ('year', 'NOUN'), ('thereabouts', 'VERB'), ('religious', 'ADJ'), ('charitable', 'ADJ'), ('society', 'NOUN'), ('England', 'PROPN'), ('seized', 'VERB'), ('desire', 'NOUN'), ('convert', 'VERB'), ('Irish', 'PROPN'), ('Roman', 'PROPN'), ('Catholics', 'PROPN'), ('Protestant', 'ADJ'), ('faith', 'NOUN'), ('clear', 'ADJ'), ('everyone', 'PRON'), ('experience', 'VERB'), ('missionary', 'ADJ'), ('societies', 'NOUN'), ('remote', 'ADJ'), ('field', 'NOUN'), ('actual', 'ADJ'), ('work', 'NOUN'), ('easier', 'AUX'), ('keep', 'VERB'), ('alive', 'ADJ'), ('interest', 'NOUN'), ('subscribers', 'NOUN'), ('mission', 'NOUN'), ('Roman', 'PROPN'), ('Catholics', 'PROPN'), ('therefore', 'ADV'), ('commenced', 'VERB'), ('western', 'ADJ'), ('portion', 'NOUN'), ('Galway', 'PROPN'), ('modern', 'ADJ'), ('tourist', 'NOUN'), ('knows', 'VERB'), ('Connemara', 'PROPN'), ('enthusiasm', 'NOUN'), ('immense', 'ADJ'), ('Elderly', 'ADJ'), ('ladies', 'NOUN'), ('often', 'ADV')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Télécharger le modèle anglais si ce n'est pas encore fait\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Charger le modèle\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Appliquer le POS tagging avec SpaCy\n",
        "pos_tagged_corpus_spacy = [\n",
        "    [(token.text, token.pos_) for token in nlp(\" \".join(tokens))]  # Convertir tokens en texte et analyser\n",
        "    for tokens in filtered_corpus\n",
        "]\n",
        "\n",
        "# Afficher les 50 premiers mots avec leurs POS tags pour chaque livre\n",
        "for i, tagged_tokens in enumerate(pos_tagged_corpus_spacy):\n",
        "    print(f\"\\n📖 Livre {i+1} - 50 premiers mots avec leurs POS tags :\")\n",
        "    print(tagged_tokens[:50])  # Afficher les 50 premiers mots avec leur POS tag"
      ],
      "metadata": {
        "id": "gDFUU_lREC_C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}