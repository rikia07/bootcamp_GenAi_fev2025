{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Daily Challenge: Simplified Self-Attention Explained"
      ],
      "metadata": {
        "id": "CiK2xJKS-SZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Simplified self-attention"
      ],
      "metadata": {
        "id": "Bkx00Dki-aYM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xg5eFJCG9-_V"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "[\n",
        "    [0.43, 0.15, 0.89], # your\n",
        "    [0.55, 0.87, 0.66], # journey\n",
        "    [0.57, 0.85, 0.64], # starts\n",
        "    [0.22, 0.58, 0.33], # with\n",
        "    [0.77, 0.25, 0.10], # one\n",
        "    [0.05, 0.80, 0.55] # step\n",
        "]\n",
        ")"
      ],
      "metadata": {
        "id": "gJx0rqgE-hN8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# connaitre la shape du tensor\n",
        "inputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_NngWmckrks",
        "outputId": "eb07fb10-c494-463b-a5b4-4299e3e15a5f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1 Computing Attention Weights for Inputs[2]:"
      ],
      "metadata": {
        "id": "3im5SUFBGJOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "query vector"
      ],
      "metadata": {
        "id": "jquA-4j_ALTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# S√©lection du Query (3e mot = \"starts\")\n",
        "query = inputs[2]\n",
        "\n",
        "# Afficher le Query s√©lectionn√© (ce=vecteur du mot)\n",
        "print(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZwB0tIR_qmF",
        "outputId": "159f5025-1b4b-4a4f-8496-57590171e475"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.5700, 0.8500, 0.6400])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1.1 Attention Score:"
      ],
      "metadata": {
        "id": "9t-A6uQkGC8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "attention score : mesure la similarit√© entre le Query (mot s√©lectionn√©) et les autres mots.plus le score est √©lev√©, plus les mots sont li√©s"
      ],
      "metadata": {
        "id": "dwT_BqOUAQWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "attn_scores_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCHb2E_XlRt0",
        "outputId": "915f2cea-d117-4c57-f9af-8184a276df82"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2.1707e-18, 7.0952e+22, 1.7748e+28, 1.8176e+31, 7.2708e+31, 5.0778e+31])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialiser un tableau pour stocker les scores d'attention\n",
        "attn_scores_2 = torch.zeros(inputs.shape[0])\n",
        "attn_scores_2"
      ],
      "metadata": {
        "id": "s5SbNL-b_rfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f37ae9e5-6bc4-408b-9901-88af225ac7cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul des scores d‚Äôattention (produit scalaire entre Query et chaque mot)\n",
        "for i, x_i in enumerate(inputs):\n",
        "  # On compare chaque mot (x_i) avec \"starts\" (query). x_i est donc le vecteur d‚Äôun mot de la phrase\n",
        "    attn_scores_2[i] = torch.dot(x_i, query)\n"
      ],
      "metadata": {
        "id": "BP2IwLUOCvA_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Ce code calcule le produit scalaire entre \"starts\" et chaque mot de la phrase."
      ],
      "metadata": {
        "id": "_dKGch-yDPt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher les scores d'attention\n",
        "print(f\"\\nScores d'attention pour le mot 'starts' :\\n{attn_scores_2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfXtUb1WC6E2",
        "outputId": "56b3d678-32b6-40d5-de59-537c6898cbd2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scores d'attention pour le mot 'starts' :\n",
            "tensor([0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->Plus le score est √©lev√©, plus le mot est proche de \"starts\""
      ],
      "metadata": {
        "id": "4mMNZWVwDTKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1.2 Attention Weights:"
      ],
      "metadata": {
        "id": "rF6drswIF9u_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F  # Pour utiliser Softmax\n"
      ],
      "metadata": {
        "id": "WdTf8cWdG_N-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax fait exactement cela :Il prend une liste de nombres et les transforme en probabilit√©s. La somme des probabilit√©s est toujours √©gale √† 1."
      ],
      "metadata": {
        "id": "g8HY4URlHKjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
        "attn_weights_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyruMKiRlrVk",
        "outputId": "1f58f6bd-f618-439a-f581-845ba22bcef3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Appliquer la fonction Softmax sur les scores d'attention\n",
        "attn_weights_2 = F.softmax(attn_scores_2, dim=0)\n",
        "#  Apr√®s Softmax, ils deviendront des probabilit√©s"
      ],
      "metadata": {
        "id": "5jlCoecjHBrN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher les poids d'attention (apr√®s Softmax)\n",
        "print(f\"\\n Poids d'attention apr√®s Softmax :\\n{attn_weights_2}\")\n",
        "# V√©rifie que la somme des poids est bien √©gale √† 1 (normalisation):\n",
        "print(f\"\\n Somme des poids d'attention : {attn_weights_2.sum()} (doit √™tre ‚âà 1)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuXm5bm2HhDW",
        "outputId": "7f455e1d-bea2-4121-f6b6-211b973a1d5f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Poids d'attention apr√®s Softmax :\n",
            "tensor([0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565])\n",
            "\n",
            " Somme des poids d'attention : 1.0000001192092896 (doit √™tre ‚âà 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 1.1.3 Context Vector:"
      ],
      "metadata": {
        "id": "xIUckMepI9Am"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "contexte vector est une combinaison pond√©r√©e des mots selon leurs poids d‚Äôattention.\n",
        "‚û° Il permet de r√©sumer toutes les informations importantes d‚Äôune phrase en un seul vecteur"
      ],
      "metadata": {
        "id": "JPY1Bac5I9pX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensuite, on fait la somme des r√©sultats pour obtenir le contexte final.\n",
        "Formule :\n",
        "           context_vector= ‚àë(attn_weights*inputs)"
      ],
      "metadata": {
        "id": "22XsQZnhJEef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul du vecteur de contexte comme somme pond√©r√©e des mots\n",
        "context_vector = torch.sum(attn_weights_2.unsqueeze(1) * inputs, dim=0)"
      ],
      "metadata": {
        "id": "L2Pt7o99JD5v"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher le vecteur de contexte\n",
        "print(context_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItnSfBgGJekv",
        "outputId": "26bca9d7-b11b-4f51-e238-d4cc4b65ebfb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4431, 0.6496, 0.5671])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "R√©sum√© : Converti les mots en vecteurs (embeddings)./Choisi un mot Query pour calculer l‚Äôattention./Calcul√© les scores d‚Äôattention avec le produit scalaire./Appliqu√© Softmax pour obtenir des poids d‚Äôattention normalis√©s./Calcul√© le vecteur de contexte, qui repr√©sente toute la phrase."
      ],
      "metadata": {
        "id": "u5SXirFYJvb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-Attention est essentiel dans les Transformers : Self-Attention analyse tous les mots en m√™me temps; \\Les Transformers utilisent plusieurs couches de Self-Attention en m√™me temps (on appelle cela Multi-Head Attention)"
      ],
      "metadata": {
        "id": "KNSlsNjFKhcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 Computing Attention Weights for All Inputs:"
      ],
      "metadata": {
        "id": "ulUAkDEiLz2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Impl√©mentation : Calculer les Scores d‚ÄôAttention pour Tous les Mots"
      ],
      "metadata": {
        "id": "ISj8AAiWLyZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul de la matrice des scores d'attention\n",
        "attn_scores_matrix = torch.matmul(inputs, inputs.T)  # Produit matriciel entre les mots"
      ],
      "metadata": {
        "id": "8qaDU1YfJlOe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Affichage de la matrice des scores\n",
        "print(attn_scores_matrix)\n",
        "# Cela va donner une \"matrice d‚Äôattention\", une table qui montre les relations entre chaque mot.\n",
        "# Chaque cellule de cette matrice repr√©sente le score d‚Äôattention entre deux mots."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_-sfqIPMAl3",
        "outputId": "c0b3bf33-fcb6-4a8b-c939-b6a4f84f8341"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On applique Softmax sur chaque ligne de la matrice (dim=1) pour obtenir les poids d‚Äôattention."
      ],
      "metadata": {
        "id": "91wK63rUMm9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F  # Pour utiliser Softmax"
      ],
      "metadata": {
        "id": "BHoYM6jAMo7I"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Appliquer Softmax pour normaliser les scores d‚Äôattention\n",
        "attn_weights_matrix = F.softmax(attn_scores_matrix, dim=1)"
      ],
      "metadata": {
        "id": "vNBxnEraMYKf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Affichage de la matrice des poids d'attention\n",
        "print(f\"\\nMatrice des poids d'attention (apr√®s Softmax) :\\n{attn_weights_matrix}\")\n",
        "print(f\"\\n Somme des poids d'attention par ligne :\\n{attn_weights_matrix.sum(dim=1)} (doit √™tre ‚âà 1)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI2igZx2Mt0_",
        "outputId": "162d89dc-4694-4adb-b98a-0edee7797251"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Matrice des poids d'attention (apr√®s Softmax) :\n",
            "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
            "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
            "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
            "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
            "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
            "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
            "\n",
            " Somme des poids d'attention par ligne :\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]) (doit √™tre ‚âà 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul des context vectors pour chaque mot / multiplier la matrice des poids d‚Äôattention par nos embeddings d‚Äôentr√©e.\n",
        "context_vectors = torch.matmul(attn_weights_matrix, inputs)"
      ],
      "metadata": {
        "id": "QTCowt6gNL8f"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Affichage des context vectors\n",
        "print(context_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Fo_gq8KNSue",
        "outputId": "816fda7d-1cf3-432f-db5b-6105f874c153"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4421, 0.5931, 0.5790],\n",
            "        [0.4419, 0.6515, 0.5683],\n",
            "        [0.4431, 0.6496, 0.5671],\n",
            "        [0.4304, 0.6298, 0.5510],\n",
            "        [0.4671, 0.5910, 0.5266],\n",
            "        [0.4177, 0.6503, 0.5645]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "R√©sum√© du processus complet du Self-Attention: 1.Converti les mots en vecteurs (embeddings). 2.Calcul√© les scores d‚Äôattention entre tous les mots (produit scalaire).3.Appliqu√© Softmax pour normaliser les scores en probabilit√©s.4.G√©n√©r√© les context vectors en combinant chaque mot avec ses voisins."
      ],
      "metadata": {
        "id": "Be9tGxhWN9y_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. The ‚ÄòSelf‚Äô in Self-Attention¬∂"
      ],
      "metadata": {
        "id": "W2imqxRDV3In"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2.1 Initialiser les trois matrices de poids Wq, Wk, Wv :"
      ],
      "metadata": {
        "id": "r6Ciex0jayP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Impl√©mentation : Initialisation des Matrices Wq, Wk, Wv"
      ],
      "metadata": {
        "id": "NE8qetdMV4OP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_qg11VwJhxz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dans les Transformers, ces embeddings sont d‚Äôabord transform√©s par des matrices de poids Wq, Wk, Wv.\n",
        "Wq (Weights for Queries) ‚Üí Transforme un mot en une question pour chercher l‚Äôinformation pertinente.\n",
        "Wk (Weights for Keys) ‚Üí Transforme un mot en r√©f√©rence √† laquelle on compare les questions.\n",
        "Wv (Weights for Values) ‚Üí Transforme un mot en information finale qui sera transmise.\n",
        "initialise ces matrices comme des poids entra√Ænables en PyTorch"
      ],
      "metadata": {
        "id": "IaO89P2sh2hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# D√©finir la dimension des embeddings\n",
        "embedding_dim = inputs.shape[1]  # Ici, nos embeddings ont 3 dimensions"
      ],
      "metadata": {
        "id": "v5JdUZ69NrtG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialiser les matrices Wq, Wk, Wv (3x3 car on a 3 dimensions d'embeddings)\n",
        "Wq = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim))\n",
        "Wk = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim))\n",
        "Wv = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim))\n",
        "# Ce code cr√©e les trois matrices Wq, Wk, Wv qui serviront √† transformer nos mots\n",
        "# Ces matrices vont apprendre, au fil de l'entra√Ænement, comment transformer correctement les mots en Q, K et V."
      ],
      "metadata": {
        "id": "a2LwpIYwWORX"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher les matrices\n",
        "print(f\"Matrice Wq :\\n{Wq}\")\n",
        "print(f\"Matrice Wk :\\n{Wk}\")\n",
        "print(f\"Matrice Wv :\\n{Wv}\")\n",
        "# Ces matrices seront utilis√©es pour transformer nos embeddings de mots en Query, Key et Value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfFWZ8SsWOB9",
        "outputId": "08dcb1d8-8420-4cf7-8e0d-146b9397c386"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrice Wq :\n",
            "Parameter containing:\n",
            "tensor([[0.8713, 0.5234, 0.9295],\n",
            "        [0.5059, 0.8670, 0.0284],\n",
            "        [0.1292, 0.1072, 0.9899]], requires_grad=True)\n",
            "Matrice Wk :\n",
            "Parameter containing:\n",
            "tensor([[0.8595, 0.2962, 0.6314],\n",
            "        [0.3953, 0.2804, 0.5433],\n",
            "        [0.0554, 0.0690, 0.4600]], requires_grad=True)\n",
            "Matrice Wv :\n",
            "Parameter containing:\n",
            "tensor([[0.0840, 0.9858, 0.9330],\n",
            "        [0.5648, 0.4157, 0.6031],\n",
            "        [0.5617, 0.1062, 0.0103]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dans les Transformers, ces embeddings sont d‚Äôabord transform√©s par des matrices de poids Wq, Wk, Wv.\\\n",
        "Wq (Weights for Queries) ‚Üí Transforme un mot en une question pour chercher l‚Äôinformation pertinente.\\\n",
        "Wk (Weights for Keys) ‚Üí Transforme un mot en r√©f√©rence √† laquelle on compare les questions.\\\n",
        "Wv (Weights for Values) ‚Üí Transforme un mot en information finale qui sera transmise.\\\n",
        " initialise ces matrices comme des poids entra√Ænables en PyTorch"
      ],
      "metadata": {
        "id": "1QFHNBCPQQnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->Ces matrices seront utilis√©es pour transformer nos embeddings de mots en Query, Key et Value !"
      ],
      "metadata": {
        "id": "58VTH6zQRSiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impl√©mentation : Calcul des Q, K, V pour tous les mots\n",
        "# Calcul des Query, Key et Value en multipliant les inputs par Wq, Wk, Wv\n",
        "queries = torch.matmul(inputs, Wq)\n",
        "keys = torch.matmul(inputs, Wk)\n",
        "values = torch.matmul(inputs, Wv)\n",
        "\n",
        "# Affichage des r√©sultats\n",
        "print(f\"\\n Queries (Q) :\\n{queries}\")\n",
        "print(f\"\\n Keys (K) :\\n{keys}\")\n",
        "print(f\"\\n Values (V) :\\n{values}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV46_CaQQar9",
        "outputId": "46ce66f1-4d5c-45ea-d1a0-82096ed1f2fd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Queries (Q) :\n",
            "tensor([[0.5656, 0.4505, 1.2850],\n",
            "        [1.0046, 1.1129, 1.1893],\n",
            "        [1.0094, 1.1039, 1.1875],\n",
            "        [0.5278, 0.6534, 0.5476],\n",
            "        [0.8103, 0.6305, 0.8218],\n",
            "        [0.5194, 0.7787, 0.6137]], grad_fn=<MmBackward0>)\n",
            "\n",
            " Keys (K) :\n",
            "tensor([[0.4782, 0.2308, 0.7624],\n",
            "        [0.8532, 0.4524, 1.1236],\n",
            "        [0.8614, 0.4513, 1.1161],\n",
            "        [0.4366, 0.2506, 0.6058],\n",
            "        [0.7661, 0.3051, 0.6680],\n",
            "        [0.3897, 0.2771, 0.7192]], grad_fn=<MmBackward0>)\n",
            "\n",
            " Values (V) :\n",
            "tensor([[0.6208, 0.5808, 0.5008],\n",
            "        [0.9083, 0.9740, 1.0446],\n",
            "        [0.8874, 0.9832, 1.0510],\n",
            "        [0.5314, 0.4930, 0.5584],\n",
            "        [0.2620, 0.8736, 0.8702],\n",
            "        [0.7650, 0.4403, 0.5348]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2.2 Calculer les vecteurs de requ√™te, de cl√© et de valeur pour les entr√©es[1] :"
      ],
      "metadata": {
        "id": "7nV53mAqaos2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer inputs[1] en Q, K et V"
      ],
      "metadata": {
        "id": "qJgXvPwLZ-nO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# S√©lectionner le mot `inputs[1]` (2e mot de la phrase)\n",
        "word_1 = inputs[1]  # Ce mot repr√©sente \"journey\" dans notre dataset\n",
        "\n",
        "# Transformer en Query, Key et Value\n",
        "query_1 = torch.matmul(word_1, Wq)\n",
        "key_1 = torch.matmul(word_1, Wk)\n",
        "value_1 = torch.matmul(word_1, Wv)"
      ],
      "metadata": {
        "id": "qyiTHRp8Z_tG"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n Query (Q) pour inputs[1] :\\n{query_1}\")\n",
        "print(f\"\\n Key (K) pour inputs[1] :\\n{key_1}\")\n",
        "print(f\"\\nValue (V) pour inputs[1] :\\n{value_1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8nx2-cDaND-",
        "outputId": "0541c965-528a-48a3-d74a-2fa99e445869"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Query (Q) pour inputs[1] :\n",
            "tensor([1.0046, 1.1129, 1.1893], grad_fn=<SqueezeBackward4>)\n",
            "\n",
            "üîë Key (K) pour inputs[1] :\n",
            "tensor([0.8532, 0.4524, 1.1236], grad_fn=<SqueezeBackward4>)\n",
            "\n",
            "üì¶ Value (V) pour inputs[1] :\n",
            "tensor([0.9083, 0.9740, 1.0446], grad_fn=<SqueezeBackward4>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2.3 Calculez les entr√©es du score d'attention[1][1] ou œâ11 :"
      ],
      "metadata": {
        "id": "lL1L0F6ra2qO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "mesurer la similarit√© entre query_1 et key_1\\ns utiliserons le produit scalaire (dot product) pour mesurer cette similarit√©."
      ],
      "metadata": {
        "id": "zwOOzCbQbPHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul du score d‚Äôattention entre inputs[1] et lui-m√™me (œâ‚ÇÅ‚ÇÅ)\n",
        "attn_score_11 = torch.dot(query_1, key_1)"
      ],
      "metadata": {
        "id": "710sBfRma3v3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher le r√©sultat\n",
        "print(attn_score_11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Or3JsZIZa_L3",
        "outputId": "57df2429-bb84-4c59-a008-9311bf65930a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.6969, grad_fn=<DotBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2.4 Calculer tous les scores d'attention pour les entr√©es[1] :"
      ],
      "metadata": {
        "id": "b3niLyUJdYA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul des scores d'attention entre inputs[1] et TOUS les autres mots\n",
        "attn_scores_1 = torch.matmul(query_1, keys.T)"
      ],
      "metadata": {
        "id": "KKL2U88vdbi-"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (attn_scores_1)\n",
        "# Chaque valeur indique la force de la relation entre inputs[1] et un autre mot."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JJzU-USdcqt",
        "outputId": "58b6d527-61a8-4bea-f10b-cdba64ad55d9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.6440, 2.6969, 2.6950, 1.4380, 1.9037, 1.5552],\n",
            "       grad_fn=<SqueezeBackward4>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2.5 Pond√©rations d'attention pour les entr√©es[1] :"
      ],
      "metadata": {
        "id": "5_sLlaE-d5b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F  # Pour utiliser Softmax"
      ],
      "metadata": {
        "id": "Gj8p52XOgJ-2"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Appliquer Softmax pour normaliser les scores d'attention\n",
        "attn_weights_1 = F.softmax(attn_scores_1, dim=0)"
      ],
      "metadata": {
        "id": "BhAUIDNjd5Ke"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher les poids d'attention normalis√©s\n",
        "print (attn_weights_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aim6SioMgPW2",
        "outputId": "e6fbcab8-8204-46a9-c19e-7ee7f548d1e5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1025, 0.2939, 0.2933, 0.0835, 0.1330, 0.0938],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(attn_weights_1.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmL7ZytAgbJf",
        "outputId": "f71f0a0e-6f06-4521-e501-a5e566e2ba2a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2.6 Calculer le vecteur de contexte pour les entr√©es[1] :"
      ],
      "metadata": {
        "id": "syY7dpsJh0PG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "vecteur de contexte est un r√©sum√© de toute l‚Äôinformation importante pour un mot donn√© (inputs[1])."
      ],
      "metadata": {
        "id": "XRXGkvwXh5jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul du vecteur de contexte comme somme pond√©r√©e des valeurs (V)\n",
        "context_vector_1 = torch.sum(attn_weights_1.unsqueeze(1) * values, dim=0)\n",
        "\n",
        "# Afficher le vecteur de contexte\n",
        "print(f\"\\n Vecteur de Contexte pour inputs[1] :\\n{context_vector_1}\")\n",
        "# Ce vecteur unique de 3 dimensions repr√©sente maintenant toute la phrase du point de vue du mot inputs[1] !"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8riwz3Ih1J_",
        "outputId": "0f6ef893-b098-4434-ea4b-0ee4d562abcb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Vecteur de Contexte pour inputs[1] :\n",
            "tensor([0.7419, 0.8328, 0.8791], grad_fn=<SumBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3 Calcul des param√®tres de pond√©ration pour toutes les entr√©es :"
      ],
      "metadata": {
        "id": "JdOdwXoDi-Wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£ D√©finition des dimensions et Initialisation des Matrices Wq, Wk, Wv\n",
        "embedding_dim = inputs.shape[1]  # 3 dimensions pour chaque mot\n",
        "Wq = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim))  # Matrice pour Query\n",
        "Wk = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim))  # Matrice pour Key\n",
        "Wv = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim))  # Matrice pour Value\n",
        "\n",
        "# 2Ô∏è‚É£ Transformation des mots en Query (Q), Key (K) et Value (V)\n",
        "queries = torch.matmul(inputs, Wq)  # Appliquer Wq sur les mots\n",
        "keys = torch.matmul(inputs, Wk)     # Appliquer Wk sur les mots\n",
        "values = torch.matmul(inputs, Wv)   # Appliquer Wv sur les mots\n",
        "\n",
        "# 3Ô∏è‚É£ Calcul de la Matrice des Scores d‚ÄôAttention (Produit scalaire Q * K^T)\n",
        "attn_scores = torch.matmul(queries, keys.T)\n",
        "\n",
        "# 4Ô∏è‚É£ Normalisation des Scores avec Softmax pour obtenir les Poids d‚ÄôAttention\n",
        "attn_weights = F.softmax(attn_scores, dim=1)\n",
        "\n",
        "# 5Ô∏è‚É£ Calcul des Vecteurs de Contexte (somme pond√©r√©e des valeurs)\n",
        "context_vectors = torch.matmul(attn_weights, values)\n",
        "\n",
        "# 6Ô∏è‚É£ Affichage des r√©sultats\n",
        "print(f\"\\nüîç Matrice des Queries (Q) :\\n{queries}\")\n",
        "print(f\"\\nüîë Matrice des Keys (K) :\\n{keys}\")\n",
        "print(f\"\\nüì¶ Matrice des Values (V) :\\n{values}\")\n",
        "print(f\"\\nüìä Matrice des Scores d‚ÄôAttention (avant Softmax) :\\n{attn_scores}\")\n",
        "print(f\"\\nüéØ Matrice des Poids d‚ÄôAttention (apr√®s Softmax) :\\n{attn_weights}\")\n",
        "print(f\"\\nüìå Matrice des Vecteurs de Contexte :\\n{context_vectors}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePWJO4dOmwYo",
        "outputId": "6881abaa-04fa-41b1-9b24-24a0936b94da"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Matrice des Queries (Q) :\n",
            "tensor([[1.3146, 0.8511, 0.8483],\n",
            "        [1.6573, 0.8360, 0.9098],\n",
            "        [1.6471, 0.8177, 0.9169],\n",
            "        [0.8609, 0.4420, 0.4164],\n",
            "        [1.0012, 0.2578, 0.7865],\n",
            "        [1.0219, 0.6512, 0.3851]], grad_fn=<MmBackward0>)\n",
            "\n",
            "üîë Matrice des Keys (K) :\n",
            "tensor([[0.4441, 0.7701, 0.7344],\n",
            "        [0.9655, 0.9465, 0.6822],\n",
            "        [0.9468, 0.9366, 0.6785],\n",
            "        [0.5895, 0.5001, 0.3222],\n",
            "        [0.3427, 0.4962, 0.4212],\n",
            "        [0.8030, 0.6308, 0.3857]], grad_fn=<MmBackward0>)\n",
            "\n",
            "üì¶ Matrice des Values (V) :\n",
            "tensor([[0.7897, 0.5539, 0.3574],\n",
            "        [0.9177, 1.0407, 0.4663],\n",
            "        [0.9033, 1.0475, 0.4683],\n",
            "        [0.4922, 0.5463, 0.2303],\n",
            "        [0.3886, 0.8753, 0.3729],\n",
            "        [0.6665, 0.5190, 0.2289]], grad_fn=<MmBackward0>)\n",
            "\n",
            "üìä Matrice des Scores d‚ÄôAttention (avant Softmax) :\n",
            "tensor([[1.8623, 2.6535, 2.6174, 1.4739, 1.2301, 1.9196],\n",
            "        [2.0481, 3.0120, 2.9694, 1.6882, 1.3660, 2.2090],\n",
            "        [2.0347, 2.9897, 2.9474, 1.6753, 1.3564, 2.1920],\n",
            "        [1.0286, 1.5336, 1.5116, 0.8627, 0.6897, 1.1307],\n",
            "        [1.2208, 1.7472, 1.7230, 0.9725, 0.8023, 1.2699],\n",
            "        [1.2381, 1.8656, 1.8387, 1.0521, 0.8355, 1.3798]],\n",
            "       grad_fn=<MmBackward0>)\n",
            "\n",
            "üéØ Matrice des Poids d‚ÄôAttention (apr√®s Softmax) :\n",
            "tensor([[0.1315, 0.2902, 0.2799, 0.0892, 0.0699, 0.1393],\n",
            "        [0.1175, 0.3080, 0.2952, 0.0820, 0.0594, 0.1380],\n",
            "        [0.1181, 0.3070, 0.2943, 0.0825, 0.0599, 0.1382],\n",
            "        [0.1440, 0.2386, 0.2334, 0.1220, 0.1026, 0.1595],\n",
            "        [0.1463, 0.2477, 0.2418, 0.1142, 0.0963, 0.1537],\n",
            "        [0.1361, 0.2549, 0.2481, 0.1130, 0.0910, 0.1568]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "\n",
            "üìå Matrice des Vecteurs de Contexte :\n",
            "tensor([[0.7869, 0.8502, 0.3919],\n",
            "        [0.7975, 0.8632, 0.3965],\n",
            "        [0.7968, 0.8624, 0.3962],\n",
            "        [0.7497, 0.8117, 0.3749],\n",
            "        [0.7574, 0.8186, 0.3784],\n",
            "        [0.7611, 0.8234, 0.3796]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5Ô∏è‚É£ Calcul des Vecteurs de Contexte (somme pond√©r√©e des valeurs)\n",
        "context_vectors = torch.matmul(attn_weights, values)\n",
        "\n",
        "# 6Ô∏è‚É£ Affichage des r√©sultats\n",
        "print(f\"\\nüîç Matrice des Queries (Q) :\\n{queries}\")\n",
        "print(f\"\\nüîë Matrice des Keys (K) :\\n{keys}\")\n",
        "print(f\"\\nüì¶ Matrice des Values (V) :\\n{values}\")\n",
        "print(f\"\\nüìä Matrice des Scores d‚ÄôAttention (avant Softmax) :\\n{attn_scores}\")\n",
        "print(f\"\\nüéØ Matrice des Poids d‚ÄôAttention (apr√®s Softmax) :\\n{attn_weights}\")\n",
        "print(f\"\\nüìå Matrice des Vecteurs de Contexte :\\n{context_vectors}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aw_l8ycmVYJ",
        "outputId": "8525dcf4-833b-442d-fe5f-feaa81018acf"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Matrice des Queries (Q) :\n",
            "tensor([[0.5656, 0.4505, 1.2850],\n",
            "        [1.0046, 1.1129, 1.1893],\n",
            "        [1.0094, 1.1039, 1.1875],\n",
            "        [0.5278, 0.6534, 0.5476],\n",
            "        [0.8103, 0.6305, 0.8218],\n",
            "        [0.5194, 0.7787, 0.6137]], grad_fn=<MmBackward0>)\n",
            "\n",
            "üîë Matrice des Keys (K) :\n",
            "tensor([[0.4782, 0.2308, 0.7624],\n",
            "        [0.8532, 0.4524, 1.1236],\n",
            "        [0.8614, 0.4513, 1.1161],\n",
            "        [0.4366, 0.2506, 0.6058],\n",
            "        [0.7661, 0.3051, 0.6680],\n",
            "        [0.3897, 0.2771, 0.7192]], grad_fn=<MmBackward0>)\n",
            "\n",
            "üì¶ Matrice des Values (V) :\n",
            "tensor([[0.6208, 0.5808, 0.5008],\n",
            "        [0.9083, 0.9740, 1.0446],\n",
            "        [0.8874, 0.9832, 1.0510],\n",
            "        [0.5314, 0.4930, 0.5584],\n",
            "        [0.2620, 0.8736, 0.8702],\n",
            "        [0.7650, 0.4403, 0.5348]], grad_fn=<MmBackward0>)\n",
            "\n",
            "üìä Matrice des Scores d‚ÄôAttention (avant Softmax) :\n",
            "tensor([[1.3541, 2.1301, 2.1247, 1.1383, 1.4291, 1.2694],\n",
            "        [1.6440, 2.6969, 2.6950, 1.4380, 1.9037, 1.5552],\n",
            "        [1.6429, 2.6948, 2.6931, 1.4368, 1.9034, 1.5533],\n",
            "        [0.8207, 1.3612, 1.3607, 0.7259, 0.9695, 0.7806],\n",
            "        [1.1595, 1.8999, 1.8997, 1.0096, 1.3621, 1.0815],\n",
            "        [0.8960, 1.4849, 1.4838, 0.7937, 1.0454, 0.8595]],\n",
            "       grad_fn=<MmBackward0>)\n",
            "\n",
            "üéØ Matrice des Poids d‚ÄôAttention (apr√®s Softmax) :\n",
            "tensor([[0.1229, 0.2670, 0.2656, 0.0990, 0.1325, 0.1129],\n",
            "        [0.1025, 0.2939, 0.2933, 0.0835, 0.1330, 0.0938],\n",
            "        [0.1026, 0.2938, 0.2932, 0.0835, 0.1331, 0.0938],\n",
            "        [0.1340, 0.2300, 0.2299, 0.1219, 0.1555, 0.1287],\n",
            "        [0.1219, 0.2556, 0.2555, 0.1049, 0.1493, 0.1128],\n",
            "        [0.1311, 0.2362, 0.2359, 0.1183, 0.1522, 0.1264]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "\n",
            "üìå Matrice des Vecteurs de Contexte :\n",
            "tensor([[0.7283, 0.8069, 0.8506],\n",
            "        [0.7419, 0.8328, 0.8791],\n",
            "        [0.7417, 0.8328, 0.8791],\n",
            "        [0.7001, 0.7805, 0.8212],\n",
            "        [0.7157, 0.8028, 0.8454],\n",
            "        [0.7046, 0.7850, 0.8264]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    }
  ]
}