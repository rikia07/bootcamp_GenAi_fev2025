{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Daily Challenge: Simplified Self-Attention Explained"
      ],
      "metadata": {
        "id": "CiK2xJKS-SZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Simplified self-attention"
      ],
      "metadata": {
        "id": "Bkx00Dki-aYM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xg5eFJCG9-_V"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "[\n",
        "    [0.43, 0.15, 0.89], # your\n",
        "    [0.55, 0.87, 0.66], # journey\n",
        "    [0.57, 0.85, 0.64], # starts\n",
        "    [0.22, 0.58, 0.33], # with\n",
        "    [0.77, 0.25, 0.10], # one\n",
        "    [0.05, 0.80, 0.55] # step\n",
        "]\n",
        ")"
      ],
      "metadata": {
        "id": "gJx0rqgE-hN8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# connaitre la shape du tensor\n",
        "inputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_NngWmckrks",
        "outputId": "eb07fb10-c494-463b-a5b4-4299e3e15a5f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1 Computing Attention Weights for Inputs[2]:"
      ],
      "metadata": {
        "id": "3im5SUFBGJOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "query vector"
      ],
      "metadata": {
        "id": "jquA-4j_ALTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sélection du Query (3e mot = \"starts\")\n",
        "query = inputs[2]\n",
        "\n",
        "# Afficher le Query sélectionné (ce=vecteur du mot)\n",
        "print(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZwB0tIR_qmF",
        "outputId": "159f5025-1b4b-4a4f-8496-57590171e475"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.5700, 0.8500, 0.6400])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1.1 Attention Score:"
      ],
      "metadata": {
        "id": "9t-A6uQkGC8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "attention score : mesure la similarité entre le Query (mot sélectionné) et les autres mots.plus le score est élevé, plus les mots sont liés"
      ],
      "metadata": {
        "id": "dwT_BqOUAQWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "attn_scores_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCHb2E_XlRt0",
        "outputId": "915f2cea-d117-4c57-f9af-8184a276df82"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2.1707e-18, 7.0952e+22, 1.7748e+28, 1.8176e+31, 7.2708e+31, 5.0778e+31])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialiser un tableau pour stocker les scores d'attention\n",
        "attn_scores_2 = torch.zeros(inputs.shape[0])\n",
        "attn_scores_2"
      ],
      "metadata": {
        "id": "s5SbNL-b_rfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f37ae9e5-6bc4-408b-9901-88af225ac7cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul des scores d’attention (produit scalaire entre Query et chaque mot)\n",
        "for i, x_i in enumerate(inputs):\n",
        "  # On compare chaque mot (x_i) avec \"starts\" (query). x_i est donc le vecteur d’un mot de la phrase\n",
        "    attn_scores_2[i] = torch.dot(x_i, query)\n"
      ],
      "metadata": {
        "id": "BP2IwLUOCvA_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Ce code calcule le produit scalaire entre \"starts\" et chaque mot de la phrase."
      ],
      "metadata": {
        "id": "_dKGch-yDPt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher les scores d'attention\n",
        "print(f\"\\nScores d'attention pour le mot 'starts' :\\n{attn_scores_2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfXtUb1WC6E2",
        "outputId": "56b3d678-32b6-40d5-de59-537c6898cbd2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scores d'attention pour le mot 'starts' :\n",
            "tensor([0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->Plus le score est élevé, plus le mot est proche de \"starts\""
      ],
      "metadata": {
        "id": "4mMNZWVwDTKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1.2 Attention Weights:"
      ],
      "metadata": {
        "id": "rF6drswIF9u_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F  # Pour utiliser Softmax\n"
      ],
      "metadata": {
        "id": "WdTf8cWdG_N-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax fait exactement cela :Il prend une liste de nombres et les transforme en probabilités. La somme des probabilités est toujours égale à 1."
      ],
      "metadata": {
        "id": "g8HY4URlHKjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
        "attn_weights_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyruMKiRlrVk",
        "outputId": "1f58f6bd-f618-439a-f581-845ba22bcef3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Appliquer la fonction Softmax sur les scores d'attention\n",
        "attn_weights_2 = F.softmax(attn_scores_2, dim=0)\n",
        "#  Après Softmax, ils deviendront des probabilités"
      ],
      "metadata": {
        "id": "5jlCoecjHBrN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher les poids d'attention (après Softmax)\n",
        "print(f\"\\n Poids d'attention après Softmax :\\n{attn_weights_2}\")\n",
        "# Vérifie que la somme des poids est bien égale à 1 (normalisation):\n",
        "print(f\"\\n Somme des poids d'attention : {attn_weights_2.sum()} (doit être ≈ 1)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuXm5bm2HhDW",
        "outputId": "7f455e1d-bea2-4121-f6b6-211b973a1d5f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Poids d'attention après Softmax :\n",
            "tensor([0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565])\n",
            "\n",
            " Somme des poids d'attention : 1.0000001192092896 (doit être ≈ 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 1.1.3 Context Vector:"
      ],
      "metadata": {
        "id": "xIUckMepI9Am"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "contexte vector est une combinaison pondérée des mots selon leurs poids d’attention.\n",
        "➡ Il permet de résumer toutes les informations importantes d’une phrase en un seul vecteur"
      ],
      "metadata": {
        "id": "JPY1Bac5I9pX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensuite, on fait la somme des résultats pour obtenir le contexte final.\n",
        "Formule :\n",
        "           context_vector= ∑(attn_weights*inputs)"
      ],
      "metadata": {
        "id": "22XsQZnhJEef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul du vecteur de contexte comme somme pondérée des mots\n",
        "context_vector = torch.sum(attn_weights_2.unsqueeze(1) * inputs, dim=0)"
      ],
      "metadata": {
        "id": "L2Pt7o99JD5v"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher le vecteur de contexte\n",
        "print(context_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItnSfBgGJekv",
        "outputId": "26bca9d7-b11b-4f51-e238-d4cc4b65ebfb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4431, 0.6496, 0.5671])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Résumé : Converti les mots en vecteurs (embeddings)./Choisi un mot Query pour calculer l’attention./Calculé les scores d’attention avec le produit scalaire./Appliqué Softmax pour obtenir des poids d’attention normalisés./Calculé le vecteur de contexte, qui représente toute la phrase."
      ],
      "metadata": {
        "id": "u5SXirFYJvb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-Attention est essentiel dans les Transformers : Self-Attention analyse tous les mots en même temps; \\Les Transformers utilisent plusieurs couches de Self-Attention en même temps (on appelle cela Multi-Head Attention)"
      ],
      "metadata": {
        "id": "KNSlsNjFKhcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 Computing Attention Weights for All Inputs:"
      ],
      "metadata": {
        "id": "ulUAkDEiLz2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implémentation : Calculer les Scores d’Attention pour Tous les Mots"
      ],
      "metadata": {
        "id": "ISj8AAiWLyZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul de la matrice des scores d'attention\n",
        "attn_scores_matrix = torch.matmul(inputs, inputs.T)  # Produit matriciel entre les mots"
      ],
      "metadata": {
        "id": "8qaDU1YfJlOe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Affichage de la matrice des scores\n",
        "print(attn_scores_matrix)\n",
        "# Cela va donner une \"matrice d’attention\", une table qui montre les relations entre chaque mot.\n",
        "# Chaque cellule de cette matrice représente le score d’attention entre deux mots."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_-sfqIPMAl3",
        "outputId": "c0b3bf33-fcb6-4a8b-c939-b6a4f84f8341"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On applique Softmax sur chaque ligne de la matrice (dim=1) pour obtenir les poids d’attention."
      ],
      "metadata": {
        "id": "91wK63rUMm9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F  # Pour utiliser Softmax"
      ],
      "metadata": {
        "id": "BHoYM6jAMo7I"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Appliquer Softmax pour normaliser les scores d’attention\n",
        "attn_weights_matrix = F.softmax(attn_scores_matrix, dim=1)"
      ],
      "metadata": {
        "id": "vNBxnEraMYKf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Affichage de la matrice des poids d'attention\n",
        "print(f\"\\nMatrice des poids d'attention (après Softmax) :\\n{attn_weights_matrix}\")\n",
        "print(f\"\\n Somme des poids d'attention par ligne :\\n{attn_weights_matrix.sum(dim=1)} (doit être ≈ 1)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI2igZx2Mt0_",
        "outputId": "162d89dc-4694-4adb-b98a-0edee7797251"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Matrice des poids d'attention (après Softmax) :\n",
            "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
            "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
            "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
            "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
            "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
            "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
            "\n",
            " Somme des poids d'attention par ligne :\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]) (doit être ≈ 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul des context vectors pour chaque mot / multiplier la matrice des poids d’attention par nos embeddings d’entrée.\n",
        "context_vectors = torch.matmul(attn_weights_matrix, inputs)"
      ],
      "metadata": {
        "id": "QTCowt6gNL8f"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Affichage des context vectors\n",
        "print(context_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Fo_gq8KNSue",
        "outputId": "816fda7d-1cf3-432f-db5b-6105f874c153"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4421, 0.5931, 0.5790],\n",
            "        [0.4419, 0.6515, 0.5683],\n",
            "        [0.4431, 0.6496, 0.5671],\n",
            "        [0.4304, 0.6298, 0.5510],\n",
            "        [0.4671, 0.5910, 0.5266],\n",
            "        [0.4177, 0.6503, 0.5645]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Résumé du processus complet du Self-Attention: 1.Converti les mots en vecteurs (embeddings). 2.Calculé les scores d’attention entre tous les mots (produit scalaire).3.Appliqué Softmax pour normaliser les scores en probabilités.4.Généré les context vectors en combinant chaque mot avec ses voisins."
      ],
      "metadata": {
        "id": "Be9tGxhWN9y_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. The ‘Self’ in Self-Attention¶"
      ],
      "metadata": {
        "id": "W2imqxRDV3In"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2.1 Initialiser les trois matrices de poids Wq, Wk, Wv :"
      ],
      "metadata": {
        "id": "r6Ciex0jayP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implémentation : Initialisation des Matrices Wq, Wk, Wv"
      ],
      "metadata": {
        "id": "NE8qetdMV4OP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_qg11VwJhxz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dans les Transformers, ces embeddings sont d’abord transformés par des matrices de poids Wq, Wk, Wv.\n",
        "Wq (Weights for Queries) → Transforme un mot en une question pour chercher l’information pertinente.\n",
        "Wk (Weights for Keys) → Transforme un mot en référence à laquelle on compare les questions.\n",
        "Wv (Weights for Values) → Transforme un mot en information finale qui sera transmise.\n",
        "initialise ces matrices comme des poids entraînables en PyTorch"
      ],
      "metadata": {
        "id": "IaO89P2sh2hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Définir la dimension des embeddings\n",
        "embedding_dim = inputs.shape[1]  # Ici, nos embeddings ont 3 dimensions"
      ],
      "metadata": {
        "id": "v5JdUZ69NrtG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialiser les matrices Wq, Wk, Wv (3x3 car on a 3 dimensions d'embeddings)\n",
        "Wq = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim))\n",
        "Wk = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim))\n",
        "Wv = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim))\n",
        "# Ce code crée les trois matrices Wq, Wk, Wv qui serviront à transformer nos mots\n",
        "# Ces matrices vont apprendre, au fil de l'entraînement, comment transformer correctement les mots en Q, K et V."
      ],
      "metadata": {
        "id": "a2LwpIYwWORX"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher les matrices\n",
        "print(f\"Matrice Wq :\\n{Wq}\")\n",
        "print(f\"Matrice Wk :\\n{Wk}\")\n",
        "print(f\"Matrice Wv :\\n{Wv}\")\n",
        "# Ces matrices seront utilisées pour transformer nos embeddings de mots en Query, Key et Value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfFWZ8SsWOB9",
        "outputId": "08dcb1d8-8420-4cf7-8e0d-146b9397c386"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrice Wq :\n",
            "Parameter containing:\n",
            "tensor([[0.8713, 0.5234, 0.9295],\n",
            "        [0.5059, 0.8670, 0.0284],\n",
            "        [0.1292, 0.1072, 0.9899]], requires_grad=True)\n",
            "Matrice Wk :\n",
            "Parameter containing:\n",
            "tensor([[0.8595, 0.2962, 0.6314],\n",
            "        [0.3953, 0.2804, 0.5433],\n",
            "        [0.0554, 0.0690, 0.4600]], requires_grad=True)\n",
            "Matrice Wv :\n",
            "Parameter containing:\n",
            "tensor([[0.0840, 0.9858, 0.9330],\n",
            "        [0.5648, 0.4157, 0.6031],\n",
            "        [0.5617, 0.1062, 0.0103]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dans les Transformers, ces embeddings sont d’abord transformés par des matrices de poids Wq, Wk, Wv.\\\n",
        "Wq (Weights for Queries) → Transforme un mot en une question pour chercher l’information pertinente.\\\n",
        "Wk (Weights for Keys) → Transforme un mot en référence à laquelle on compare les questions.\\\n",
        "Wv (Weights for Values) → Transforme un mot en information finale qui sera transmise.\\\n",
        " initialise ces matrices comme des poids entraînables en PyTorch"
      ],
      "metadata": {
        "id": "1QFHNBCPQQnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->Ces matrices seront utilisées pour transformer nos embeddings de mots en Query, Key et Value !"
      ],
      "metadata": {
        "id": "58VTH6zQRSiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implémentation : Calcul des Q, K, V pour tous les mots\n",
        "# Calcul des Query, Key et Value en multipliant les inputs par Wq, Wk, Wv\n",
        "queries = torch.matmul(inputs, Wq)\n",
        "keys = torch.matmul(inputs, Wk)\n",
        "values = torch.matmul(inputs, Wv)\n",
        "\n",
        "# Affichage des résultats\n",
        "print(f\"\\n Queries (Q) :\\n{queries}\")\n",
        "print(f\"\\n Keys (K) :\\n{keys}\")\n",
        "print(f\"\\n Values (V) :\\n{values}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV46_CaQQar9",
        "outputId": "46ce66f1-4d5c-45ea-d1a0-82096ed1f2fd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Queries (Q) :\n",
            "tensor([[0.5656, 0.4505, 1.2850],\n",
            "        [1.0046, 1.1129, 1.1893],\n",
            "        [1.0094, 1.1039, 1.1875],\n",
            "        [0.5278, 0.6534, 0.5476],\n",
            "        [0.8103, 0.6305, 0.8218],\n",
            "        [0.5194, 0.7787, 0.6137]], grad_fn=<MmBackward0>)\n",
            "\n",
            " Keys (K) :\n",
            "tensor([[0.4782, 0.2308, 0.7624],\n",
            "        [0.8532, 0.4524, 1.1236],\n",
            "        [0.8614, 0.4513, 1.1161],\n",
            "        [0.4366, 0.2506, 0.6058],\n",
            "        [0.7661, 0.3051, 0.6680],\n",
            "        [0.3897, 0.2771, 0.7192]], grad_fn=<MmBackward0>)\n",
            "\n",
            " Values (V) :\n",
            "tensor([[0.6208, 0.5808, 0.5008],\n",
            "        [0.9083, 0.9740, 1.0446],\n",
            "        [0.8874, 0.9832, 1.0510],\n",
            "        [0.5314, 0.4930, 0.5584],\n",
            "        [0.2620, 0.8736, 0.8702],\n",
            "        [0.7650, 0.4403, 0.5348]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2.2 Calculer les vecteurs de requête, de clé et de valeur pour les entrées[1] :"
      ],
      "metadata": {
        "id": "7nV53mAqaos2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer inputs[1] en Q, K et V"
      ],
      "metadata": {
        "id": "qJgXvPwLZ-nO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sélectionner le mot `inputs[1]` (2e mot de la phrase)\n",
        "word_1 = inputs[1]  # Ce mot représente \"journey\" dans notre dataset\n",
        "\n",
        "# Transformer en Query, Key et Value\n",
        "query_1 = torch.matmul(word_1, Wq)\n",
        "key_1 = torch.matmul(word_1, Wk)\n",
        "value_1 = torch.matmul(word_1, Wv)"
      ],
      "metadata": {
        "id": "qyiTHRp8Z_tG"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n Query (Q) pour inputs[1] :\\n{query_1}\")\n",
        "print(f\"\\n Key (K) pour inputs[1] :\\n{key_1}\")\n",
        "print(f\"\\nValue (V) pour inputs[1] :\\n{value_1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8nx2-cDaND-",
        "outputId": "0541c965-528a-48a3-d74a-2fa99e445869"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Query (Q) pour inputs[1] :\n",
            "tensor([1.0046, 1.1129, 1.1893], grad_fn=<SqueezeBackward4>)\n",
            "\n",
            "🔑 Key (K) pour inputs[1] :\n",
            "tensor([0.8532, 0.4524, 1.1236], grad_fn=<SqueezeBackward4>)\n",
            "\n",
            "📦 Value (V) pour inputs[1] :\n",
            "tensor([0.9083, 0.9740, 1.0446], grad_fn=<SqueezeBackward4>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2.3 Calculez les entrées du score d'attention[1][1] ou ω11 :"
      ],
      "metadata": {
        "id": "lL1L0F6ra2qO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "mesurer la similarité entre query_1 et key_1\\ns utiliserons le produit scalaire (dot product) pour mesurer cette similarité."
      ],
      "metadata": {
        "id": "zwOOzCbQbPHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul du score d’attention entre inputs[1] et lui-même (ω₁₁)\n",
        "attn_score_11 = torch.dot(query_1, key_1)"
      ],
      "metadata": {
        "id": "710sBfRma3v3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher le résultat\n",
        "print(attn_score_11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Or3JsZIZa_L3",
        "outputId": "57df2429-bb84-4c59-a008-9311bf65930a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.6969, grad_fn=<DotBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2.4 Calculer tous les scores d'attention pour les entrées[1] :"
      ],
      "metadata": {
        "id": "b3niLyUJdYA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul des scores d'attention entre inputs[1] et TOUS les autres mots\n",
        "attn_scores_1 = torch.matmul(query_1, keys.T)"
      ],
      "metadata": {
        "id": "KKL2U88vdbi-"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (attn_scores_1)\n",
        "# Chaque valeur indique la force de la relation entre inputs[1] et un autre mot."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JJzU-USdcqt",
        "outputId": "58b6d527-61a8-4bea-f10b-cdba64ad55d9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.6440, 2.6969, 2.6950, 1.4380, 1.9037, 1.5552],\n",
            "       grad_fn=<SqueezeBackward4>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2.5 Pondérations d'attention pour les entrées[1] :"
      ],
      "metadata": {
        "id": "5_sLlaE-d5b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F  # Pour utiliser Softmax"
      ],
      "metadata": {
        "id": "Gj8p52XOgJ-2"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Appliquer Softmax pour normaliser les scores d'attention\n",
        "attn_weights_1 = F.softmax(attn_scores_1, dim=0)"
      ],
      "metadata": {
        "id": "BhAUIDNjd5Ke"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher les poids d'attention normalisés\n",
        "print (attn_weights_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aim6SioMgPW2",
        "outputId": "e6fbcab8-8204-46a9-c19e-7ee7f548d1e5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1025, 0.2939, 0.2933, 0.0835, 0.1330, 0.0938],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(attn_weights_1.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmL7ZytAgbJf",
        "outputId": "f71f0a0e-6f06-4521-e501-a5e566e2ba2a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2.6 Calculer le vecteur de contexte pour les entrées[1] :"
      ],
      "metadata": {
        "id": "syY7dpsJh0PG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "vecteur de contexte est un résumé de toute l’information importante pour un mot donné (inputs[1])."
      ],
      "metadata": {
        "id": "XRXGkvwXh5jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul du vecteur de contexte comme somme pondérée des valeurs (V)\n",
        "context_vector_1 = torch.sum(attn_weights_1.unsqueeze(1) * values, dim=0)\n",
        "\n",
        "# Afficher le vecteur de contexte\n",
        "print(f\"\\n Vecteur de Contexte pour inputs[1] :\\n{context_vector_1}\")\n",
        "# Ce vecteur unique de 3 dimensions représente maintenant toute la phrase du point de vue du mot inputs[1] !"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8riwz3Ih1J_",
        "outputId": "0f6ef893-b098-4434-ea4b-0ee4d562abcb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Vecteur de Contexte pour inputs[1] :\n",
            "tensor([0.7419, 0.8328, 0.8791], grad_fn=<SumBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3 Calcul des paramètres de pondération pour toutes les entrées :"
      ],
      "metadata": {
        "id": "JdOdwXoDi-Wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Définition des dimensions et Initialisation des Matrices Wq, Wk, Wv\n",
        "embedding_dim = inputs.shape[1]  # 3 dimensions pour chaque mot\n",
        "Wq = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim))  # Matrice pour Query\n",
        "Wk = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim))  # Matrice pour Key\n",
        "Wv = torch.nn.Parameter(torch.rand(embedding_dim, embedding_dim))  # Matrice pour Value\n",
        "\n",
        "# 2️⃣ Transformation des mots en Query (Q), Key (K) et Value (V)\n",
        "queries = torch.matmul(inputs, Wq)  # Appliquer Wq sur les mots\n",
        "keys = torch.matmul(inputs, Wk)     # Appliquer Wk sur les mots\n",
        "values = torch.matmul(inputs, Wv)   # Appliquer Wv sur les mots\n",
        "\n",
        "# 3️⃣ Calcul de la Matrice des Scores d’Attention (Produit scalaire Q * K^T)\n",
        "attn_scores = torch.matmul(queries, keys.T)\n",
        "\n",
        "# 4️⃣ Normalisation des Scores avec Softmax pour obtenir les Poids d’Attention\n",
        "attn_weights = F.softmax(attn_scores, dim=1)\n",
        "\n",
        "# 5️⃣ Calcul des Vecteurs de Contexte (somme pondérée des valeurs)\n",
        "context_vectors = torch.matmul(attn_weights, values)\n",
        "\n",
        "# 6️⃣ Affichage des résultats\n",
        "print(f\"\\n🔍 Matrice des Queries (Q) :\\n{queries}\")\n",
        "print(f\"\\n🔑 Matrice des Keys (K) :\\n{keys}\")\n",
        "print(f\"\\n📦 Matrice des Values (V) :\\n{values}\")\n",
        "print(f\"\\n📊 Matrice des Scores d’Attention (avant Softmax) :\\n{attn_scores}\")\n",
        "print(f\"\\n🎯 Matrice des Poids d’Attention (après Softmax) :\\n{attn_weights}\")\n",
        "print(f\"\\n📌 Matrice des Vecteurs de Contexte :\\n{context_vectors}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePWJO4dOmwYo",
        "outputId": "6881abaa-04fa-41b1-9b24-24a0936b94da"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Matrice des Queries (Q) :\n",
            "tensor([[1.3146, 0.8511, 0.8483],\n",
            "        [1.6573, 0.8360, 0.9098],\n",
            "        [1.6471, 0.8177, 0.9169],\n",
            "        [0.8609, 0.4420, 0.4164],\n",
            "        [1.0012, 0.2578, 0.7865],\n",
            "        [1.0219, 0.6512, 0.3851]], grad_fn=<MmBackward0>)\n",
            "\n",
            "🔑 Matrice des Keys (K) :\n",
            "tensor([[0.4441, 0.7701, 0.7344],\n",
            "        [0.9655, 0.9465, 0.6822],\n",
            "        [0.9468, 0.9366, 0.6785],\n",
            "        [0.5895, 0.5001, 0.3222],\n",
            "        [0.3427, 0.4962, 0.4212],\n",
            "        [0.8030, 0.6308, 0.3857]], grad_fn=<MmBackward0>)\n",
            "\n",
            "📦 Matrice des Values (V) :\n",
            "tensor([[0.7897, 0.5539, 0.3574],\n",
            "        [0.9177, 1.0407, 0.4663],\n",
            "        [0.9033, 1.0475, 0.4683],\n",
            "        [0.4922, 0.5463, 0.2303],\n",
            "        [0.3886, 0.8753, 0.3729],\n",
            "        [0.6665, 0.5190, 0.2289]], grad_fn=<MmBackward0>)\n",
            "\n",
            "📊 Matrice des Scores d’Attention (avant Softmax) :\n",
            "tensor([[1.8623, 2.6535, 2.6174, 1.4739, 1.2301, 1.9196],\n",
            "        [2.0481, 3.0120, 2.9694, 1.6882, 1.3660, 2.2090],\n",
            "        [2.0347, 2.9897, 2.9474, 1.6753, 1.3564, 2.1920],\n",
            "        [1.0286, 1.5336, 1.5116, 0.8627, 0.6897, 1.1307],\n",
            "        [1.2208, 1.7472, 1.7230, 0.9725, 0.8023, 1.2699],\n",
            "        [1.2381, 1.8656, 1.8387, 1.0521, 0.8355, 1.3798]],\n",
            "       grad_fn=<MmBackward0>)\n",
            "\n",
            "🎯 Matrice des Poids d’Attention (après Softmax) :\n",
            "tensor([[0.1315, 0.2902, 0.2799, 0.0892, 0.0699, 0.1393],\n",
            "        [0.1175, 0.3080, 0.2952, 0.0820, 0.0594, 0.1380],\n",
            "        [0.1181, 0.3070, 0.2943, 0.0825, 0.0599, 0.1382],\n",
            "        [0.1440, 0.2386, 0.2334, 0.1220, 0.1026, 0.1595],\n",
            "        [0.1463, 0.2477, 0.2418, 0.1142, 0.0963, 0.1537],\n",
            "        [0.1361, 0.2549, 0.2481, 0.1130, 0.0910, 0.1568]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "\n",
            "📌 Matrice des Vecteurs de Contexte :\n",
            "tensor([[0.7869, 0.8502, 0.3919],\n",
            "        [0.7975, 0.8632, 0.3965],\n",
            "        [0.7968, 0.8624, 0.3962],\n",
            "        [0.7497, 0.8117, 0.3749],\n",
            "        [0.7574, 0.8186, 0.3784],\n",
            "        [0.7611, 0.8234, 0.3796]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5️⃣ Calcul des Vecteurs de Contexte (somme pondérée des valeurs)\n",
        "context_vectors = torch.matmul(attn_weights, values)\n",
        "\n",
        "# 6️⃣ Affichage des résultats\n",
        "print(f\"\\n🔍 Matrice des Queries (Q) :\\n{queries}\")\n",
        "print(f\"\\n🔑 Matrice des Keys (K) :\\n{keys}\")\n",
        "print(f\"\\n📦 Matrice des Values (V) :\\n{values}\")\n",
        "print(f\"\\n📊 Matrice des Scores d’Attention (avant Softmax) :\\n{attn_scores}\")\n",
        "print(f\"\\n🎯 Matrice des Poids d’Attention (après Softmax) :\\n{attn_weights}\")\n",
        "print(f\"\\n📌 Matrice des Vecteurs de Contexte :\\n{context_vectors}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aw_l8ycmVYJ",
        "outputId": "8525dcf4-833b-442d-fe5f-feaa81018acf"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Matrice des Queries (Q) :\n",
            "tensor([[0.5656, 0.4505, 1.2850],\n",
            "        [1.0046, 1.1129, 1.1893],\n",
            "        [1.0094, 1.1039, 1.1875],\n",
            "        [0.5278, 0.6534, 0.5476],\n",
            "        [0.8103, 0.6305, 0.8218],\n",
            "        [0.5194, 0.7787, 0.6137]], grad_fn=<MmBackward0>)\n",
            "\n",
            "🔑 Matrice des Keys (K) :\n",
            "tensor([[0.4782, 0.2308, 0.7624],\n",
            "        [0.8532, 0.4524, 1.1236],\n",
            "        [0.8614, 0.4513, 1.1161],\n",
            "        [0.4366, 0.2506, 0.6058],\n",
            "        [0.7661, 0.3051, 0.6680],\n",
            "        [0.3897, 0.2771, 0.7192]], grad_fn=<MmBackward0>)\n",
            "\n",
            "📦 Matrice des Values (V) :\n",
            "tensor([[0.6208, 0.5808, 0.5008],\n",
            "        [0.9083, 0.9740, 1.0446],\n",
            "        [0.8874, 0.9832, 1.0510],\n",
            "        [0.5314, 0.4930, 0.5584],\n",
            "        [0.2620, 0.8736, 0.8702],\n",
            "        [0.7650, 0.4403, 0.5348]], grad_fn=<MmBackward0>)\n",
            "\n",
            "📊 Matrice des Scores d’Attention (avant Softmax) :\n",
            "tensor([[1.3541, 2.1301, 2.1247, 1.1383, 1.4291, 1.2694],\n",
            "        [1.6440, 2.6969, 2.6950, 1.4380, 1.9037, 1.5552],\n",
            "        [1.6429, 2.6948, 2.6931, 1.4368, 1.9034, 1.5533],\n",
            "        [0.8207, 1.3612, 1.3607, 0.7259, 0.9695, 0.7806],\n",
            "        [1.1595, 1.8999, 1.8997, 1.0096, 1.3621, 1.0815],\n",
            "        [0.8960, 1.4849, 1.4838, 0.7937, 1.0454, 0.8595]],\n",
            "       grad_fn=<MmBackward0>)\n",
            "\n",
            "🎯 Matrice des Poids d’Attention (après Softmax) :\n",
            "tensor([[0.1229, 0.2670, 0.2656, 0.0990, 0.1325, 0.1129],\n",
            "        [0.1025, 0.2939, 0.2933, 0.0835, 0.1330, 0.0938],\n",
            "        [0.1026, 0.2938, 0.2932, 0.0835, 0.1331, 0.0938],\n",
            "        [0.1340, 0.2300, 0.2299, 0.1219, 0.1555, 0.1287],\n",
            "        [0.1219, 0.2556, 0.2555, 0.1049, 0.1493, 0.1128],\n",
            "        [0.1311, 0.2362, 0.2359, 0.1183, 0.1522, 0.1264]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "\n",
            "📌 Matrice des Vecteurs de Contexte :\n",
            "tensor([[0.7283, 0.8069, 0.8506],\n",
            "        [0.7419, 0.8328, 0.8791],\n",
            "        [0.7417, 0.8328, 0.8791],\n",
            "        [0.7001, 0.7805, 0.8212],\n",
            "        [0.7157, 0.8028, 0.8454],\n",
            "        [0.7046, 0.7850, 0.8264]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    }
  ]
}