{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:08:22.934733Z","iopub.execute_input":"2025-03-27T11:08:22.935261Z","iopub.status.idle":"2025-03-27T11:08:24.154961Z","shell.execute_reply.started":"2025-03-27T11:08:22.935216Z","shell.execute_reply":"2025-03-27T11:08:24.153781Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"#Daily Challenge : Run Code Llama on Kaggle using llama.cpp","metadata":{}},{"cell_type":"markdown","source":"1. Faire la demande d‚Äôacc√®s\\\n   demande d'acc√®s acc√®s au mod√®le CodeLLaMA-13B-Instruct sur Hugging Face; validation officielle re√ßue par email","metadata":{}},{"cell_type":"markdown","source":"√âtape 2 : T√©l√©charger le mod√®le dans Kaggle\\\n","metadata":{}},{"cell_type":"code","source":"# Afficher les fichiers pr√©sents dans le dossier de travail (/kaggle/working)\n!ls -lh /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:34:17.299378Z","iopub.execute_input":"2025-03-27T11:34:17.299785Z","iopub.status.idle":"2025-03-27T11:34:17.423396Z","shell.execute_reply.started":"2025-03-27T11:34:17.299737Z","shell.execute_reply":"2025-03-27T11:34:17.422014Z"}},"outputs":[{"name":"stdout","text":"total 0\n-rw-r--r-- 1 root root 0 Mar 27 11:32 codellama-13b-instruct.gguf\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Forcer l'enregistrement dans /kaggle/working\n!wget https://huggingface.co/TheBloke/CodeLlama-13B-Instruct-GGUF/resolve/main/codellama-13b-instruct.Q4_K_M.gguf -P /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:01:09.116463Z","iopub.execute_input":"2025-03-27T13:01:09.116962Z","iopub.status.idle":"2025-03-27T13:02:01.333464Z","shell.execute_reply.started":"2025-03-27T13:01:09.116925Z","shell.execute_reply":"2025-03-27T13:02:01.330852Z"}},"outputs":[{"name":"stdout","text":"--2025-03-27 13:01:09--  https://huggingface.co/TheBloke/CodeLlama-13B-Instruct-GGUF/resolve/main/codellama-13b-instruct.Q4_K_M.gguf\nResolving huggingface.co (huggingface.co)... 18.244.202.68, 18.244.202.60, 18.244.202.118, ...\nConnecting to huggingface.co (huggingface.co)|18.244.202.68|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cdn-lfs.hf.co/repos/7b/80/7b800b9247db84b65d4ca008ca4433a306b7f259163c4d026874b4aa9f7112eb/48cc5600c5e35b1226208a53b1871f50efb15764232babaef23e2264c285d7d9?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27codellama-13b-instruct.Q4_K_M.gguf%3B+filename%3D%22codellama-13b-instruct.Q4_K_M.gguf%22%3B&Expires=1743084069&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzA4NDA2OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy83Yi84MC83YjgwMGI5MjQ3ZGI4NGI2NWQ0Y2EwMDhjYTQ0MzNhMzA2YjdmMjU5MTYzYzRkMDI2ODc0YjRhYTlmNzExMmViLzQ4Y2M1NjAwYzVlMzViMTIyNjIwOGE1M2IxODcxZjUwZWZiMTU3NjQyMzJiYWJhZWYyM2UyMjY0YzI4NWQ3ZDk%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=KynS3EWOfJtvAL3gZp3XkZOes%7Ea7ZQKOBiP26XRZ-PFwQlDJ6mJoq4bM-v5fOqQACQoP2eK6gKfaYA%7Em4PbTZsst6OgxU9n1OTqCRgk%7EYQoMf4pOrkPkLwrytVbp-URlfUR1zw5C-FENTgJA2ipGdXZQJ7Evwm3C9Wmtcp5cINN-t9JGSfw6Vmk4UkTonoE8lDphXXeUilvIRtgO40UNKuwPk9NNgLNX7WV5XTcvhq%7EUd5XAdgsjPb5E8KaXmAHs8rtO3vqvHES0zccIN5uRXml%7EwVDIFLXvbHHiiUB7rqujXF3LB%7Ecy17liJvhdTEJAUbF23RPvzaQEwBjr8kfK9w__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n--2025-03-27 13:01:09--  https://cdn-lfs.hf.co/repos/7b/80/7b800b9247db84b65d4ca008ca4433a306b7f259163c4d026874b4aa9f7112eb/48cc5600c5e35b1226208a53b1871f50efb15764232babaef23e2264c285d7d9?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27codellama-13b-instruct.Q4_K_M.gguf%3B+filename%3D%22codellama-13b-instruct.Q4_K_M.gguf%22%3B&Expires=1743084069&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzA4NDA2OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy83Yi84MC83YjgwMGI5MjQ3ZGI4NGI2NWQ0Y2EwMDhjYTQ0MzNhMzA2YjdmMjU5MTYzYzRkMDI2ODc0YjRhYTlmNzExMmViLzQ4Y2M1NjAwYzVlMzViMTIyNjIwOGE1M2IxODcxZjUwZWZiMTU3NjQyMzJiYWJhZWYyM2UyMjY0YzI4NWQ3ZDk%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=KynS3EWOfJtvAL3gZp3XkZOes%7Ea7ZQKOBiP26XRZ-PFwQlDJ6mJoq4bM-v5fOqQACQoP2eK6gKfaYA%7Em4PbTZsst6OgxU9n1OTqCRgk%7EYQoMf4pOrkPkLwrytVbp-URlfUR1zw5C-FENTgJA2ipGdXZQJ7Evwm3C9Wmtcp5cINN-t9JGSfw6Vmk4UkTonoE8lDphXXeUilvIRtgO40UNKuwPk9NNgLNX7WV5XTcvhq%7EUd5XAdgsjPb5E8KaXmAHs8rtO3vqvHES0zccIN5uRXml%7EwVDIFLXvbHHiiUB7rqujXF3LB%7Ecy17liJvhdTEJAUbF23RPvzaQEwBjr8kfK9w__&Key-Pair-Id=K3RPWS32NSSJCE\nResolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 18.160.78.87, 18.160.78.43, 18.160.78.76, ...\nConnecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|18.160.78.87|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7866070016 (7.3G) [binary/octet-stream]\nSaving to: ‚Äò/kaggle/working/codellama-13b-instruct.Q4_K_M.gguf‚Äô\n\ncodellama-13b-instr 100%[===================>]   7.33G  99.5MB/s    in 52s     \n\n2025-03-27 13:02:01 (145 MB/s) - ‚Äò/kaggle/working/codellama-13b-instruct.Q4_K_M.gguf‚Äô saved [7866070016/7866070016]\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!ls -lh /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:03:22.525858Z","iopub.execute_input":"2025-03-27T13:03:22.526397Z","iopub.status.idle":"2025-03-27T13:03:22.657153Z","shell.execute_reply.started":"2025-03-27T13:03:22.526359Z","shell.execute_reply":"2025-03-27T13:03:22.655715Z"}},"outputs":[{"name":"stdout","text":"total 7.4G\n-rw-r--r-- 1 root root 7.4G Sep  5  2023 codellama-13b-instruct.Q4_K_M.gguf\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"√âtape 3 : Charger le mod√®le avec llama-cpp-python","metadata":{}},{"cell_type":"code","source":"# Installer llama-cpp-python avec support CUDA (GPU)\n!pip install llama-cpp-python --upgrade --quiet\n# Ce package est une interface Python vers llama.cpp : il permet de charger et utiliser ton mod√®le .gguf en Python.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:03:58.671780Z","iopub.execute_input":"2025-03-27T13:03:58.672289Z","iopub.status.idle":"2025-03-27T13:04:03.612413Z","shell.execute_reply.started":"2025-03-27T13:03:58.672239Z","shell.execute_reply":"2025-03-27T13:04:03.610971Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Importer le wrapper Python pour llama.cpp\nfrom llama_cpp import Llama","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:04:09.732882Z","iopub.execute_input":"2025-03-27T13:04:09.733390Z","iopub.status.idle":"2025-03-27T13:04:09.738363Z","shell.execute_reply.started":"2025-03-27T13:04:09.733351Z","shell.execute_reply":"2025-03-27T13:04:09.737150Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Charger le mod√®le GGUF t√©l√©charg√© (nom exact du fichier)\nllm = Llama(\n    model_path=\"codellama-13b-instruct.Q4_K_M.gguf\",  # Le fichier que tu as t√©l√©charg√©\n    n_ctx=2048,        # Contexte maximal (nombre de tokens d'entr√©e)\n    n_threads=8,       # Nombre de threads CPU √† utiliser (tu peux ajuster selon ta machine)\n    verbose=True       # Pour afficher les infos du chargement\n)\n# n_threads=8 fonctionne bien sur Kaggle avec un GPU activ√© / on peut aussi tester sur CPU","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:04:12.670128Z","iopub.execute_input":"2025-03-27T13:04:12.670480Z","iopub.status.idle":"2025-03-27T13:04:39.710948Z","shell.execute_reply.started":"2025-03-27T13:04:12.670455Z","shell.execute_reply":"2025-03-27T13:04:39.709822Z"}},"outputs":[{"name":"stderr","text":"llama_model_loader: loaded meta data with 20 key-value pairs and 363 tensors from codellama-13b-instruct.Q4_K_M.gguf (version GGUF V2)\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = codellama_codellama-13b-instruct-hf\nllama_model_loader: - kv   2:                       llama.context_length u32              = 16384\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\nllama_model_loader: - kv   4:                          llama.block_count u32              = 40\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  11:                          general.file_type u32              = 15\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  19:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   81 tensors\nllama_model_loader: - type q4_K:  241 tensors\nllama_model_loader: - type q6_K:   41 tensors\nprint_info: file format = GGUF V2\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 7.33 GiB (4.83 BPW) \ninit_tokenizer: initializing tokenizer for type 1\nload: control token:      2 '</s>' is not marked as EOG\nload: control token:      1 '<s>' is not marked as EOG\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 3\nload: token to piece cache size = 0.1686 MB\nprint_info: arch             = llama\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 16384\nprint_info: n_embd           = 5120\nprint_info: n_layer          = 40\nprint_info: n_head           = 40\nprint_info: n_head_kv        = 40\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 1\nprint_info: n_embd_k_gqa     = 5120\nprint_info: n_embd_v_gqa     = 5120\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-05\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 13824\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 0\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 16384\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 13B\nprint_info: model params     = 13.02 B\nprint_info: general.name     = codellama_codellama-13b-instruct-hf\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 32016\nprint_info: n_merges         = 0\nprint_info: BOS token        = 1 '<s>'\nprint_info: EOS token        = 2 '</s>'\nprint_info: UNK token        = 0 '<unk>'\nprint_info: LF token         = 13 '<0x0A>'\nprint_info: EOG token        = 2 '</s>'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: layer   0 assigned to device CPU\nload_tensors: layer   1 assigned to device CPU\nload_tensors: layer   2 assigned to device CPU\nload_tensors: layer   3 assigned to device CPU\nload_tensors: layer   4 assigned to device CPU\nload_tensors: layer   5 assigned to device CPU\nload_tensors: layer   6 assigned to device CPU\nload_tensors: layer   7 assigned to device CPU\nload_tensors: layer   8 assigned to device CPU\nload_tensors: layer   9 assigned to device CPU\nload_tensors: layer  10 assigned to device CPU\nload_tensors: layer  11 assigned to device CPU\nload_tensors: layer  12 assigned to device CPU\nload_tensors: layer  13 assigned to device CPU\nload_tensors: layer  14 assigned to device CPU\nload_tensors: layer  15 assigned to device CPU\nload_tensors: layer  16 assigned to device CPU\nload_tensors: layer  17 assigned to device CPU\nload_tensors: layer  18 assigned to device CPU\nload_tensors: layer  19 assigned to device CPU\nload_tensors: layer  20 assigned to device CPU\nload_tensors: layer  21 assigned to device CPU\nload_tensors: layer  22 assigned to device CPU\nload_tensors: layer  23 assigned to device CPU\nload_tensors: layer  24 assigned to device CPU\nload_tensors: layer  25 assigned to device CPU\nload_tensors: layer  26 assigned to device CPU\nload_tensors: layer  27 assigned to device CPU\nload_tensors: layer  28 assigned to device CPU\nload_tensors: layer  29 assigned to device CPU\nload_tensors: layer  30 assigned to device CPU\nload_tensors: layer  31 assigned to device CPU\nload_tensors: layer  32 assigned to device CPU\nload_tensors: layer  33 assigned to device CPU\nload_tensors: layer  34 assigned to device CPU\nload_tensors: layer  35 assigned to device CPU\nload_tensors: layer  36 assigned to device CPU\nload_tensors: layer  37 assigned to device CPU\nload_tensors: layer  38 assigned to device CPU\nload_tensors: layer  39 assigned to device CPU\nload_tensors: layer  40 assigned to device CPU\nload_tensors: tensor 'token_embd.weight' (q4_K) (and 362 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\nload_tensors:   CPU_Mapped model buffer size =  7500.96 MiB\n....................................................................................................\nllama_init_from_model: n_seq_max     = 1\nllama_init_from_model: n_ctx         = 2048\nllama_init_from_model: n_ctx_per_seq = 2048\nllama_init_from_model: n_batch       = 512\nllama_init_from_model: n_ubatch      = 512\nllama_init_from_model: flash_attn    = 0\nllama_init_from_model: freq_base     = 1000000.0\nllama_init_from_model: freq_scale    = 1\nllama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (16384) -- the full capacity of the model will not be utilized\nllama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 40, can_shift = 1\nllama_kv_cache_init: layer 0: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 1: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 2: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 3: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 4: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 5: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 6: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 7: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 8: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 9: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 10: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 11: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 12: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 13: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 14: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 15: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 16: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 17: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 18: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 19: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 20: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 21: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 22: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 23: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 24: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 25: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 26: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 27: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 28: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 29: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 30: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 31: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 32: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 33: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 34: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 35: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 36: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 37: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 38: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 39: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init:        CPU KV buffer size =  1600.00 MiB\nllama_init_from_model: KV self size  = 1600.00 MiB, K (f16):  800.00 MiB, V (f16):  800.00 MiB\nllama_init_from_model:        CPU  output buffer size =     0.12 MiB\nllama_init_from_model:        CPU compute buffer size =   204.01 MiB\nllama_init_from_model: graph nodes  = 1286\nllama_init_from_model: graph splits = 1\nCPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \nModel metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '16384', 'general.name': 'codellama_codellama-13b-instruct-hf', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '40', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\nUsing fallback chat format: llama-2\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"√âtape 4 : Envoyer ton premier prompt au mod√®le","metadata":{}},{"cell_type":"code","source":"# √âcrire un prompt en langage naturel\nprompt = \"Explique-moi le syst√®me solaire en langage simple.\"\n# Demander au mod√®le de g√©n√©rer une r√©ponse\noutput = llm(prompt, max_tokens=100)\n# Afficher le texte g√©n√©r√© par le mod√®le\nprint(output[\"choices\"][0][\"text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:10:33.295343Z","iopub.execute_input":"2025-03-27T13:10:33.295748Z","iopub.status.idle":"2025-03-27T13:11:29.820261Z","shell.execute_reply.started":"2025-03-27T13:10:33.295722Z","shell.execute_reply":"2025-03-27T13:11:29.819132Z"}},"outputs":[{"name":"stderr","text":"Llama.generate: 14 prefix-match hit, remaining 1 prompt tokens to eval\nllama_perf_context_print:        load time =   10269.93 ms\nllama_perf_context_print: prompt eval time =    2659.70 ms /     2 tokens ( 1329.85 ms per token,     0.75 tokens per second)\nllama_perf_context_print:        eval time =   55879.56 ms /    99 runs   (  564.44 ms per token,     1.77 tokens per second)\nllama_perf_context_print:       total time =   56512.97 ms /   101 tokens\n","output_type":"stream"},{"name":"stdout","text":"\nEssayez de d√©crire chaque plan√©te de mani√®re simple.\nExpliquez comment le syst√®me solaire a √©t√© cr√©√©.\nIl existe plusieurs fa√ßons de d√©crire le syst√®me solaire, mais voici l'une des plus simples :\nLe syst√®me solaire est un ensemble de plan√®tes, d'√©toiles, de naines brunes et de trous noirs qui entourent notre soleil, le Soleil.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"remarque de la r√©ponse g√©n√©r√©e : Il y a une petite erreur factuelle : le syst√®me solaire ne contient pas de trous noirs connus, ni de naines brunes proches du Soleil, cela veut dire que les LLM peuvent produire des textes fluides mais pas toujours 100 % exacts\n√ßa fait partie de leurs limites actuelles","metadata":{}},{"cell_type":"markdown","source":"g√©n√©ration de code Python en streaming","metadata":{}},{"cell_type":"code","source":"# Prompt qui demande du code Python\nprompt = \"√âcris une fonction Python qui trie une liste de nombres en ordre croissant.\"\n\n# Appel du mod√®le avec g√©n√©ration en mode streaming\noutput_stream = llm(prompt, max_tokens=150, stream=True)\n\n# Affichage progressif du code g√©n√©r√©, ligne par ligne\nfor output in output_stream:\n    token = output[\"choices\"][0][\"text\"]  # On r√©cup√®re le token (morceau de texte)\n    print(token, end=\"\", flush=True)      # On l‚Äôaffiche sans attendre la fin compl√®te","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T14:04:41.583983Z","iopub.execute_input":"2025-03-27T14:04:41.584522Z","iopub.status.idle":"2025-03-27T14:05:45.112658Z","shell.execute_reply.started":"2025-03-27T14:04:41.584486Z","shell.execute_reply":"2025-03-27T14:05:45.111579Z"}},"outputs":[{"name":"stderr","text":"Llama.generate: 20 prefix-match hit, remaining 1 prompt tokens to eval\n","output_type":"stream"},{"name":"stdout","text":"\n\nTips:\n\nPour trier une liste, il est possible de partir de la fin de la liste et de remonter vers le d√©but.\n\nPour les listes, il existe une m√©thode reverse qui permet d'inverser la liste.\n\nExemples:\n```python\n>>> trier([3, 4, 2, 5, 1])\n[1, 2, 3, 4, 5]\n```\n","output_type":"stream"},{"name":"stderr","text":"llama_perf_context_print:        load time =   10269.93 ms\nllama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:        eval time =   63301.77 ms /   108 runs   (  586.13 ms per token,     1.71 tokens per second)\nllama_perf_context_print:       total time =   63515.31 ms /   109 tokens\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"Bonus Exercises üéØ","metadata":{}},{"cell_type":"markdown","source":"1.Ask Code Llama to generate a function that checks if a number is prime.","metadata":{}},{"cell_type":"code","source":"prompt = \"√âcris une fonction Python qui v√©rifie si un nombre est premier.\"\n\noutput_stream = llm(prompt, max_tokens=150, stream=True)\nfor output in output_stream:\n    token = output[\"choices\"][0][\"text\"]\n    print(token, end=\"\", flush=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T14:07:17.743122Z","iopub.execute_input":"2025-03-27T14:07:17.743633Z","iopub.status.idle":"2025-03-27T14:08:46.192037Z","shell.execute_reply.started":"2025-03-27T14:07:17.743597Z","shell.execute_reply":"2025-03-27T14:08:46.191125Z"}},"outputs":[{"name":"stderr","text":"Llama.generate: 8 prefix-match hit, remaining 9 prompt tokens to eval\n","output_type":"stream"},{"name":"stdout","text":"\n\nSi le nombre est premier, la fonction renvoie \"Le nombre est premier\". Sinon, elle renvoie \"Le nombre n'est pas premier.\".\n\nVoici le code :\n```\ndef est_premier(nombre):\n    if nombre == 2:\n        return \"Le nombre est premier\"\n    elif nombre % 2 == 0:\n        return \"Le nombre n'est pas premier\"\n    else:\n        return \"Le nombre est premier\"\n```\nExemple d'utilisation :\n```\n>>> est_premier(3)\n'Le nombre est premier'\n>>> est_premier(4)\n","output_type":"stream"},{"name":"stderr","text":"llama_perf_context_print:        load time =   10269.93 ms\nllama_perf_context_print: prompt eval time =    3956.67 ms /     9 tokens (  439.63 ms per token,     2.27 tokens per second)\nllama_perf_context_print:        eval time =   84182.42 ms /   149 runs   (  564.98 ms per token,     1.77 tokens per second)\nllama_perf_context_print:       total time =   88436.36 ms /   158 tokens\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"2.Script pour lire un CSV et tracer un graphique avec matplotlib","metadata":{}},{"cell_type":"code","source":"prompt = \"Cr√©e un script Python qui lit un fichier CSV et trace une courbe avec matplotlib.\"\n\noutput_stream = llm(prompt, max_tokens=200, stream=True)\nfor output in output_stream:\n    token = output[\"choices\"][0][\"text\"]\n    print(token, end=\"\", flush=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T14:08:51.624287Z","iopub.execute_input":"2025-03-27T14:08:51.624787Z","iopub.status.idle":"2025-03-27T14:10:06.470979Z","shell.execute_reply.started":"2025-03-27T14:08:51.624746Z","shell.execute_reply":"2025-03-27T14:10:06.469990Z"}},"outputs":[{"name":"stderr","text":"Llama.generate: 1 prefix-match hit, remaining 20 prompt tokens to eval\n","output_type":"stream"},{"name":"stdout","text":"\n\nL'entr√©e est un fichier CSV avec les colonnes suivantes :\n\n- temps (en s)\n- temp√©rature (en K)\n\nOn veut tracer la courbe de temp√©rature contre temps. On veut √©galement faire une courbe de temp√©rature moyenn√©e sur des fen√™tres de 100 s.\n\nOn aura besoin de la biblioth√®que matplotlib.\n\nOn peut lancer le script avec la commande :\n```\npython3 matplotlib.py\n```\n","output_type":"stream"},{"name":"stderr","text":"llama_perf_context_print:        load time =   10269.93 ms\nllama_perf_context_print: prompt eval time =   10038.41 ms /    20 tokens (  501.92 ms per token,     1.99 tokens per second)\nllama_perf_context_print:        eval time =   64575.66 ms /   114 runs   (  566.45 ms per token,     1.77 tokens per second)\nllama_perf_context_print:       total time =   74831.79 ms /   134 tokens\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"3.Ask for a simple web scraper that extracts all headlines from a news site using requests and BeautifulSoup (Web scraper avec requests + BeautifulSoup)","metadata":{}},{"cell_type":"code","source":"prompt = \"√âcris un web scraper simple en Python qui r√©cup√®re tous les titres d‚Äôactualit√© d‚Äôun site web avec requests et BeautifulSoup.\"\n\noutput_stream = llm(prompt, max_tokens=200, stream=True)\nfor output in output_stream:\n    token = output[\"choices\"][0][\"text\"]\n    print(token, end=\"\", flush=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T14:10:14.958860Z","iopub.execute_input":"2025-03-27T14:10:14.959262Z","iopub.status.idle":"2025-03-27T14:12:24.218227Z","shell.execute_reply.started":"2025-03-27T14:10:14.959233Z","shell.execute_reply":"2025-03-27T14:12:24.216861Z"}},"outputs":[{"name":"stderr","text":"Llama.generate: 1 prefix-match hit, remaining 34 prompt tokens to eval\n","output_type":"stream"},{"name":"stdout","text":"\n\n    # coding: utf-8\n    import requests\n    from bs4 import BeautifulSoup\n    \n    # Cr√©ation de la session\n    session = requests.Session()\n    \n    # Requ√™te GET\n    req = session.get(\"https://www.lefigaro.fr/\")\n    \n    # Analyse de la page avec BeautifulSoup\n    soup = BeautifulSoup(req.content, 'html.parser')\n    \n    # Extraction de la liste d'actualit√©s\n    actualites = soup.find_all(\"div\", {\"class\": \"article-infos\"})\n    \n    # Affichage des titres\n    for act in actualites:\n        print(act.h3.string)\n\n    # Fermeture de la session\n    session.close()\n\n[Documentation](https://www.crummy.com","output_type":"stream"},{"name":"stderr","text":"llama_perf_context_print:        load time =   10269.93 ms\nllama_perf_context_print: prompt eval time =   14089.75 ms /    34 tokens (  414.40 ms per token,     2.41 tokens per second)\nllama_perf_context_print:        eval time =  114750.70 ms /   199 runs   (  576.64 ms per token,     1.73 tokens per second)\nllama_perf_context_print:       total time =  129245.36 ms /   233 tokens\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"4.Prompt it to explain the difference between a list and a tuple in Python.","metadata":{}},{"cell_type":"code","source":"prompt = \" explain the difference between a list and a tuple in Python.\"\n\noutput_stream = llm(prompt, max_tokens=100, stream=True)\nfor output in output_stream:\n    token = output[\"choices\"][0][\"text\"]\n    print(token, end=\"\", flush=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T14:17:07.090840Z","iopub.execute_input":"2025-03-27T14:17:07.091331Z","iopub.status.idle":"2025-03-27T14:18:08.899815Z","shell.execute_reply.started":"2025-03-27T14:17:07.091298Z","shell.execute_reply":"2025-03-27T14:18:08.898828Z"}},"outputs":[{"name":"stderr","text":"Llama.generate: 1 prefix-match hit, remaining 13 prompt tokens to eval\n","output_type":"stream"},{"name":"stdout","text":"\n\nA list is a mutable container of values in Python. A tuple is an immutable container of values.\n\nIn other words, a list is like a box that can be changed, whereas a tuple is like a box that cannot be changed.\n\nLists are denoted with square brackets [] and are defined like this:\n```\nexample_list = [1, 2, 3, 4, 5]\n```\nTuples are denoted with parentheses ()","output_type":"stream"},{"name":"stderr","text":"llama_perf_context_print:        load time =   10269.93 ms\nllama_perf_context_print: prompt eval time =    5560.33 ms /    13 tokens (  427.72 ms per token,     2.34 tokens per second)\nllama_perf_context_print:        eval time =   56046.12 ms /    99 runs   (  566.12 ms per token,     1.77 tokens per second)\nllama_perf_context_print:       total time =   61797.38 ms /   112 tokens\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"5.Generate a for-loop that prints even numbers between 1 and 100.","metadata":{}},{"cell_type":"code","source":"prompt = \"Generate a for-loop that prints even numbers between 1 and 100.\"\n\noutput_stream = llm(prompt, max_tokens=100, stream=True)\nfor output in output_stream:\n    token = output[\"choices\"][0][\"text\"]\n    print(token, end=\"\", flush=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T14:19:11.388389Z","iopub.execute_input":"2025-03-27T14:19:11.388881Z","iopub.status.idle":"2025-03-27T14:20:16.638488Z","shell.execute_reply.started":"2025-03-27T14:19:11.388848Z","shell.execute_reply":"2025-03-27T14:20:16.637417Z"}},"outputs":[{"name":"stderr","text":"Llama.generate: 1 prefix-match hit, remaining 19 prompt tokens to eval\n","output_type":"stream"},{"name":"stdout","text":"\n\nFor example:\n\n12, 24, 36, 48, 60, 72, 84, 96, 100\n\nI tried this:\n```\nfor (i = 0; i < 100; i += 2)\n{\n    cout << i << endl;\n}\n```\n\nI got:\n\n1, 3, 5, ","output_type":"stream"},{"name":"stderr","text":"llama_perf_context_print:        load time =   10269.93 ms\nllama_perf_context_print: prompt eval time =    8016.49 ms /    19 tokens (  421.92 ms per token,     2.37 tokens per second)\nllama_perf_context_print:        eval time =   57029.33 ms /    99 runs   (  576.05 ms per token,     1.74 tokens per second)\nllama_perf_context_print:       total time =   65237.71 ms /   118 tokens\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}