{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:08:22.934733Z","iopub.execute_input":"2025-03-27T11:08:22.935261Z","iopub.status.idle":"2025-03-27T11:08:24.154961Z","shell.execute_reply.started":"2025-03-27T11:08:22.935216Z","shell.execute_reply":"2025-03-27T11:08:24.153781Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"#Daily Challenge : Run Code Llama on Kaggle using llama.cpp","metadata":{}},{"cell_type":"markdown","source":"1. Faire la demande d’accès\\\n   demande d'accès accès au modèle CodeLLaMA-13B-Instruct sur Hugging Face; validation officielle reçue par email","metadata":{}},{"cell_type":"markdown","source":"Étape 2 : Télécharger le modèle dans Kaggle\\\n","metadata":{}},{"cell_type":"code","source":"# Afficher les fichiers présents dans le dossier de travail (/kaggle/working)\n!ls -lh /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:34:17.299378Z","iopub.execute_input":"2025-03-27T11:34:17.299785Z","iopub.status.idle":"2025-03-27T11:34:17.423396Z","shell.execute_reply.started":"2025-03-27T11:34:17.299737Z","shell.execute_reply":"2025-03-27T11:34:17.422014Z"}},"outputs":[{"name":"stdout","text":"total 0\n-rw-r--r-- 1 root root 0 Mar 27 11:32 codellama-13b-instruct.gguf\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Forcer l'enregistrement dans /kaggle/working\n!wget https://huggingface.co/TheBloke/CodeLlama-13B-Instruct-GGUF/resolve/main/codellama-13b-instruct.Q4_K_M.gguf -P /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:01:09.116463Z","iopub.execute_input":"2025-03-27T13:01:09.116962Z","iopub.status.idle":"2025-03-27T13:02:01.333464Z","shell.execute_reply.started":"2025-03-27T13:01:09.116925Z","shell.execute_reply":"2025-03-27T13:02:01.330852Z"}},"outputs":[{"name":"stdout","text":"--2025-03-27 13:01:09--  https://huggingface.co/TheBloke/CodeLlama-13B-Instruct-GGUF/resolve/main/codellama-13b-instruct.Q4_K_M.gguf\nResolving huggingface.co (huggingface.co)... 18.244.202.68, 18.244.202.60, 18.244.202.118, ...\nConnecting to huggingface.co (huggingface.co)|18.244.202.68|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cdn-lfs.hf.co/repos/7b/80/7b800b9247db84b65d4ca008ca4433a306b7f259163c4d026874b4aa9f7112eb/48cc5600c5e35b1226208a53b1871f50efb15764232babaef23e2264c285d7d9?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27codellama-13b-instruct.Q4_K_M.gguf%3B+filename%3D%22codellama-13b-instruct.Q4_K_M.gguf%22%3B&Expires=1743084069&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzA4NDA2OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy83Yi84MC83YjgwMGI5MjQ3ZGI4NGI2NWQ0Y2EwMDhjYTQ0MzNhMzA2YjdmMjU5MTYzYzRkMDI2ODc0YjRhYTlmNzExMmViLzQ4Y2M1NjAwYzVlMzViMTIyNjIwOGE1M2IxODcxZjUwZWZiMTU3NjQyMzJiYWJhZWYyM2UyMjY0YzI4NWQ3ZDk%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=KynS3EWOfJtvAL3gZp3XkZOes%7Ea7ZQKOBiP26XRZ-PFwQlDJ6mJoq4bM-v5fOqQACQoP2eK6gKfaYA%7Em4PbTZsst6OgxU9n1OTqCRgk%7EYQoMf4pOrkPkLwrytVbp-URlfUR1zw5C-FENTgJA2ipGdXZQJ7Evwm3C9Wmtcp5cINN-t9JGSfw6Vmk4UkTonoE8lDphXXeUilvIRtgO40UNKuwPk9NNgLNX7WV5XTcvhq%7EUd5XAdgsjPb5E8KaXmAHs8rtO3vqvHES0zccIN5uRXml%7EwVDIFLXvbHHiiUB7rqujXF3LB%7Ecy17liJvhdTEJAUbF23RPvzaQEwBjr8kfK9w__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n--2025-03-27 13:01:09--  https://cdn-lfs.hf.co/repos/7b/80/7b800b9247db84b65d4ca008ca4433a306b7f259163c4d026874b4aa9f7112eb/48cc5600c5e35b1226208a53b1871f50efb15764232babaef23e2264c285d7d9?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27codellama-13b-instruct.Q4_K_M.gguf%3B+filename%3D%22codellama-13b-instruct.Q4_K_M.gguf%22%3B&Expires=1743084069&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzA4NDA2OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy83Yi84MC83YjgwMGI5MjQ3ZGI4NGI2NWQ0Y2EwMDhjYTQ0MzNhMzA2YjdmMjU5MTYzYzRkMDI2ODc0YjRhYTlmNzExMmViLzQ4Y2M1NjAwYzVlMzViMTIyNjIwOGE1M2IxODcxZjUwZWZiMTU3NjQyMzJiYWJhZWYyM2UyMjY0YzI4NWQ3ZDk%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=KynS3EWOfJtvAL3gZp3XkZOes%7Ea7ZQKOBiP26XRZ-PFwQlDJ6mJoq4bM-v5fOqQACQoP2eK6gKfaYA%7Em4PbTZsst6OgxU9n1OTqCRgk%7EYQoMf4pOrkPkLwrytVbp-URlfUR1zw5C-FENTgJA2ipGdXZQJ7Evwm3C9Wmtcp5cINN-t9JGSfw6Vmk4UkTonoE8lDphXXeUilvIRtgO40UNKuwPk9NNgLNX7WV5XTcvhq%7EUd5XAdgsjPb5E8KaXmAHs8rtO3vqvHES0zccIN5uRXml%7EwVDIFLXvbHHiiUB7rqujXF3LB%7Ecy17liJvhdTEJAUbF23RPvzaQEwBjr8kfK9w__&Key-Pair-Id=K3RPWS32NSSJCE\nResolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 18.160.78.87, 18.160.78.43, 18.160.78.76, ...\nConnecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|18.160.78.87|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7866070016 (7.3G) [binary/octet-stream]\nSaving to: ‘/kaggle/working/codellama-13b-instruct.Q4_K_M.gguf’\n\ncodellama-13b-instr 100%[===================>]   7.33G  99.5MB/s    in 52s     \n\n2025-03-27 13:02:01 (145 MB/s) - ‘/kaggle/working/codellama-13b-instruct.Q4_K_M.gguf’ saved [7866070016/7866070016]\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!ls -lh /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:03:22.525858Z","iopub.execute_input":"2025-03-27T13:03:22.526397Z","iopub.status.idle":"2025-03-27T13:03:22.657153Z","shell.execute_reply.started":"2025-03-27T13:03:22.526359Z","shell.execute_reply":"2025-03-27T13:03:22.655715Z"}},"outputs":[{"name":"stdout","text":"total 7.4G\n-rw-r--r-- 1 root root 7.4G Sep  5  2023 codellama-13b-instruct.Q4_K_M.gguf\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"Étape 3 : Charger le modèle avec llama-cpp-python","metadata":{}},{"cell_type":"code","source":"# Installer llama-cpp-python avec support CUDA (GPU)\n!pip install llama-cpp-python --upgrade --quiet\n# Ce package est une interface Python vers llama.cpp : il permet de charger et utiliser ton modèle .gguf en Python.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:03:58.671780Z","iopub.execute_input":"2025-03-27T13:03:58.672289Z","iopub.status.idle":"2025-03-27T13:04:03.612413Z","shell.execute_reply.started":"2025-03-27T13:03:58.672239Z","shell.execute_reply":"2025-03-27T13:04:03.610971Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Importer le wrapper Python pour llama.cpp\nfrom llama_cpp import Llama","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:04:09.732882Z","iopub.execute_input":"2025-03-27T13:04:09.733390Z","iopub.status.idle":"2025-03-27T13:04:09.738363Z","shell.execute_reply.started":"2025-03-27T13:04:09.733351Z","shell.execute_reply":"2025-03-27T13:04:09.737150Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Charger le modèle GGUF téléchargé (nom exact du fichier)\nllm = Llama(\n    model_path=\"codellama-13b-instruct.Q4_K_M.gguf\",  # Le fichier que tu as téléchargé\n    n_ctx=2048,        # Contexte maximal (nombre de tokens d'entrée)\n    n_threads=8,       # Nombre de threads CPU à utiliser (tu peux ajuster selon ta machine)\n    verbose=True       # Pour afficher les infos du chargement\n)\n# n_threads=8 fonctionne bien sur Kaggle avec un GPU activé / on peut aussi tester sur CPU","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:04:12.670128Z","iopub.execute_input":"2025-03-27T13:04:12.670480Z","iopub.status.idle":"2025-03-27T13:04:39.710948Z","shell.execute_reply.started":"2025-03-27T13:04:12.670455Z","shell.execute_reply":"2025-03-27T13:04:39.709822Z"}},"outputs":[{"name":"stderr","text":"llama_model_loader: loaded meta data with 20 key-value pairs and 363 tensors from codellama-13b-instruct.Q4_K_M.gguf (version GGUF V2)\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = codellama_codellama-13b-instruct-hf\nllama_model_loader: - kv   2:                       llama.context_length u32              = 16384\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\nllama_model_loader: - kv   4:                          llama.block_count u32              = 40\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\nllama_model_loader: - kv  11:                          general.file_type u32              = 15\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  19:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   81 tensors\nllama_model_loader: - type q4_K:  241 tensors\nllama_model_loader: - type q6_K:   41 tensors\nprint_info: file format = GGUF V2\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 7.33 GiB (4.83 BPW) \ninit_tokenizer: initializing tokenizer for type 1\nload: control token:      2 '</s>' is not marked as EOG\nload: control token:      1 '<s>' is not marked as EOG\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 3\nload: token to piece cache size = 0.1686 MB\nprint_info: arch             = llama\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 16384\nprint_info: n_embd           = 5120\nprint_info: n_layer          = 40\nprint_info: n_head           = 40\nprint_info: n_head_kv        = 40\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 1\nprint_info: n_embd_k_gqa     = 5120\nprint_info: n_embd_v_gqa     = 5120\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-05\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 13824\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 0\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 1000000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 16384\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 13B\nprint_info: model params     = 13.02 B\nprint_info: general.name     = codellama_codellama-13b-instruct-hf\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 32016\nprint_info: n_merges         = 0\nprint_info: BOS token        = 1 '<s>'\nprint_info: EOS token        = 2 '</s>'\nprint_info: UNK token        = 0 '<unk>'\nprint_info: LF token         = 13 '<0x0A>'\nprint_info: EOG token        = 2 '</s>'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: layer   0 assigned to device CPU\nload_tensors: layer   1 assigned to device CPU\nload_tensors: layer   2 assigned to device CPU\nload_tensors: layer   3 assigned to device CPU\nload_tensors: layer   4 assigned to device CPU\nload_tensors: layer   5 assigned to device CPU\nload_tensors: layer   6 assigned to device CPU\nload_tensors: layer   7 assigned to device CPU\nload_tensors: layer   8 assigned to device CPU\nload_tensors: layer   9 assigned to device CPU\nload_tensors: layer  10 assigned to device CPU\nload_tensors: layer  11 assigned to device CPU\nload_tensors: layer  12 assigned to device CPU\nload_tensors: layer  13 assigned to device CPU\nload_tensors: layer  14 assigned to device CPU\nload_tensors: layer  15 assigned to device CPU\nload_tensors: layer  16 assigned to device CPU\nload_tensors: layer  17 assigned to device CPU\nload_tensors: layer  18 assigned to device CPU\nload_tensors: layer  19 assigned to device CPU\nload_tensors: layer  20 assigned to device CPU\nload_tensors: layer  21 assigned to device CPU\nload_tensors: layer  22 assigned to device CPU\nload_tensors: layer  23 assigned to device CPU\nload_tensors: layer  24 assigned to device CPU\nload_tensors: layer  25 assigned to device CPU\nload_tensors: layer  26 assigned to device CPU\nload_tensors: layer  27 assigned to device CPU\nload_tensors: layer  28 assigned to device CPU\nload_tensors: layer  29 assigned to device CPU\nload_tensors: layer  30 assigned to device CPU\nload_tensors: layer  31 assigned to device CPU\nload_tensors: layer  32 assigned to device CPU\nload_tensors: layer  33 assigned to device CPU\nload_tensors: layer  34 assigned to device CPU\nload_tensors: layer  35 assigned to device CPU\nload_tensors: layer  36 assigned to device CPU\nload_tensors: layer  37 assigned to device CPU\nload_tensors: layer  38 assigned to device CPU\nload_tensors: layer  39 assigned to device CPU\nload_tensors: layer  40 assigned to device CPU\nload_tensors: tensor 'token_embd.weight' (q4_K) (and 362 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\nload_tensors:   CPU_Mapped model buffer size =  7500.96 MiB\n....................................................................................................\nllama_init_from_model: n_seq_max     = 1\nllama_init_from_model: n_ctx         = 2048\nllama_init_from_model: n_ctx_per_seq = 2048\nllama_init_from_model: n_batch       = 512\nllama_init_from_model: n_ubatch      = 512\nllama_init_from_model: flash_attn    = 0\nllama_init_from_model: freq_base     = 1000000.0\nllama_init_from_model: freq_scale    = 1\nllama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (16384) -- the full capacity of the model will not be utilized\nllama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 40, can_shift = 1\nllama_kv_cache_init: layer 0: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 1: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 2: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 3: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 4: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 5: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 6: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 7: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 8: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 9: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 10: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 11: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 12: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 13: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 14: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 15: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 16: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 17: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 18: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 19: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 20: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 21: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 22: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 23: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 24: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 25: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 26: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 27: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 28: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 29: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 30: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 31: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 32: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 33: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 34: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 35: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 36: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 37: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 38: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init: layer 39: n_embd_k_gqa = 5120, n_embd_v_gqa = 5120\nllama_kv_cache_init:        CPU KV buffer size =  1600.00 MiB\nllama_init_from_model: KV self size  = 1600.00 MiB, K (f16):  800.00 MiB, V (f16):  800.00 MiB\nllama_init_from_model:        CPU  output buffer size =     0.12 MiB\nllama_init_from_model:        CPU compute buffer size =   204.01 MiB\nllama_init_from_model: graph nodes  = 1286\nllama_init_from_model: graph splits = 1\nCPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \nModel metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '16384', 'general.name': 'codellama_codellama-13b-instruct-hf', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '40', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\nUsing fallback chat format: llama-2\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"Étape 4 : Envoyer ton premier prompt au modèle","metadata":{}},{"cell_type":"code","source":"# Écrire un prompt en langage naturel\nprompt = \"Explique-moi le système solaire en langage simple.\"\n# Demander au modèle de générer une réponse\noutput = llm(prompt, max_tokens=100)\n# Afficher le texte généré par le modèle\nprint(output[\"choices\"][0][\"text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T13:10:33.295343Z","iopub.execute_input":"2025-03-27T13:10:33.295748Z","iopub.status.idle":"2025-03-27T13:11:29.820261Z","shell.execute_reply.started":"2025-03-27T13:10:33.295722Z","shell.execute_reply":"2025-03-27T13:11:29.819132Z"}},"outputs":[{"name":"stderr","text":"Llama.generate: 14 prefix-match hit, remaining 1 prompt tokens to eval\nllama_perf_context_print:        load time =   10269.93 ms\nllama_perf_context_print: prompt eval time =    2659.70 ms /     2 tokens ( 1329.85 ms per token,     0.75 tokens per second)\nllama_perf_context_print:        eval time =   55879.56 ms /    99 runs   (  564.44 ms per token,     1.77 tokens per second)\nllama_perf_context_print:       total time =   56512.97 ms /   101 tokens\n","output_type":"stream"},{"name":"stdout","text":"\nEssayez de décrire chaque planéte de manière simple.\nExpliquez comment le système solaire a été créé.\nIl existe plusieurs façons de décrire le système solaire, mais voici l'une des plus simples :\nLe système solaire est un ensemble de planètes, d'étoiles, de naines brunes et de trous noirs qui entourent notre soleil, le Soleil.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"remarque de la réponse générée : Il y a une petite erreur factuelle : le système solaire ne contient pas de trous noirs connus, ni de naines brunes proches du Soleil, cela veut dire que les LLM peuvent produire des textes fluides mais pas toujours 100 % exacts\nça fait partie de leurs limites actuelles","metadata":{}},{"cell_type":"markdown","source":"génération de code Python en streaming","metadata":{}},{"cell_type":"code","source":"# Prompt qui demande du code Python\nprompt = \"Écris une fonction Python qui trie une liste de nombres en ordre croissant.\"\n\n# Appel du modèle avec génération en mode streaming\noutput_stream = llm(prompt, max_tokens=150, stream=True)\n\n# Affichage progressif du code généré, ligne par ligne\nfor output in output_stream:\n    token = output[\"choices\"][0][\"text\"]  # On récupère le token (morceau de texte)\n    print(token, end=\"\", flush=True)      # On l’affiche sans attendre la fin complète","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T14:04:41.583983Z","iopub.execute_input":"2025-03-27T14:04:41.584522Z","iopub.status.idle":"2025-03-27T14:05:45.112658Z","shell.execute_reply.started":"2025-03-27T14:04:41.584486Z","shell.execute_reply":"2025-03-27T14:05:45.111579Z"}},"outputs":[{"name":"stderr","text":"Llama.generate: 20 prefix-match hit, remaining 1 prompt tokens to eval\n","output_type":"stream"},{"name":"stdout","text":"\n\nTips:\n\nPour trier une liste, il est possible de partir de la fin de la liste et de remonter vers le début.\n\nPour les listes, il existe une méthode reverse qui permet d'inverser la liste.\n\nExemples:\n```python\n>>> trier([3, 4, 2, 5, 1])\n[1, 2, 3, 4, 5]\n```\n","output_type":"stream"},{"name":"stderr","text":"llama_perf_context_print:        load time =   10269.93 ms\nllama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:        eval time =   63301.77 ms /   108 runs   (  586.13 ms per token,     1.71 tokens per second)\nllama_perf_context_print:       total time =   63515.31 ms /   109 tokens\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"Bonus Exercises 🎯","metadata":{}},{"cell_type":"markdown","source":"1.Ask Code Llama to generate a function that checks if a number is prime.","metadata":{}},{"cell_type":"code","source":"prompt = \"Écris une fonction Python qui vérifie si un nombre est premier.\"\n\noutput_stream = llm(prompt, max_tokens=150, stream=True)\nfor output in output_stream:\n    token = output[\"choices\"][0][\"text\"]\n    print(token, end=\"\", flush=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T14:07:17.743122Z","iopub.execute_input":"2025-03-27T14:07:17.743633Z","iopub.status.idle":"2025-03-27T14:08:46.192037Z","shell.execute_reply.started":"2025-03-27T14:07:17.743597Z","shell.execute_reply":"2025-03-27T14:08:46.191125Z"}},"outputs":[{"name":"stderr","text":"Llama.generate: 8 prefix-match hit, remaining 9 prompt tokens to eval\n","output_type":"stream"},{"name":"stdout","text":"\n\nSi le nombre est premier, la fonction renvoie \"Le nombre est premier\". Sinon, elle renvoie \"Le nombre n'est pas premier.\".\n\nVoici le code :\n```\ndef est_premier(nombre):\n    if nombre == 2:\n        return \"Le nombre est premier\"\n    elif nombre % 2 == 0:\n        return \"Le nombre n'est pas premier\"\n    else:\n        return \"Le nombre est premier\"\n```\nExemple d'utilisation :\n```\n>>> est_premier(3)\n'Le nombre est premier'\n>>> est_premier(4)\n","output_type":"stream"},{"name":"stderr","text":"llama_perf_context_print:        load time =   10269.93 ms\nllama_perf_context_print: prompt eval time =    3956.67 ms /     9 tokens (  439.63 ms per token,     2.27 tokens per second)\nllama_perf_context_print:        eval time =   84182.42 ms /   149 runs   (  564.98 ms per token,     1.77 tokens per second)\nllama_perf_context_print:       total time =   88436.36 ms /   158 tokens\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"2.Script pour lire un CSV et tracer un graphique avec matplotlib","metadata":{}},{"cell_type":"code","source":"prompt = \"Crée un script Python qui lit un fichier CSV et trace une courbe avec matplotlib.\"\n\noutput_stream = llm(prompt, max_tokens=200, stream=True)\nfor output in output_stream:\n    token = output[\"choices\"][0][\"text\"]\n    print(token, end=\"\", flush=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T14:08:51.624287Z","iopub.execute_input":"2025-03-27T14:08:51.624787Z","iopub.status.idle":"2025-03-27T14:10:06.470979Z","shell.execute_reply.started":"2025-03-27T14:08:51.624746Z","shell.execute_reply":"2025-03-27T14:10:06.469990Z"}},"outputs":[{"name":"stderr","text":"Llama.generate: 1 prefix-match hit, remaining 20 prompt tokens to eval\n","output_type":"stream"},{"name":"stdout","text":"\n\nL'entrée est un fichier CSV avec les colonnes suivantes :\n\n- temps (en s)\n- température (en K)\n\nOn veut tracer la courbe de température contre temps. On veut également faire une courbe de température moyennée sur des fenêtres de 100 s.\n\nOn aura besoin de la bibliothèque matplotlib.\n\nOn peut lancer le script avec la commande :\n```\npython3 matplotlib.py\n```\n","output_type":"stream"},{"name":"stderr","text":"llama_perf_context_print:        load time =   10269.93 ms\nllama_perf_context_print: prompt eval time =   10038.41 ms /    20 tokens (  501.92 ms per token,     1.99 tokens per second)\nllama_perf_context_print:        eval time =   64575.66 ms /   114 runs   (  566.45 ms per token,     1.77 tokens per second)\nllama_perf_context_print:       total time =   74831.79 ms /   134 tokens\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"3.Ask for a simple web scraper that extracts all headlines from a news site using requests and BeautifulSoup (Web scraper avec requests + BeautifulSoup)","metadata":{}},{"cell_type":"code","source":"prompt = \"Écris un web scraper simple en Python qui récupère tous les titres d’actualité d’un site web avec requests et BeautifulSoup.\"\n\noutput_stream = llm(prompt, max_tokens=200, stream=True)\nfor output in output_stream:\n    token = output[\"choices\"][0][\"text\"]\n    print(token, end=\"\", flush=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T14:10:14.958860Z","iopub.execute_input":"2025-03-27T14:10:14.959262Z","iopub.status.idle":"2025-03-27T14:12:24.218227Z","shell.execute_reply.started":"2025-03-27T14:10:14.959233Z","shell.execute_reply":"2025-03-27T14:12:24.216861Z"}},"outputs":[{"name":"stderr","text":"Llama.generate: 1 prefix-match hit, remaining 34 prompt tokens to eval\n","output_type":"stream"},{"name":"stdout","text":"\n\n    # coding: utf-8\n    import requests\n    from bs4 import BeautifulSoup\n    \n    # Création de la session\n    session = requests.Session()\n    \n    # Requête GET\n    req = session.get(\"https://www.lefigaro.fr/\")\n    \n    # Analyse de la page avec BeautifulSoup\n    soup = BeautifulSoup(req.content, 'html.parser')\n    \n    # Extraction de la liste d'actualités\n    actualites = soup.find_all(\"div\", {\"class\": \"article-infos\"})\n    \n    # Affichage des titres\n    for act in actualites:\n        print(act.h3.string)\n\n    # Fermeture de la session\n    session.close()\n\n[Documentation](https://www.crummy.com","output_type":"stream"},{"name":"stderr","text":"llama_perf_context_print:        load time =   10269.93 ms\nllama_perf_context_print: prompt eval time =   14089.75 ms /    34 tokens (  414.40 ms per token,     2.41 tokens per second)\nllama_perf_context_print:        eval time =  114750.70 ms /   199 runs   (  576.64 ms per token,     1.73 tokens per second)\nllama_perf_context_print:       total time =  129245.36 ms /   233 tokens\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"4.Prompt it to explain the difference between a list and a tuple in Python.","metadata":{}},{"cell_type":"code","source":"prompt = \" explain the difference between a list and a tuple in Python.\"\n\noutput_stream = llm(prompt, max_tokens=100, stream=True)\nfor output in output_stream:\n    token = output[\"choices\"][0][\"text\"]\n    print(token, end=\"\", flush=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T14:17:07.090840Z","iopub.execute_input":"2025-03-27T14:17:07.091331Z","iopub.status.idle":"2025-03-27T14:18:08.899815Z","shell.execute_reply.started":"2025-03-27T14:17:07.091298Z","shell.execute_reply":"2025-03-27T14:18:08.898828Z"}},"outputs":[{"name":"stderr","text":"Llama.generate: 1 prefix-match hit, remaining 13 prompt tokens to eval\n","output_type":"stream"},{"name":"stdout","text":"\n\nA list is a mutable container of values in Python. A tuple is an immutable container of values.\n\nIn other words, a list is like a box that can be changed, whereas a tuple is like a box that cannot be changed.\n\nLists are denoted with square brackets [] and are defined like this:\n```\nexample_list = [1, 2, 3, 4, 5]\n```\nTuples are denoted with parentheses ()","output_type":"stream"},{"name":"stderr","text":"llama_perf_context_print:        load time =   10269.93 ms\nllama_perf_context_print: prompt eval time =    5560.33 ms /    13 tokens (  427.72 ms per token,     2.34 tokens per second)\nllama_perf_context_print:        eval time =   56046.12 ms /    99 runs   (  566.12 ms per token,     1.77 tokens per second)\nllama_perf_context_print:       total time =   61797.38 ms /   112 tokens\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"5.Generate a for-loop that prints even numbers between 1 and 100.","metadata":{}},{"cell_type":"code","source":"prompt = \"Generate a for-loop that prints even numbers between 1 and 100.\"\n\noutput_stream = llm(prompt, max_tokens=100, stream=True)\nfor output in output_stream:\n    token = output[\"choices\"][0][\"text\"]\n    print(token, end=\"\", flush=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T14:19:11.388389Z","iopub.execute_input":"2025-03-27T14:19:11.388881Z","iopub.status.idle":"2025-03-27T14:20:16.638488Z","shell.execute_reply.started":"2025-03-27T14:19:11.388848Z","shell.execute_reply":"2025-03-27T14:20:16.637417Z"}},"outputs":[{"name":"stderr","text":"Llama.generate: 1 prefix-match hit, remaining 19 prompt tokens to eval\n","output_type":"stream"},{"name":"stdout","text":"\n\nFor example:\n\n12, 24, 36, 48, 60, 72, 84, 96, 100\n\nI tried this:\n```\nfor (i = 0; i < 100; i += 2)\n{\n    cout << i << endl;\n}\n```\n\nI got:\n\n1, 3, 5, ","output_type":"stream"},{"name":"stderr","text":"llama_perf_context_print:        load time =   10269.93 ms\nllama_perf_context_print: prompt eval time =    8016.49 ms /    19 tokens (  421.92 ms per token,     2.37 tokens per second)\nllama_perf_context_print:        eval time =   57029.33 ms /    99 runs   (  576.05 ms per token,     1.74 tokens per second)\nllama_perf_context_print:       total time =   65237.71 ms /   118 tokens\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}